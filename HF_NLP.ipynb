{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dad4f73d",
   "metadata": {},
   "source": [
    "#### **Natural Language Processing**\n",
    "\n",
    "`NLP` is a field of computer linguistics that focuses on enabling computers to understand, interpet and generate human language. `LLMs` are the most advanced type of models in NLP. NLP is categorized into some tasks, that models attempt to complete:\n",
    "- `Clasifying words`: NER, Part of Speech Tagging\n",
    "- `Classifying sentences`: Sentiment Analysis\n",
    "- `Generating text`: summarisation, autocompletion\n",
    "- `Extracting answers`: QA\n",
    "\n",
    "> **What are LLMs?**\n",
    "> **How do they differ from other NLP models built?**\n",
    "\n",
    "`LLMs` are NLP models characterised by their massive size, extensive training data and ability to complete a wide array of language tasks with minimal task specific training. At the heart of LLMs is an architecture known as the `Transformer`.\n",
    "##### **Characteristics of LLMS**\n",
    "1. Scale\n",
    "2. Emergence\n",
    "3. General capabilities\n",
    "4. In-context learning\n",
    "\n",
    "Similar to code, where multiple people around the globe can write it, build applications and websites and have a need for sharing the code either to benefit anyone out there or to collaborate with collagues, people working on NLP models might also need to share models or collaborate with others. That way for any NLP task we can think of, we do not need to build models from scratch but can work on or finetune already built models. Similarly we might build a pretty amazing model ourselves and want to share it with others. What `GitHub` is for developers and software engineers, `ðŸ¤—` is for data scientists, AI engineers, ML engineers and just about anyone working on AI, Machine Learning and NLP. This central collection of all the state of the art models alongside the `Transformers` API developed by ðŸ¤— allows us to quikcly accomplish NLP tasks with very high levels of accuracy without building models from scratch or hosting them ourselves locally.\n",
    "\n",
    "At the heart of it all is the `pipeline`. For any NLP task, we have a ML model. ML models need inputs to be numeric or categorical at least, and language is nothing close to that. To convert input which could be a word, phrase or sentence into an input the model can understand, get model predictions/output and then convert that to human language is a process that has been studied widely and can therefore be standardised into a series of steps, known as `pre-processing` steps. The pipeline is like a housing for all this intricate conversions, transformations and inference. You text goes into the pipeline and text comes out of it without ever having to explicitly design what happens in between.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af616084",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f9ed9d",
   "metadata": {},
   "source": [
    "##### **Sentiment Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc54f8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Morgan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1e088f17c7942f3916e7f5f3e1861ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Morgan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Morgan\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Morgan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b56f9543424a529723a03b7fe73526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8fe81e5f75e44f0a49763f1aed9ed02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use 0\n"
     ]
    }
   ],
   "source": [
    "sent_class = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4fbc22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\n",
    "    \"I have been waiting for this job for a really long time\",\n",
    "    \"Anger is an amazing motivator if chanelled correctly\",\n",
    "    \"The Kenyan government is very unpopular with its citizens\",\n",
    "    \"I hate this so much!\",\n",
    "    \"HuggingFace courses are amazing\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b0fe508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9700242877006531},\n",
       " {'label': 'POSITIVE', 'score': 0.9994468092918396},\n",
       " {'label': 'NEGATIVE', 'score': 0.9991427659988403},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455},\n",
       " {'label': 'POSITIVE', 'score': 0.9998717308044434}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_class(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104a7fcb",
   "metadata": {},
   "source": [
    "##### **Text Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3b6be40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99674c5e0e914b49a993a6590a6cfaa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Morgan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Morgan\\.cache\\huggingface\\hub\\models--openai-community--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a7eea4b439d457e87b442a780bd4d52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dbef6874dc0471c907d3471edbc36dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f58044e695b048689a96214d3e31fe94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b294187933e24ddd92c5a148eee12d17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27eb64409c984b4fb9370e457c8d2f22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use 0\n"
     ]
    }
   ],
   "source": [
    "generator  = pipeline(\"text-generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33355c3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Some NLP projects that force any junior to learn the necessary skils to one day be a great professional are NLP classes that put a junior in a class of only ten (10) that takes advantage of him/herself to find the proper skils. It's usually called a nrlclass, a very high-stakes nrl class to get a high ranking by the junior through a rigorous, thorough and hard-to-understand study.\\n\\nThere are NLP classes in the public university of any state, even with a private university. What exactly is a nrlclass\"},\n",
       " {'generated_text': \"Some NLP projects that force any junior to learn the necessary skils to one day be a great professional are: â€“ the 'Inner Skil' programme, a comprehensive online workshop, which teaches new skil-related concepts and has been running for some 7 years. â€“ (formerly known as the 'Orientation Centre for the Development of Skil Learning') the 'Skil Day' course, at which junior students are given to discover what works and isn't effective with the rest of the curriculum. Many more organisations have also taken on this opportunity â€“ we've been fortunate enough to meet\"},\n",
       " {'generated_text': \"Some NLP projects that force any junior to learn the necessary skils to one day be a great professional are those projects when you don't know what new skills to get you through the year so they need to learn that particular vocabulary.\\n\\nThe challenge is to put your hands on those three points. Don't just rely on your ego, your mind or any other type of wisdom as you make a new decision. Instead, take things slow, ask questions and focus on what you are learning. As you are learning, think about your expectations, your feelings and your dreams and make sure you do\"},\n",
       " {'generated_text': 'Some NLP projects that force any junior to learn the necessary skils to one day be a great professional are still very limited.\\n\\nSome projects force a junior to learn the necessary Skils to be a great professional are still very limited.\\n\\nSome of the projects have a low success rate while others are more likely than not to see a decline.'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"Some NLP projects that force any junior to learn the necessary skils to one day be a great professional are\", num_return_sequences = 4, max_length = 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a574d5",
   "metadata": {},
   "source": [
    "##### **Text to audio**\n",
    "\n",
    "While there are HuggingFace Models for text to audio although not native to the pipeline API, we can use `pyttsx3`, a Python library to come up with audio from text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8549634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyttsx3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eba1385c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speak(text):\n",
    "    engine = pyttsx3.init()\n",
    "    engine.setProperty('rate', 140)\n",
    "    voices = engine.getProperty('voices')\n",
    "    engine.setProperty('voice',voices[0].id)\n",
    "    engine.setProperty('volume',0.8)\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54fa49a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "speak(\"Fun isn't something one considers when balancing the fate of the universe, but this; I'm going to enjoy very much!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5499cef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = pyttsx3.init()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}


.World War I[b] or the First World War (28 July 1914 – 11 November 1918), also known as the Great War, was a global conflict between two coalitions: the Allies (or Entente) and the Central Powers. Fighting took place mainly in Europe and the Middle East, as well as in parts of Africa and the Asia-Pacific, and in Europe was characterised by trench warfare; the widespread use of artillery, machine guns, and chemical weapons (gas); and the introductions of tanks and aircraft. World War I was one of the deadliest conflicts in history, resulting in an estimated 10 million military dead and more than 20 million wounded, plus some 10 million civilian dead from causes including genocide. The movement of large numbers of people was a major factor in the deadly Spanish flu pandemic.
.The causes of World War I included the rise of Germany and decline of the Ottoman Empire, which disturbed the long-standing balance of power in Europe, and rising economic competition between nations driven by industrialisation and imperialism. Growing tensions between the great powers and in the Balkans reached a breaking point on 28 June 1914, when Gavrilo Princip, a Bosnian Serb, assassinated the heir to the Austro-Hungarian throne. Austria-Hungary blamed Serbia, and declared war on 28 July. After Russia mobilised in Serbia's defence, Germany declared war on Russia and France, who had an alliance. The United Kingdom entered after Germany invaded Belgium, and the Ottomans joined the Central Powers in November. Germany's strategy in 1914 was to quickly defeat France then transfer its forces to the east, but its advance was halted in September, and by the end of the year the Western Front consisted of a near-continuous line of trenches from the English Channel to Switzerland. The Eastern Front was more dynamic, but neither side gained a decisive advantage, despite costly offensives. Italy, Bulgaria, Romania, Greece and others joined in from 1915 onward.
.Major battles, including at Verdun, the Somme, and Passchendaele, failed to break the stalemate on the Western Front. In April 1917, the United States joined the Allies after Germany resumed unrestricted submarine warfare against Atlantic shipping. Later that year, the Bolsheviks seized power in Russia in the October Revolution; Soviet Russia signed an armistice with the Central Powers in December, followed by a separate peace in March 1918. That month, Germany launched a spring offensive in the west, which despite initial successes left the German Army exhausted and demoralised. The Allied Hundred Days Offensive beginning in August 1918 caused a collapse of the German front line. Following the Vardar Offensive, Bulgaria signed an armistice in late September. By early November, the Ottoman Empire and Austria-Hungary had each signed armistices with the Allies, leaving Germany isolated. Facing a revolution at home, Kaiser Wilhelm II abdicated on 9 November, and the war ended with the Armistice of 11 November 1918.
.The Paris Peace Conference of 1919–1920 imposed settlements on the defeated powers, most notably the Treaty of Versailles, by which Germany lost significant territories, was disarmed, and was required to pay large war reparations to the Allies. The dissolution of the Russian, German, Austro-Hungarian, and Ottoman Empires redrew national boundaries and resulted in the creation of new independent states, including Poland, Finland, the Baltic states, Czechoslovakia, and Yugoslavia. The League of Nations was established to maintain world peace, but its failure to manage instability during the interwar period contributed to the outbreak of World War II in 1939.
.Before World War II, the events of 1914–1918 were generally known as the Great War or simply the World War.[1] In August 1914, the magazine The Independent wrote "This is the Great War. It names itself."[2] In October 1914, the Canadian magazine Maclean's similarly wrote, "Some wars name themselves. This is the Great War."[3] Contemporary Europeans also referred to it as "the war to end war" and it was also described as "the war to end all wars" due to their perception of its unparalleled scale, devastation, and loss of life.[4] The first recorded use of the term First World War was in September 1914 by German biologist and philosopher Ernst Haeckel who stated, "There is no doubt that the course and character of the feared 'European War' ... will become the first world war in the full sense of the word."[5]
.For much of the 19th century, the major European powers maintained a tenuous balance of power, known as the Concert of Europe.[6] After 1848, this was challenged by Britain's withdrawal into so-called splendid isolation, the decline of the Ottoman Empire, New Imperialism, and the rise of Prussia under Otto von Bismarck. Victory in the 1870–1871 Franco-Prussian War allowed Bismarck to consolidate a German Empire. Post-1871, the primary aim of French policy was to avenge this defeat,[7] but by the early 1890s, this had switched to the expansion of the French colonial empire.[8]
.In 1873, Bismarck negotiated the League of the Three Emperors, which included Austria-Hungary, Russia, and Germany. After the 1877–1878 Russo-Turkish War, the League was dissolved due to Austrian concerns over the expansion of Russian influence in the Balkans, an area they considered to be of vital strategic interest. Germany and Austria-Hungary then formed the 1879 Dual Alliance, which became the Triple Alliance when Italy joined in 1882.[9] For Bismarck, the purpose of these agreements was to isolate France by ensuring the three empires resolved any disputes among themselves. In 1887, Bismarck set up the Reinsurance Treaty, a secret agreement between Germany and Russia to remain neutral if either were attacked by France or Austria-Hungary.[10]
.For Bismarck, peace with Russia was the foundation of German foreign policy, but in 1890, he was forced to retire by Wilhelm II. The latter was persuaded not to renew the Reinsurance Treaty by his new Chancellor, Leo von Caprivi.[11] This gave France an opening to agree to the Franco-Russian Alliance in 1894, which was then followed by the 1904 Entente Cordiale with Britain. The Triple Entente was completed by the 1907 Anglo-Russian Convention. While not formal alliances, by settling longstanding colonial disputes in Asia and Africa, British support for France or Russia in any future conflict became a possibility.[12] This was accentuated by British and Russian support for France against Germany during the 1911 Agadir Crisis.[13]
.German economic and industrial strength continued to expand rapidly post-1871. Backed by Wilhelm II, Admiral Alfred von Tirpitz sought to use this growth to build an Imperial German Navy, that could compete with the British Royal Navy.[14] This policy was based on the work of US naval author Alfred Thayer Mahan, who argued that possession of a blue-water navy was vital for global power projection; Tirpitz had his books translated into German, while Wilhelm made them required reading for his advisors and senior military personnel.[15]
.However, it was also an emotional decision, driven by Wilhelm's simultaneous admiration for the Royal Navy and desire to surpass it. Bismarck thought that the British would not interfere in Europe, as long as its maritime supremacy remained secure, but his dismissal in 1890 led to a change in policy and an Anglo-German naval arms race began.[16] Despite the vast sums spent by Tirpitz, the launch of HMS Dreadnought in 1906 gave the British a technological advantage.[14] Ultimately, the race diverted huge resources into creating a German navy large enough to antagonise Britain, but not defeat it; in 1911, Chancellor Theobald von Bethmann Hollweg acknowledged defeat, leading to the Rüstungswende or 'armaments turning point', when he switched expenditure from the navy to the army.[17]
.This decision was not driven by a reduction in political tensions but by German concern over Russia's quick recovery from its defeat in the Russo-Japanese War and the subsequent 1905 Russian Revolution. Economic reforms led to a significant post-1908 expansion of railways and transportation infrastructure, particularly in its western border regions.[18] Since Germany and Austria-Hungary relied on faster mobilisation to compensate for their numerical inferiority compared to Russia, the threat posed by the closing of this gap was more important than competing with the Royal Navy. After Germany expanded its standing army by 170,000 troops in 1913, France extended compulsory military service from two to three years; similar measures were taken by the Balkan powers and Italy, which led to increased expenditure by the Ottomans and Austria-Hungary. Absolute figures are difficult to calculate due to differences in categorising expenditure since they often omit civilian infrastructure projects like railways which had logistical importance and military use. It is known, however, that from 1908 to 1913, military spending by the six major European powers increased by over 50% in real terms.[19]
.The years before 1914 were marked by a series of crises in the Balkans, as other powers sought to benefit from the Ottoman decline. While Pan-Slavic and Orthodox Russia considered itself the protector of Serbia and other Slav states, they preferred the strategically vital Bosporus straits to be controlled by a weak Ottoman government, rather than an ambitious Slav power like Bulgaria. Russia had ambitions in northeastern Anatolia while its clients had overlapping claims in the Balkans. These competing interests divided Russian policy-makers and added to regional instability.[20]
.Austrian statesmen viewed the Balkans as essential for the continued existence of their Empire and saw Serbian expansion as a direct threat. The 1908–1909 Bosnian Crisis began when Austria annexed the former Ottoman territory of Bosnia and Herzegovina, which it had occupied since 1878. Timed to coincide with the Bulgarian Declaration of Independence from the Ottoman Empire, this unilateral action was denounced by the European powers, but accepted as there was no consensus on how to resolve the situation. Some historians see this as a significant escalation, ending any chance of Austria cooperating with Russia in the Balkans, while also damaging diplomatic relations between Serbia and Italy.[21]
.Tensions increased after the 1911–1912 Italo-Turkish War demonstrated Ottoman weakness and led to the formation of the Balkan League, an alliance of Serbia, Bulgaria, Montenegro, and Greece.[22] The League quickly overran most of the Ottomans' territory in the Balkans during the 1912–1913 First Balkan War, much to the surprise of outside observers.[23] The Serbian capture of ports on the Adriatic resulted in partial Austrian mobilisation, starting on 21 November 1912, including units along the Russian border in Galicia. The Russian government decided not to mobilise in response, unprepared to precipitate a war.[24]
.The Great Powers sought to re-assert control through the 1913 Treaty of London, which had created an independent Albania while enlarging the territories of Bulgaria, Serbia, Montenegro and Greece. However, disputes between the victors sparked the 33-day Second Balkan War, when Bulgaria attacked Serbia and Greece on 16 June 1913; it was defeated, losing most of Macedonia to Serbia and Greece, and Southern Dobruja to Romania.[25] The result was that even countries which benefited from the Balkan Wars, such as Serbia and Greece, felt cheated of their "rightful gains", while for Austria it demonstrated the apparent indifference with which other powers viewed their concerns, including Germany.[26] This complex mix of resentment, nationalism and insecurity helps explain why the pre-1914 Balkans became known as the "powder keg of Europe".[27][28][29][30]
.On 28 June 1914, Archduke Franz Ferdinand of Austria, heir presumptive to Emperor Franz Joseph I of Austria, visited Sarajevo, the capital of the recently annexed Bosnia and Herzegovina. Cvjetko Popović, Gavrilo Princip, Nedeljko Čabrinović, Trifko Grabež, Vaso Čubrilović (Bosnian Serbs) and Muhamed Mehmedbašić (from the Bosniaks community),[33] from the movement known as Young Bosnia, took up positions along the Archduke's motorcade route, to assassinate him. Supplied with arms by extremists within the Serbian Black Hand intelligence organisation, they hoped his death would free Bosnia from Austrian rule.[34]
.Čabrinović threw a grenade at the Archduke's car and injured two of his aides. The other assassins were also unsuccessful. An hour later, as Ferdinand was returning from visiting the injured officers in hospital, his car took a wrong turn into a street where Gavrilo Princip was standing. He fired two pistol shots, fatally wounding Ferdinand and his wife Sophie.[35]
.According to historian Zbyněk Zeman, in Vienna "the event almost failed to make any impression whatsoever. On 28 and 29 June, the crowds listened to music and drank wine, as if nothing had happened."[36] Nevertheless, the impact of the murder of the heir to the throne was significant, and has been described by historian Christopher Clark as a "9/11 effect, a terrorist event charged with historic meaning, transforming the political chemistry in Vienna".[37]
.Austro-Hungarian authorities encouraged subsequent anti-Serb riots in Sarajevo.[38][39] Violent actions against ethnic Serbs were also organised outside Sarajevo, in other cities in Austro-Hungarian-controlled Bosnia and Herzegovina, Croatia and Slovenia. Austro-Hungarian authorities in Bosnia and Herzegovina imprisoned approximately 5,500 prominent Serbs, 700 to 2,200 of whom died in prison. A further 460 Serbs were sentenced to death. A predominantly Bosniak special militia known as the Schutzkorps was established, and carried out the persecution of Serbs.[40][41][42][43]
.The assassination initiated the July Crisis, a month of diplomatic manoeuvring among Austria-Hungary, Germany, Russia, France and Britain. Believing that Serbian intelligence helped organise Franz Ferdinand's murder, Austrian officials wanted to use the opportunity to end Serbian interference in Bosnia and saw war as the best way of achieving this.[44] However, the Foreign Ministry had no solid proof of Serbian involvement.[45] On 23 July, Austria delivered an ultimatum to Serbia, listing ten demands made intentionally unacceptable to provide an excuse for starting hostilities.[46]
.Serbia ordered general mobilisation on 25 July, but accepted all the terms, except for those empowering Austrian representatives to suppress "subversive elements" inside Serbia, and take part in the investigation and trial of Serbians linked to the assassination.[47][48] Claiming this amounted to rejection, Austria broke off diplomatic relations and ordered partial mobilisation the next day; on 28 July, they declared war on Serbia and began shelling Belgrade. Russia ordered general mobilisation in support of Serbia on 30 July.[49]
.Anxious to ensure backing from the SPD political opposition by presenting Russia as the aggressor, German Chancellor Bethmann Hollweg delayed the commencement of war preparations until 31 July.[50] That afternoon, the Russian government were handed a note requiring them to "cease all war measures against Germany and Austria-Hungary" within 12 hours.[51] A further German demand for neutrality was refused by the French who ordered general mobilisation but delayed declaring war.[52] The German General Staff had long assumed they faced a war on two fronts; the Schlieffen Plan envisaged using 80% of the army to defeat France, then switching to Russia. Since this required them to move quickly, mobilisation orders were issued that afternoon.[53] Once the German ultimatum to Russia expired on the morning of 1 August, the two countries were at war.
.At a meeting on 29 July, the British cabinet had narrowly decided its obligations to Belgium under the 1839 Treaty of London did not require it to oppose a German invasion with military force; however, Prime Minister Asquith and his senior Cabinet ministers were already committed to supporting France, the Royal Navy had been mobilised, and public opinion was strongly in favour of intervention.[54] On 31 July, Britain sent notes to Germany and France, asking them to respect Belgian neutrality; France pledged to do so, but Germany did not reply.[55] Aware of German plans to attack through Belgium, French Commander-in-Chief Joseph Joffre asked his government for permission to cross the border and pre-empt such a move. To avoid violating Belgian neutrality, he was told any advance could come only after a German invasion.[56] Instead, the French cabinet ordered its Army to withdraw 10 km behind the German frontier, to avoid provoking war. On 2 August, Germany occupied Luxembourg and exchanged fire with French units when German patrols entered French territory; on 3 August, they declared war on France and demanded free passage across Belgium, which was refused. Early on the morning of 4 August, the Germans invaded, and Albert I of Belgium called for assistance under the Treaty of London.[57][58] Britain sent Germany an ultimatum demanding they withdraw from Belgium; when this expired at midnight, without a response, the two empires were at war.[59]
.Germany promised to support Austria-Hungary's invasion of Serbia, but interpretations of what this meant differed. Previously tested deployment plans had been replaced early in 1914, but those had never been tested in exercises. Austro-Hungarian leaders believed Germany would cover its northern flank against Russia.[60]
.Beginning on 12 August, the Austrians and Serbs clashed at the battles of the Cer and Kolubara; over the next two weeks, Austrian attacks were repulsed with heavy losses. As a result, Austria had to keep sizeable forces on the Serbian front, weakening their efforts against Russia.[61] Serbia's victory against Austria-Hungary in the 1914 invasion has been called one of the major upset victories of the twentieth century.[62] In 1915, the campaign saw the first use of anti-aircraft warfare after an Austrian plane was shot down with ground-to-air fire, as well as the first medical evacuation by the Serbian army.[63][64]
.Upon mobilisation, in accordance with the Schlieffen Plan, 80% of the German Army was located on the Western Front, with the remainder acting as a screening force in the East. Rather than a direct attack across their shared frontier, the German right wing would sweep through the Netherlands and Belgium, then swing south, encircling Paris and trapping the French army against the Swiss border. The plan's creator, Alfred von Schlieffen, head of the German General Staff from 1891 to 1906, estimated that this would take six weeks, after which the German army would transfer to the East and defeat the Russians.[65]
.The plan was substantially modified by his successor, Helmuth von Moltke the Younger. Under Schlieffen, 85% of German forces in the west were assigned to the right wing, with the remainder holding along the frontier. By keeping his left-wing deliberately weak, he hoped to lure the French into an offensive into the "lost provinces" of Alsace-Lorraine, which was the strategy envisaged by their Plan XVII.[65] However, Moltke grew concerned that the French might push too hard on his left flank and as the German Army increased in size from 1908 to 1914, he changed the allocation of forces between the two wings to 70:30.[66] He also considered Dutch neutrality essential for German trade and cancelled the incursion into the Netherlands, which meant any delays in Belgium threatened the viability of the plan.[67] Historian Richard Holmes argues that these changes meant the right wing was not strong enough to achieve decisive success.[68]
.The initial German advance in the West was very successful. By the end of August, the Allied left, which included the British Expeditionary Force (BEF), was in full retreat, and the French offensive in Alsace-Lorraine was a disastrous failure, with casualties exceeding 260,000.[69] German planning provided broad strategic instructions while allowing army commanders considerable freedom in carrying them out at the front, but Alexander von Kluck used this freedom to disobey orders, opening a gap between the German armies as they closed on Paris.[70] The French army, reinforced by the British expeditionary corps, seized this opportunity to counter-attack and pushed the German army 40 to 80 km back. Both armies were then so exhausted that no decisive move could be implemented, so they settled in trenches, with the vain hope of breaking through as soon as they could build local superiority.
.In 1911, the Russian Stavka agreed with the French to attack Germany within fifteen days of mobilisation, ten days before the Germans had anticipated, although it meant the two Russian armies that entered East Prussia on 17 August did so without many of their support elements.[71]
.By the end of 1914, German troops held strong defensive positions inside France, controlled the bulk of France's domestic coalfields, and inflicted 230,000 more casualties than it lost itself. However, communications problems and questionable command decisions cost Germany the chance of a decisive outcome, while it had failed to achieve the primary objective of avoiding a long, two-front war.[72] As was apparent to several German leaders, this amounted to a strategic defeat; shortly after the First Battle of the Marne, Crown Prince Wilhelm told an American reporter "We have lost the war. It will go on for a long time but lost it is already."[73]
.On 30 August 1914, New Zealand occupied German Samoa (now Samoa). On 11 September, the Australian Naval and Military Expeditionary Force landed on the island of New Britain, then part of German New Guinea. On 28 October, the German cruiser SMS Emden sank the Russian cruiser Zhemchug in the Battle of Penang. Japan declared war on Germany before seizing territories in the Pacific, which later became the South Seas Mandate, as well as German Treaty ports on the Chinese Shandong peninsula at Tsingtao. After Vienna refused to withdraw its cruiser SMS Kaiserin Elisabeth from Tsingtao, Japan declared war on Austria-Hungary, and the ship was sunk in November 1914.[74] Within a few months, Allied forces had seized all German territories in the Pacific, leaving only isolated commerce raiders and a few holdouts in New Guinea.[75][76]
.Some of the first clashes of the war involved British, French, and German colonial forces in Africa. On 6–7 August, French and British troops invaded the German protectorates of Togoland and Kamerun. On 10 August, German forces in South-West Africa attacked South Africa; sporadic and fierce fighting continued for the rest of the war. The German colonial forces in German East Africa, led by Colonel Paul von Lettow-Vorbeck, fought a guerrilla warfare campaign and only surrendered two weeks after the armistice took effect in Europe.[77]
.Before the war, Germany had attempted to use Indian nationalism and pan-Islamism to its advantage, a policy continued post-1914 by instigating uprisings in India, while the Niedermayer–Hentig Expedition urged Afghanistan to join the war on the side of Central Powers. However, contrary to British fears of a revolt in India, the outbreak of the war saw a reduction in nationalist activity.[78][79] Leaders from the Indian National Congress and other groups believed support for the British war effort would hasten Indian Home Rule, a promise allegedly made explicit in 1917 by Edwin Montagu, the Secretary of State for India.[80]
.In 1914, the British Indian Army was larger than the British Army itself, and between 1914 and 1918 an estimated 1.3 million Indian soldiers and labourers served in Europe, Africa, and the Middle East. In all, 140,000 soldiers served on the Western Front and nearly 700,000 in the Middle East, with 47,746 killed and 65,126 wounded.[81] The suffering engendered by the war, as well as the failure of the British government to grant self-government to India afterward, bred disillusionment, resulting in the campaign for full independence led by Mahatma Gandhi.[82]
.Pre-war military tactics that had emphasised open warfare and individual riflemen proved obsolete when confronted with conditions prevailing in 1914. Technological advances allowed the creation of strong defensive systems largely impervious to massed infantry advances, such as barbed wire, machine guns and above all far more powerful artillery, which dominated the battlefield and made crossing open ground extremely difficult.[83] Both sides struggled to develop tactics for breaching entrenched positions without heavy casualties. In time, technology enabled the production of new offensive weapons, such as gas warfare and the tank.[84]
.After the First Battle of the Marne in September 1914, Allied and German forces unsuccessfully tried to outflank each other, a series of manoeuvres later known as the "Race to the Sea". By the end of 1914, the opposing forces confronted each other along an uninterrupted line of entrenched positions from the Channel to the Swiss border.[85] Since the Germans were normally able to choose where to stand, they generally held the high ground, while their trenches tended to be better built; those constructed by the French and English were initially considered "temporary", only needed until an offensive would destroy the German defences.[86] Both sides tried to break the stalemate using scientific and technological advances. On 22 April 1915, at the Second Battle of Ypres, the Germans (violating the Hague Convention) used chlorine gas for the first time on the Western Front. Several types of gas soon became widely used by both sides and though it never proved a decisive, battle-winning weapon, it became one of the most feared and best-remembered horrors of the war.[87][88]
.In February 1916, the Germans attacked French defensive positions at the Battle of Verdun, lasting until December 1916. Casualties were greater for the French, but the Germans bled heavily as well, with anywhere from 700,000[89] to 975,000[90] casualties between the two combatants. Verdun became a symbol of French determination and self-sacrifice.[91]
.The Battle of the Somme was an Anglo-French offensive from July to November 1916. The opening day, on 1 July 1916, was the bloodiest single day in the history of the British Army, which suffered 57,500 casualties, including 19,200 dead. As a whole, the Somme offensive led to an estimated 420,000 British casualties, along with 200,000 French and 500,000 Germans.[92] The diseases that emerged in the trenches were a major killer on both sides. The living conditions led to disease and infection, such as trench foot, lice, typhus, trench fever, and the 'Spanish flu'.[93]
.At the start of the war, German cruisers were scattered across the globe, some of which were subsequently used to attack Allied merchant shipping. These were systematically hunted down by the Royal Navy, though not before causing considerable damage. One of the most successful was the SMS Emden, part of the German East Asia Squadron stationed at Qingdao, which seized or sank 15 merchantmen, a Russian cruiser and a French destroyer. Most of the squadron was returning to Germany when it sank two British armoured cruisers at the Battle of Coronel in November 1914, before being virtually destroyed at the Battle of the Falkland Islands in December. The SMS Dresden escaped with a few auxiliaries, but after the Battle of Más a Tierra, these too were either destroyed or interned.[94]
.Soon after the outbreak of hostilities, Britain began a naval blockade of Germany. This proved effective in cutting off vital supplies, though it violated accepted international law.[95] Britain also mined international waters which closed off entire sections of the ocean, even to neutral ships.[96] Since there was limited response to this tactic, Germany expected a similar response to its unrestricted submarine warfare.[97]
.The Battle of Jutland[d] in May/June 1916 was the only full-scale clash of battleships during the war, and one of the largest in history. The clash was indecisive, though the Germans inflicted more damage than they received; thereafter the bulk of the German High Seas Fleet was confined to port.[98]
.German U-boats attempted to cut the supply lines between North America and Britain.[99] The nature of submarine warfare meant that attacks often came without warning, giving the crews of the merchant ships little hope of survival.[99][100] The United States launched a protest, and Germany changed its rules of engagement. After the sinking of the passenger ship RMS Lusitania in 1915, Germany promised not to target passenger liners, while Britain armed its merchant ships, placing them beyond the protection of the "cruiser rules", which demanded warning and movement of crews to "a place of safety" (a standard that lifeboats did not meet).[101] Finally, in early 1917, Germany adopted a policy of unrestricted submarine warfare, realising the Americans would eventually enter the war.[99][102] Germany sought to strangle Allied sea lanes before the United States could transport a large army overseas, but, after initial successes, eventually failed to do so.[99]
.The U-boat threat lessened in 1917, when merchant ships began travelling in convoys, escorted by destroyers. This tactic made it difficult for U-boats to find targets, which significantly lessened losses; after the hydrophone and depth charges were introduced, destroyers could potentially successfully attack a submerged submarine. Convoys slowed the flow of supplies since ships had to wait as convoys were assembled; the solution was an extensive program of building new freighters. Troopships were too fast for the submarines and did not travel the North Atlantic in convoys.[103] The U-boats sunk more than 5,000 Allied ships, at the cost of 199 submarines.[104]
.World War I also saw the first use of aircraft carriers in combat, with HMS Furious launching Sopwith Camels in a successful raid against the Zeppelin hangars at Tondern in July 1918, as well as blimps for antisubmarine patrol.[105]
.Faced with Russia in the east, Austria-Hungary could spare only one-third of its army to attack Serbia. After suffering heavy losses, the Austrians briefly occupied the Serbian capital, Belgrade. A Serbian counter-attack in the Battle of Kolubara succeeded in driving them from the country by the end of 1914. For the first 10 months of 1915, Austria-Hungary used most of its military reserves to fight Italy. German and Austro-Hungarian diplomats scored a coup by persuading Bulgaria to join the attack on Serbia.[106] The Austro-Hungarian provinces of Slovenia, Croatia and Bosnia provided troops for Austria-Hungary. Montenegro allied itself with Serbia.[107]
.Bulgaria declared war on Serbia on 14 October 1915, and joined in the attack by the Austro-Hungarian army under Mackensen's army of 250,000 that was already underway. Serbia was conquered in a little more than a month, as the Central Powers, now including Bulgaria, sent in 600,000 troops in total. The Serbian army, fighting on two fronts and facing certain defeat, retreated into northern Albania. The Serbs suffered defeat in the Battle of Kosovo. Montenegro covered the Serbian retreat toward the Adriatic coast in the Battle of Mojkovac on 6–7 January 1916, but ultimately the Austrians also conquered Montenegro. The surviving Serbian soldiers were evacuated to Greece.[108] After the conquest, Serbia was divided between Austro-Hungary and Bulgaria.[109]
.In late 1915, a Franco-British force landed at Salonica in Greece to offer assistance and to pressure its government to declare war against the Central Powers. However, the pro-German King Constantine I dismissed the pro-Allied government of Eleftherios Venizelos before the Allied expeditionary force arrived.[110]
.The Macedonian front was at first mostly static. French and Serbian forces retook limited areas of Macedonia by recapturing Bitola on 19 November 1916, following the costly Monastir offensive, which brought stabilisation of the front.[111]
.Serbian and French troops finally made a breakthrough in September 1918 in the Vardar offensive, after most German and Austro-Hungarian troops had been withdrawn. The Bulgarians were defeated at the Battle of Dobro Pole, and by 25 September British and French troops had crossed the border into Bulgaria proper as the Bulgarian army collapsed. Bulgaria capitulated four days later, on 29 September 1918.[113] The German high command responded by despatching troops to hold the line, but these forces were too weak to re-establish a front.[114]
.The Allied breakthrough on the Macedonian front cut communications between the Ottoman Empire and the other Central Powers, and made Vienna vulnerable to attack. Hindenburg and Ludendorff concluded that the strategic and operational balance had now shifted decidedly against the Central Powers and, a day after the Bulgarian collapse, insisted on an immediate peace settlement.[115]
.The Ottomans threatened Russia's Caucasian territories and Britain's communications with India via the Suez Canal. The Ottoman Empire took advantage of the European powers' preoccupation with the war and conducted large-scale ethnic cleansing of the Armenian, Greek, and Assyrian Christian populations—the Armenian genocide, Greek genocide, and Assyrian genocide respectively.[116][117]
[118]
.The British and French opened overseas fronts with the Gallipoli (1915) and Mesopotamian campaigns (1914). In Gallipoli, the Ottoman Empire successfully repelled the British, French, and Australian and New Zealand Army Corps (ANZACs). In Mesopotamia, by contrast, after the defeat of the British defenders in the siege of Kut by the Ottomans (1915–1916), British Imperial forces reorganised and captured Baghdad in March 1917. The British were aided in Mesopotamia by local Arab and Assyrian fighters, while the Ottomans employed local Kurdish and Turcoman tribes.[119]
.The Suez Canal was defended from Ottoman attacks in 1915 and 1916; in August 1916, a German and Ottoman force was defeated at the Battle of Romani by the ANZAC Mounted Division and the 52nd (Lowland) Infantry Division. Following this victory, an Egyptian Expeditionary Force advanced across the Sinai Peninsula, pushing Ottoman forces back in the Battle of Magdhaba in December and the Battle of Rafa on the border between the Egyptian Sinai and Ottoman Palestine in January 1917.[120]
.Russian armies generally had success in the Caucasus campaign. Enver Pasha, supreme commander of the Ottoman armed forces, dreamed of re-conquering central Asia and areas that had been previously lost to Russia. He was, however, a poor commander.[121] He launched an offensive against the Russians in the Caucasus in December 1914 with 100,000 troops, insisting on a frontal attack against mountainous Russian positions in winter. He lost 86% of his force at the Battle of Sarikamish.[122] General Nikolai Yudenich, the Russian commander from 1915 to 1916, drove the Turks out of most of the southern Caucasus.[122]
.The Ottoman Empire, with German support, invaded Persia (modern Iran) in December 1914 to cut off British and Russian access to petroleum reservoirs around Baku.[123] Persia, ostensibly neutral, had long been under British and Russian influence. The Ottomans and Germans were aided by Kurdish and Azeri forces, together with a large number of major Iranian tribes, while the Russians and British had the support of Armenian and Assyrian forces. The Persian campaign lasted until 1918 and ended in failure for the Ottomans and their allies. However, the Russian withdrawal from the war in 1917 led Armenian and Assyrian forces to be cut off from supply lines, outnumbered, outgunned and isolated, forcing them to fight and flee towards British lines in northern Mesopotamia.[124]
.The Arab Revolt, instigated by the British Foreign Office, started in June 1916 with the Battle of Mecca, led by Sharif Hussein. The Sharif declared the independence of the Kingdom of Hejaz and, with British assistance, conquered much of Ottoman-held Arabia, resulting finally in the Ottoman surrender of Damascus. Fakhri Pasha, the Ottoman commander of Medina, resisted for more than 2+1⁄2 years during the siege of Medina before surrendering in January 1919.[125]
.The Senussi tribe, along the border of Italian Libya and British Egypt, incited and armed by the Turks, waged a small-scale guerrilla war against Allied troops. The British were forced to dispatch 12,000 troops to oppose them in the Senussi campaign. Their rebellion was finally crushed in mid-1916.[126]
.Total Allied casualties on the Ottoman fronts amounted to 650,000 men. Total Ottoman casualties were 725,000, with 325,000 dead and 400,000 wounded.[127]
.Though Italy joined the Triple Alliance in 1882, a treaty with its traditional Austrian enemy was so controversial that subsequent governments denied its existence and the terms were only made public in 1915.[128] This arose from nationalist designs on Austro-Hungarian territory in Trentino, the Austrian Littoral, Rijeka and Dalmatia, considered vital to secure the borders established in 1866.[129] In 1902, Rome secretly had agreed with France to remain neutral if the latter was attacked by Germany, effectively nullifying its role in the Triple Alliance.[130]
.
When the war began in 1914, Italy argued the Triple Alliance was defensive and it was not obliged to support an Austrian attack on Serbia. Opposition to joining the Central Powers increased when Turkey became a member in September, since in 1911 Italy had occupied Ottoman possessions in Libya and the Dodecanese islands.[131] To secure Italian neutrality, the Central Powers offered them Tunisia, while in return for an immediate entry into the war, the Allies agreed to their demands for Austrian territory and sovereignty over the Dodecanese.[132] Although they remained secret, these provisions were incorporated into the April 1915 Treaty of London; Italy joined the Triple Entente and, on 23 May, declared war on Austria-Hungary,[133] followed by Germany fifteen months later..The pre-1914 Italian army was short of officers, trained men, adequate transport and modern weapons; by April 1915, some of these deficiencies had been remedied but it was still unprepared for the major offensive required by the Treaty of London.[134] The advantage of superior numbers was offset by the difficult terrain; much of the fighting took place high in the Alps and Dolomites, where trench lines had to be cut through rock and ice and keeping troops supplied was a major challenge. These issues were exacerbated by unimaginative strategies and tactics.[135] Between 1915 and 1917, the Italian commander, Luigi Cadorna, undertook a series of frontal assaults along the Isonzo, which made little progress and cost many lives; by the end of the war, Italian combat deaths totalled around 548,000.[136]
.In the spring of 1916, the Austro-Hungarians counterattacked in Asiago in the Strafexpedition, but made little progress and were pushed by the Italians back to Tyrol.[137] Although Italy occupied southern Albania in May 1916, their main focus was the Isonzo front which, after the capture of Gorizia in August 1916, remained static until October 1917. After a combined Austro-German force won a major victory at Caporetto, Cadorna was replaced by Armando Diaz who retreated more than 100 kilometres (62 mi) before holding positions along the Piave River.[138] A second Austrian offensive was repulsed in June 1918. On 24 October, Diaz launched the Battle of Vittorio Veneto and initially met stubborn resistance,[139] but with Austria-Hungary collapsing, Hungarian divisions in Italy demanded they be sent home.[140] When this was granted, many others followed and the Imperial army disintegrated, the Italians taking over 300,000 prisoners.[141] On 3 November, the Armistice of Villa Giusti ended hostilities between Austria-Hungary and Italy which occupied Trieste and areas along the Adriatic Sea awarded to it in 1915.[142]
.As previously agreed with French president Raymond Poincaré, Russian plans at the start of the war were to simultaneously advance into Austrian Galicia and East Prussia as soon as possible. Although their attack on Galicia was largely successful, and the invasions achieved their aim of forcing Germany to divert troops from the Western Front, the speed of mobilisation meant they did so without much of their heavy equipment and support functions. These weaknesses contributed to Russian defeats at Tannenberg and the Masurian Lakes in August and September 1914, forcing them to withdraw from East Prussia with heavy losses.[143][144] By spring 1915, they had also retreated from Galicia, and the May 1915 Gorlice–Tarnów offensive allowed the Central Powers to invade Russian-occupied Poland.[145]
.Despite the successful June 1916 Brusilov offensive against the Austrians in eastern Galicia,[146] shortages of supplies, heavy losses and command failures prevented the Russians from fully exploiting their victory. However, it was one of the most significant offensives of the war, diverting German resources from Verdun, relieving Austro-Hungarian pressure on the Italians, and convincing Romania to enter the war on the side of the Allies on 27 August. It also fatally weakened both the Austrian and Russian armies, whose offensive capabilities were badly affected by their losses and increased disillusion with the war that ultimately led to the Russian revolutions.[147]
.Meanwhile, unrest grew in Russia as Tsar Nicholas II remained at the front, with the home front controlled by Empress Alexandra. Her increasingly incompetent rule and food shortages in urban areas led to widespread protests and the murder of her favourite, Grigori Rasputin, at the end of 1916.[148]
.Despite secretly agreeing to support the Triple Alliance in 1883, Romania increasingly found itself at odds with the Central Powers over their support for Bulgaria in the Balkan Wars and the status of ethnic Romanian communities in Hungarian-controlled Transylvania,[149] which comprised an estimated 2.8 million of the region's 5.0 million population.[150] With the ruling elite split into pro-German and pro-Entente factions,[151] Romania remained neutral for two years while allowing Germany and Austria to transport military supplies and advisors across Romanian territory.[152]
.In September 1914, Russia acknowledged Romanian rights to Austro-Hungarian territories including Transylvania and Banat, whose acquisition had widespread popular support,[150] and Russian success against Austria led Romania to join the Entente in the August 1916 Treaty of Bucharest.[152] Under the strategic plan known as Hypothesis Z, the Romanian army planned an offensive into Transylvania, while defending Southern Dobruja and Giurgiu against a possible Bulgarian counterattack.[153] On 27 August 1916, they attacked Transylvania and occupied substantial parts of the province before being driven back by the recently formed German 9th Army, led by former Chief of Staff Erich von Falkenhayn.[154] A combined German-Bulgarian-Turkish offensive captured Dobruja and Giurgiu, although the bulk of the Romanian army managed to escape encirclement and retreated to Bucharest, which surrendered to the Central Powers on 6 December 1916.[155]
.In the summer of 1917, a Central Powers offensive began in Romania under the command of August von Mackensen to knock Romania out of the war, resulting in the battles of Oituz, Mărăști and Mărășești, where up to 1,000,000 Central Powers troops were present. The battles lasted from 22 July to 3 September and eventually, the Romanian army was victorious advancing 500 km2. August von Mackensen could not plan for another offensive as he had to transfer troops to the Italian Front.[156] Following the Russian revolution, Romania found itself alone on the Eastern Front and signed the Treaty of Bucharest with the Central Powers, which recognised Romanian sovereignty over Bessarabia in return for ceding control of passes in the Carpathian Mountains to Austria-Hungary and leasing its oil wells to Germany. Although approved by Parliament, King Ferdinand I refused to sign it, hoping for an Allied victory in the west.[157] Romania re-entered the war on 10 November 1918, on the side of the Allies and the Treaty of Bucharest was formally annulled by the Armistice of 11 November 1918.[158][e]
.On 12 December 1916, after ten brutal months of the Battle of Verdun and a successful offensive against Romania, Germany attempted to negotiate a peace with the Allies.[160] However, this attempt was rejected out of hand as a "duplicitous war ruse".[160]
.US president Woodrow Wilson attempted to intervene as a peacemaker, asking for both sides to state their demands. Lloyd George's War Cabinet considered the German offer to be a ploy to create divisions among the Allies. After initial outrage and much deliberation, they took Wilson's note as a separate effort, signalling that the US was on the verge of entering the war against Germany following the "submarine outrages". While the Allies debated a response to Wilson's offer, the Germans chose to rebuff it in favour of "a direct exchange of views". Learning of the German response, the Allied governments were free to make clear demands in their response of 14 January. They sought restoration of damages, the evacuation of occupied territories, reparations for France, Russia and Romania, and a recognition of the principle of nationalities.[161] The Allies sought guarantees that would prevent or limit future wars.[162] The negotiations failed and the Entente powers rejected the German offer on the grounds of honour, and noted Germany had not put forward any specific proposals.[160]
.By the end of 1916, Russian casualties totalled nearly five million killed, wounded or captured, with major urban areas affected by food shortages and high prices. In March 1917, Tsar Nicholas ordered the military to forcibly suppress strikes in Petrograd but the troops refused to fire on the crowds.[163] Revolutionaries set up the Petrograd Soviet and fearing a left-wing takeover, the State Duma forced Nicholas to abdicate and established the Russian Provisional Government, which confirmed Russia's willingness to continue the war. However, the Petrograd Soviet refused to disband, creating competing power centres and causing confusion and chaos, with frontline soldiers becoming increasingly demoralised.[164]
.Following the tsar's abdication, Vladimir Lenin—with the help of the German government—was ushered from Switzerland into Russia on 16 April 1917. Discontent and the weaknesses of the Provisional Government led to a rise in the popularity of the Bolshevik Party, led by Lenin, which demanded an immediate end to the war. The Revolution of November was followed in December by an armistice and negotiations with Germany. At first, the Bolsheviks refused the German terms, but when German troops began marching across Ukraine unopposed, they acceded to the Treaty of Brest-Litovsk on 3 March 1918. The treaty ceded vast territories, including Finland, Estonia, Latvia, Lithuania, and parts of Poland and Ukraine to the Central Powers.[165]
.With the Russian Empire out of the war, Romania found itself alone on the Eastern Front and signed the Treaty of Bucharest with the Central Powers in May 1918. Under the terms of the treaty, Romania ceded territory to Austria-Hungary and Bulgaria and leased its oil reserves to Germany. However, the terms also included the Central Powers' recognition of the union of Bessarabia with Romania.[166][157]
.The United States was a major supplier of war material to the Allies but remained neutral in 1914, in large part due to domestic opposition.[167] The most significant factor in creating the support Wilson needed was the German submarine offensive, which not only cost American lives but paralysed trade as ships were reluctant to put to sea.[168]
.On 6 April 1917, Congress declared war on Germany as an "Associated Power" of the Allies.[169] The US Navy sent a battleship group to Scapa Flow to join the Grand Fleet, and provided convoy escorts. In April 1917, the US Army had fewer than 300,000 men, including National Guard units, compared to British and French armies of 4.1 and 8.3 million respectively. The Selective Service Act of 1917 drafted 2.8 million men, though training and equipping such numbers was a huge logistical challenge. By June 1918, over 667,000 members of the American Expeditionary Forces (AEF) were transported to France, a figure which reached 2 million by the end of November.[170]
.Despite his conviction that Germany must be defeated, Wilson went to war to ensure the US played a leading role in shaping the peace, which meant preserving the AEF as a separate military force, rather than being absorbed into British or French units as his Allies wanted.[171] He was strongly supported by AEF commander General John J. Pershing, a proponent of pre-1914 "open warfare" who considered the French and British emphasis on artillery misguided and incompatible with American "offensive spirit".[172] Much to the frustration of his Allies, who had suffered heavy losses in 1917, he insisted on retaining control of American troops, and refused to commit them to the front line until able to operate as independent units. As a result, the first significant US involvement was the Meuse–Argonne offensive in late September 1918.[173]
.In December 1916, Robert Nivelle replaced Pétain as commander of French armies on the Western Front and began planning a spring attack in Champagne, part of a joint Franco-British operation.[174] Poor security meant German intelligence was well informed on tactics and timetables, but despite this, when the attack began on 16 April the French made substantial gains, before being brought to a halt by the newly built and extremely strong defences of the Hindenburg Line. Nivelle persisted with frontal assaults and, by 25 April, the French had suffered nearly 135,000 casualties, including 30,000 dead, most incurred in the first two days.[175]
.Concurrent British attacks at Arras were more successful, though ultimately of little strategic value.[176] Operating as a separate unit for the first time, the Canadian Corps' capture of Vimy Ridge is viewed by many Canadians as a defining moment in creating a sense of national identity.[177][178] Though Nivelle continued the offensive, on 3 May the 21st Division, which had been involved in some of the heaviest fighting at Verdun, refused orders to go into battle, initiating the French Army mutinies; within days, "collective indiscipline" had spread to 54 divisions, while over 20,000 deserted.[179]
.In March and April 1917, at the First and Second Battles of Gaza, German and Ottoman forces stopped the advance of the Egyptian Expeditionary Force, which had begun in August 1916 at the Battle of Romani.[180][181] At the end of October 1917, the Sinai and Palestine campaign resumed, when General Edmund Allenby's XXth Corps, XXI Corps and Desert Mounted Corps won the Battle of Beersheba.[182] Two Ottoman armies were defeated a few weeks later at the Battle of Mughar Ridge and, early in December, Jerusalem had been captured following another Ottoman defeat at the Battle of Jerusalem.[183][184][185]
.About this time, Friedrich Freiherr Kress von Kressenstein was relieved of his duties as the Eighth Army's commander, replaced by Djevad Pasha, and a few months later the commander of the Ottoman Army in Palestine, Erich von Falkenhayn, was replaced by Otto Liman von Sanders.[186][187]
.In early 1918, the front line was extended and the Jordan Valley was occupied, following the First Transjordan and the Second Transjordan attacks by British Empire forces in March and April 1918.[188]
.In December 1917, the Central Powers signed an armistice with Russia, thus freeing large numbers of German troops for use in the West. With German reinforcements and new American troops pouring in, the outcome was to be decided on the Western Front. The Central Powers knew that they could not win a protracted war, but they held high hopes for success in a final quick offensive.[190] Ludendorff drew up plans (Operation Michael) for the 1918 offensive on the Western Front. The operation commenced on 21 March 1918, with an attack on British forces near Saint-Quentin. German forces achieved an unprecedented advance of 60 kilometres (37 mi).[191] The initial offensive was a success; after heavy fighting, however, the offensive was halted. Lacking tanks or motorised artillery, the Germans were unable to consolidate their gains. The problems of re-supply were also exacerbated by increasing distances that now stretched over terrain that was shell-torn and often impassable to traffic.[192]
Germany launched Operation Georgette against the northern English Channel ports. The Allies halted the drive after limited territorial gains by Germany. The German Army to the south then conducted Operations Blücher and Yorck, pushing broadly towards Paris. Germany launched Operation Marne (Second Battle of the Marne) on 15 July, in an attempt to encircle Reims. The resulting counter-attack, which started the Hundred Days Offensive on 8 August,[193] led to a marked collapse in German morale.[194][195][196]
.By September, the Germans had fallen back to the Hindenburg Line. The Allies had advanced to the Hindenburg Line in the north and centre. German forces launched numerous counterattacks, but positions and outposts of the Line continued falling, with the BEF alone taking 30,441 prisoners in the last week of September. On 24 September, the Supreme Army Command informed the leaders in Berlin that armistice talks were inevitable.[197]
.The final assault on the Hindenburg Line began with the Meuse-Argonne offensive, launched by American and French troops on 26 September. Two days later the Belgians, French and British attacked around Ypres, and the day after the British at St Quentin in the centre of the line. The following week, cooperating American and French units broke through in Champagne at the Battle of Blanc Mont Ridge ( 3–27 October), forcing the Germans off the commanding heights, and closing towards the Belgian frontier.[198] On 8 October, the Hindenburg Line was pierced by British and Dominion troops of the First and Third British Armies at the Second Battle of Cambrai.[199]
.Allied forces started the Vardar offensive on 15 September at two key points: Dobro Pole and near Dojran Lake. In the Battle of Dobro Pole, the Serbian and French armies had success after a three-day-long battle with relatively small casualties, and subsequently made a breakthrough in the front, something which was rarely seen in World War I. After the front was broken, Allied forces started to liberate Serbia and reached Skopje at 29 September, after which Bulgaria signed an armistice with the Allies on 30 September.[200][201]
.The collapse of the Central Powers came swiftly. Bulgaria was the first to sign an armistice, the Armistice of Salonica on 29 September 1918.[202] Wilhelm II, in a telegram to Tsar Ferdinand I of Bulgaria described the situation thus: "Disgraceful! 62,000 Serbs decided the war!".[203][204] On the same day, the German Supreme Army Command informed Wilhelm II and the Imperial Chancellor Count Georg von Hertling, that the military situation facing Germany was hopeless.[205]
.On 24 October, the Italians began a push that rapidly recovered territory lost after the Battle of Caporetto. This culminated in the Battle of Vittorio Veneto, marking the end of the Austro-Hungarian Army as an effective fighting force. The offensive also triggered the disintegration of the Austro-Hungarian Empire. During the last week of October, declarations of independence were made in Budapest, Prague, and Zagreb. On 29 October, the imperial authorities asked Italy for an armistice, but the Italians continued advancing, reaching Trento, Udine, and Trieste. On 3 November, Austria-Hungary sent a flag of truce and accepted the Armistice of Villa Giusti, arranged with the Allied Authorities in Paris. Austria and Hungary signed separate armistices following the overthrow of the Habsburg monarchy. In the following days, the Italian Army occupied Innsbruck and all Tyrol, with over 20,000 soldiers.[206] On 30 October, the Ottoman Empire capitulated, and signed the Armistice of Mudros.[202]
.With the military faltering and with widespread loss of confidence in the kaiser, Germany moved towards surrender. Prince Maximilian of Baden took charge on 3 October as Chancellor of Germany. Negotiations with President Wilson began immediately, in the hope that he would offer better terms than the British and French. Wilson demanded a constitutional monarchy and parliamentary control over the German military.[208]
.The German Revolution of 1918–1919 began at the end of October 1918. Units of the German Navy refused to set sail for a large-scale operation in a war they believed to be as good as lost. The sailors' revolt, which then ensued in the naval ports of Wilhelmshaven and Kiel, spread across the whole country within days and led to the proclamation of a republic on 9 November 1918, shortly thereafter to the abdication of Wilhelm II, and German surrender.[209][210][211][212][213]
.In the aftermath of the war, the German, Austro-Hungarian, Ottoman, and Russian empires disappeared.[f] Numerous nations regained their former independence, and new ones were created. Four dynasties fell as a result of the war: the Romanovs, the Hohenzollerns, the Habsburgs, and the Ottomans. Belgium and Serbia were badly damaged, as was France, with 1.4 million soldiers dead,[214] not counting other casualties. Germany and Russia were similarly affected.[215]
.A formal state of war between the two sides persisted for another seven months, until the signing of the Treaty of Versailles with Germany on 28 June 1919. The US Senate did not ratify the treaty despite public support for it,[216][217]and did not formally end its involvement in the war until the Knox–Porter Resolution was signed on 2 July 1921 by President Warren G. Harding.[218] For the British Empire, the state of war ceased under the provisions of the Termination of the Present War (Definition) Act 1918 concerning:
.Some war memorials date the end of the war as being when the Versailles Treaty was signed in 1919, which was when many of the troops serving abroad finally returned home; by contrast, most commemorations of the war's end concentrate on the armistice of 11 November 1918.[224]
.The Paris Peace Conference imposed a series of peace treaties on the Central Powers officially ending the war. The 1919 Treaty of Versailles dealt with Germany and, building on Wilson's 14th point, established the League of Nations on 28 June 1919.[225][226]
.The Central Powers had to acknowledge responsibility for "all the loss and damage to which the Allied and Associated Governments and their nationals have been subjected as a consequence of the war imposed upon them by" their aggression. In the Treaty of Versailles, this statement was Article 231. This article became known as the "War Guilt Clause", as the majority of Germans felt humiliated and resentful.[227] The Germans felt they had been unjustly dealt with by what they called the "diktat of Versailles". German historian Hagen Schulze said the Treaty placed Germany "under legal sanctions, deprived of military power, economically ruined, and politically humiliated."[228] Belgian historian Laurence Van Ypersele emphasises the central role played by memory of the war and the Versailles Treaty in German politics in the 1920s and 1930s:
.Active denial of war guilt in Germany and German resentment at both reparations and continued Allied occupation of the Rhineland made widespread revision of the meaning and memory of the war problematic. The legend of the "stab in the back" and the wish to revise the "Versailles diktat", and the belief in an international threat aimed at the elimination of the German nation persisted at the heart of German politics. Even a man of peace such as [Gustav] Stresemann publicly rejected German guilt. As for the Nazis, they waved the banners of domestic treason and international conspiracy in an attempt to galvanise the German nation into a spirit of revenge. Like a Fascist Italy, Nazi Germany sought to redirect the memory of the war to the benefit of its policies.[229].
Meanwhile, new nations liberated from German rule viewed the treaty as a recognition of wrongs committed against small nations by much larger aggressive neighbours.[230].Austria-Hungary was partitioned into several successor states, largely but not entirely along ethnic lines. Apart from Austria and Hungary, Czechoslovakia, Italy, Poland, Romania and Yugoslavia received territories from the Dual Monarchy (the formerly separate and autonomous Kingdom of Croatia-Slavonia was incorporated into Yugoslavia). The details were contained in the treaties of Saint-Germain-en-Laye and Trianon. As a result, Hungary lost 64% of its total population, decreasing from 20.9 million to 7.6 million, and losing 31% (3.3 out of 10.7 million) of its ethnic Hungarians.[231] According to the 1910 census, speakers of the Hungarian language included approximately 54% of the entire population of the Kingdom of Hungary. Within the country, numerous ethnic minorities were present: 16.1% Romanians, 10.5% Slovaks, 10.4% Germans, 2.5% Ruthenians, 2.5% Serbs and 8% others.[232] Between 1920 and 1924, 354,000 Hungarians fled former Hungarian territories attached to Romania, Czechoslovakia, and Yugoslavia.[233]
.The Russian Empire lost much of its western frontier as the newly independent nations of Estonia, Finland, Latvia, Lithuania, and Poland were carved from it. Romania took control of Bessarabia in April 1918.[234]
.After 123 years, Poland re-emerged as an independent country. The Kingdom of Serbia and its dynasty, as a "minor Entente nation" and the country with the most casualties per capita,[235][236][237] became the backbone of a new multinational state, the Kingdom of Serbs, Croats and Slovenes, later renamed Yugoslavia. Czechoslovakia, combining the Kingdom of Bohemia with parts of the Kingdom of Hungary, became a new nation. Romania would unite all Romanian-speaking people under a single state, leading to Greater Romania.[238]
.In Australia and New Zealand, the Battle of Gallipoli became known as those nations' "Baptism of Fire". It was the first major war in which the newly established countries fought, and it was one of the first times that Australian troops fought as Australians, not just subjects of the British Crown, and independent national identities for these nations took hold. Anzac Day, named after the Australian and New Zealand Army Corps (ANZAC), commemorates this defining moment.[239][240]
.In the aftermath of World War I, Greece fought against Turkish nationalists led by Mustafa Kemal, a war that eventually resulted in a massive population exchange between the two countries under the Treaty of Lausanne.[241] According to various sources,[242] several hundred thousand Greeks died during this period, which was tied in with the Greek genocide.[243]
.The total number of military and civilian casualties in World War I was about 40 million: estimates range from around 15 to 22 million deaths[244] and about 23 million wounded military personnel, ranking it among the deadliest conflicts in human history. The total number of deaths includes between 9 and 11 million military personnel, with an estimated civilian death toll of about 6 to 13 million.[244][245]
.Of the 60 million European military personnel who were mobilised from 1914 to 1918, an estimated 8 million were killed, 7 million were permanently disabled, and 15 million were seriously injured. Germany lost 15.1% of its active male population, Austria-Hungary lost 17.1%, and France lost 10.5%.[246] France mobilised 7.8 million men, of which 1.4 million died and 3.2 million were injured.[247] Approximately 15,000 deployed men sustained gruesome facial injuries, causing social stigma and marginalisation; they were called the gueules cassées (broken faces). In Germany, civilian deaths were 474,000 higher than in peacetime, due in large part to food shortages and malnutrition that had weakened disease resistance. These excess deaths are estimated as 271,000 in 1918, plus another 71,000 in the first half of 1919 when the blockade was still in effect.[248] Starvation caused by famine killed approximately 100,000 people in Lebanon.[249]
.Diseases flourished in the chaotic wartime conditions. In 1914 alone, louse-borne epidemic typhus killed 200,000 in Serbia.[250] Starting in early 1918, a major influenza epidemic known as Spanish flu spread across the world, accelerated by the movement of large numbers of soldiers, often crammed together in camps and transport ships with poor sanitation. The Spanish flu killed at least 17 to 25 million people,[251][252] including an estimated 2.64 million Europeans and as many as 675,000 Americans.[253] Between 1915 and 1926, an epidemic of encephalitis lethargica affected nearly 5 million people worldwide.[254][255]
.Eight million equines mostly horses, donkeys and mules died, three-quarters of them from the extreme conditions they worked in.[256]
.The German army was the first to successfully deploy chemical weapons during the Second Battle of Ypres (April–May 1915), after German scientists under the direction of Fritz Haber at the Kaiser Wilhelm Institute developed a method to weaponise chlorine.[g][258] The use of chemical weapons had been sanctioned by the German High Command to force Allied soldiers out of their entrenched positions, complementing rather than supplanting more lethal conventional weapons.[258] Chemical weapons were deployed by all major belligerents throughout the war, inflicting approximately 1.3 million casualties, of which about 90,000 were fatal.[258] The use of chemical weapons in warfare was a direct violation of the 1899 Hague Declaration Concerning Asphyxiating Gases and the 1907 Hague Convention on Land Warfare, which prohibited their use.[259][260]
.The ethnic cleansing of the Ottoman Empire's Armenian population, including mass deportations and executions, during the final years of the Ottoman Empire is considered genocide.[262] The Ottomans carried out organised and systematic massacres of the Armenian population at the beginning of the war and manipulated acts of Armenian resistance by portraying them as rebellions to justify further extermination.[263] In early 1915, several Armenians volunteered to join the Russian forces and the Ottoman government used this as a pretext to issue the Tehcir Law (Law on Deportation), which authorised the deportation of Armenians from the Empire's eastern provinces to Syria between 1915 and 1918. The Armenians were intentionally marched to death and a number were attacked by Ottoman brigands.[264] While the exact number of deaths is unknown, the International Association of Genocide Scholars estimates around 1.5 million.[262][265] The government of Turkey continues to deny the genocide to the present day, arguing that those who died were victims of inter-ethnic fighting, famine, or disease during World War I; these claims are rejected by most historians.[266]
.Other ethnic groups were similarly attacked by the Ottoman Empire during this period, including Assyrians and Greeks, and some scholars consider those events to be part of the same policy of extermination.[267][268][269] At least 250,000 Assyrian Christians, about half of the population, and 350,000–750,000 Anatolian and Pontic Greeks were killed between 1915 and 1922.[270]
.About 8 million soldiers surrendered and were held in POW camps during the war. All nations pledged to follow the Hague Conventions on fair treatment of prisoners of war, and the survival rate for POWs was generally much higher than that of combatants at the front.[271]
.Around 25–31% of Russian losses (as a proportion of those captured, wounded, or killed) were to prisoner status; for Austria-Hungary 32%; for Italy 26%; for France 12%; for Germany 9%; for Britain 7%. Prisoners from the Allied armies totalled about 1.4 million (not including Russia, which lost 2.5–3.5 million soldiers as prisoners). From the Central Powers, about 3.3 million soldiers became prisoners; most of them surrendered to Russians.[272]
.Allied personnel was around 42,928,000, while Central personnel was near 25,248,000.[215] British soldiers of the war were initially volunteers but were increasingly conscripted. Surviving veterans returning home often found they could discuss their experiences only among themselves, so formed "veterans' associations" or "Legions".
.Conscription was common in most European countries. However, it was controversial in English-speaking countries,[273] It was especially unpopular among minority ethnicities—especially the Irish Catholics in Ireland,[274] Australia,[275][page needed][276] and the French Catholics in Canada.[277][278]
.In the US, conscription began in 1917 and was generally well-received, with a few pockets of opposition in isolated rural areas.[279][page needed] The administration decided to rely primarily on conscription, rather than voluntary enlistment, to raise military manpower after only 73,000 volunteers enlisted out of the initial 1 million target in the first six weeks of war.[280]
.Military and civilian observers from every major power closely followed the course of the war.[281] Many were able to report on events from a perspective somewhat akin to modern "embedded" positions within the opposing land and naval forces.[282][283]
.
Macro- and micro-economic consequences devolved from the war. Families were altered by the departure of many men. With the death or absence of the primary wage earner, women were forced into the workforce in unprecedented numbers. At the same time, the industry needed to replace the lost labourers sent to war. This aided the struggle for voting rights for women.[284].In all nations, the government's share of GDP increased, surpassing 50% in both Germany and France and nearly reaching that level in Britain. To pay for purchases in the US, Britain cashed in its extensive investments in American railroads and then began borrowing heavily from Wall Street. President Wilson was on the verge of cutting off the loans in late 1916 but allowed a great increase in US government lending to the Allies. After 1919, the US demanded repayment of these loans. The repayments were, in part, funded by German reparations that, in turn, were supported by American loans to Germany. This circular system collapsed in 1931 and some loans were never repaid. Britain still owed the United States $4.4 billion[h] of World War I debt in 1934; the last installment was finally paid in 2015.[285]
.Britain turned to her colonies for help in obtaining essential war materials whose supply from traditional sources had become difficult. Geologists such as Albert Kitson were called on to find new resources of precious minerals in the African colonies. Kitson discovered important new deposits of manganese, used in munitions production, in the Gold Coast.[286]
.Article 231 of the Treaty of Versailles (the so-called "war guilt" clause) stated Germany accepted responsibility for "all the loss and damage to which the Allied and Associated Governments and their nationals have been subjected as a consequence of the war imposed upon them by the aggression of Germany and her allies."[287] It was worded as such to lay a legal basis for reparations, and a similar clause was inserted in the treaties with Austria and Hungary. However, neither of them interpreted it as an admission of war guilt.[288] In 1921, the total reparation sum was placed at 132 billion gold marks. However, "Allied experts knew that Germany could not pay" this sum. The total sum was divided into three categories, with the third being "deliberately designed to be chimerical" and its "primary function was to mislead public opinion ... into believing the 'total sum was being maintained.'"[289] Thus, 50 billion gold marks (12.5 billion dollars) "represented the actual Allied assessment of German capacity to pay" and "therefore ... represented the total German reparations" figure that had to be paid.[289]
.This figure could be paid in cash or in-kind (coal, timber, chemical dyes, etc.). Some of the territory lost—via the Treaty of Versailles—was credited towards the reparation figure as were other acts such as helping to restore the Library of Louvain.[290] By 1929, the Great Depression caused political chaos throughout the world.[291] In 1932 the payment of reparations was suspended by the international community, by which point Germany had paid only the equivalent of 20.598 billion gold marks.[292] With the rise of Adolf Hitler, all bonds and loans that had been issued and taken out during the 1920s and early 1930s were cancelled. David Andelman notes "Refusing to pay doesn't make an agreement null and void. The bonds, the agreement, still exist." Thus, following the Second World War, at the London Conference in 1953, Germany agreed to resume payment on the money borrowed. On 3 October 2010, Germany made the final payment on these bonds.[i]
.The Australian prime minister, Billy Hughes, wrote to the British prime minister, David Lloyd George, "You have assured us that you cannot get better terms. I much regret it, and hope even now that some way may be found of securing agreement for demanding reparation commensurate with the tremendous sacrifices made by the British Empire and her Allies." Australia received £5,571,720 in war reparations, but the direct cost of the war to Australia had been £376,993,052, and, by the mid-1930s, repatriation pensions, war gratuities, interest and sinking fund charges were £831,280,947.[297]
.In the Balkans, Yugoslav nationalists such as the leader, Ante Trumbić, strongly supported the war, desiring the freedom of Yugoslavs from Austria-Hungary and other foreign powers and the creation of an independent Yugoslavia. The Yugoslav Committee, led by Trumbić, was formed in Paris on 30 April 1915, but shortly moved its office to London.[298] In April 1918, the Rome Congress of Oppressed Nationalities met, including Czechoslovak, Italian, Polish, Transylvanian, and Yugoslav representatives who urged the Allies to support national self-determination for the peoples residing within Austria-Hungary.[299]
.In the Middle East, Arab nationalism soared in Ottoman territories in response to the rise of Turkish nationalism during the war. Arab nationalist leaders advocated the creation of a pan-Arab state. In 1916, the Arab Revolt began in Ottoman-controlled territories of the Middle East to achieve independence.[300]
.In East Africa, Iyasu V of Ethiopia was supporting the Dervish state who were at war with the British in the Somaliland campaign.[301] Von Syburg, the German envoy in Addis Ababa, said, "now the time has come for Ethiopia to regain the coast of the Red Sea driving the Italians home, to restore the Empire to its ancient size." The Ethiopian Empire was on the verge of entering World War I on the side of the Central Powers before Iyasu's overthrow at the Battle of Segale due to Allied pressure on the Ethiopian aristocracy.[302]
.Several socialist parties initially supported the war when it began in August 1914.[299] But European socialists split on national lines, with the concept of class conflict held by radical socialists such as Marxists and syndicalists being overborne by their patriotic support for the war.[303] Once the war began, Austrian, British, French, German, and Russian socialists followed the rising nationalist current by supporting their countries' intervention in the war.[304]
.Italian nationalism was stirred by the outbreak of the war and was initially strongly supported by a variety of political factions. One of the most prominent and popular Italian nationalist supporters of the war was Gabriele D'Annunzio, who promoted Italian irredentism and helped sway the Italian public to support intervention in the war.[305] The Italian Liberal Party, under the leadership of Paolo Boselli, promoted intervention in the war on the side of the Allies and used the Dante Alighieri Society to promote Italian nationalism.[306] Italian socialists were divided on whether to support the war or oppose it; some were militant supporters of the war, including Benito Mussolini and Leonida Bissolati.[307] However, the Italian Socialist Party decided to oppose the war after anti-militarist protestors were killed, resulting in a general strike called Red Week.[308] The Italian Socialist Party purged itself of pro-war nationalist members, including Mussolini.[308] Mussolini formed the pro-interventionist Il Popolo d'Italia and the Fasci Rivoluzionario d'Azione Internazionalista ("Revolutionary Fasci for International Action") in October 1914 that later developed into the Fasci Italiani di Combattimento in 1919, the origin of fascism.[309] Mussolini's nationalism enabled him to raise funds from Ansaldo (an armaments firm) and other companies to create Il Popolo d'Italia to convince socialists and revolutionaries to support the war.[310]
.On both sides, there was large-scale fundraising for soldiers' welfare, their dependents and those injured. The Nail Men were a German example. Around the British Empire, there were many patriotic funds, including the Royal Patriotic Fund Corporation, Canadian Patriotic Fund, Queensland Patriotic Fund and, by 1919, there were 983 funds in New Zealand.[311] At the start of the next world war the New Zealand funds were reformed, having been criticised as overlapping, wasteful and abused,[312] but 11 were still functioning in 2002.[313]
.
Many countries jailed those who spoke out against the conflict. These included Eugene Debs in the US and Bertrand Russell in Britain. In the US, the Espionage Act of 1917 and Sedition Act of 1918 made it a federal crime to oppose military recruitment or make any statements deemed "disloyal". Publications at all critical of the government were removed from circulation by postal censors,[314] [page needed] and many served long prison sentences for statements of fact deemed unpatriotic..Several nationalists opposed intervention, particularly within states that the nationalists were hostile to. Although the vast majority of Irish people consented to participate in the war in 1914 and 1915, a minority of advanced Irish nationalists had staunchly opposed taking part.[315][page needed] The war began amid the Home Rule crisis in Ireland that had resurfaced in 1912, and by July 1914 there was a serious possibility of an outbreak of civil war in Ireland. Irish nationalists and Marxists attempted to pursue Irish independence, culminating in the Easter Rising of 1916, with Germany sending 20,000 rifles to Ireland to stir unrest in Britain.[316] The British government placed Ireland under martial law in response to the Easter Rising, though once the immediate threat of revolution had dissipated, the authorities did try to make concessions to nationalist feeling.[317][page needed] However, opposition to involvement in the war increased in Ireland, resulting in the Conscription Crisis of 1918.
.Other opposition came from conscientious objectors—some socialist, some religious—who had refused to fight. In Britain, 16,000 people asked for conscientious objector status.[318] Some of them, most notably prominent peace activist Stephen Hobhouse, refused both military and alternative service.[319] Many suffered years of prison, including solitary confinement. Even after the war, in Britain, many job advertisements were marked "No conscientious objectors need to apply".[320]
.On 1–4 May 1917, about 100,000 workers and soldiers of Petrograd, and after them, the workers and soldiers of other Russian cities, led by the Bolsheviks, demonstrated under banners reading "Down with the war!" and "all power to the Soviets!". The mass demonstrations resulted in a crisis for the Russian Provisional Government.[321] In Milan, in May 1917, Bolshevik revolutionaries organised and engaged in rioting calling for an end to the war, and managed to close down factories and stop public transportation.[322] The Italian army was forced to enter Milan with tanks and machine guns to face Bolsheviks and anarchists, who fought violently until 23 May when the army gained control of the city. Almost 50 people (including three Italian soldiers) were killed and over 800 people were arrested.[322]
.World War I began as a clash of 20th-century technology and 19th-century tactics, with the inevitably large ensuing casualties. By the end of 1917, however, the major armies had modernised and were making use of telephone, wireless communication,[324] armoured cars, tanks (especially with the advent of the prototype tank, Little Willie), and aircraft.[325]
.Artillery also underwent a revolution. In 1914, cannons were positioned in the front line and fired directly at their targets. By 1917, indirect fire with guns (as well as mortars and even machine guns) was commonplace, using new techniques for spotting and ranging, notably, aircraft and the field telephone.[326]
.Fixed-wing aircraft were initially used for reconnaissance and ground attack. To shoot down enemy planes, anti-aircraft guns and fighter aircraft were developed. Strategic bombers were created, principally by the Germans and British, though the former used Zeppelins as well.[327] Towards the end of the conflict, aircraft carriers were used for the first time, with HMS Furious launching Sopwith Camels in a raid to destroy the Zeppelin hangars at Tønder in 1918.[328]
.The non-military diplomatic and propaganda interactions among the nations were designed to build support for the cause or to undermine support for the enemy. For the most part, wartime diplomacy focused on five issues: propaganda campaigns; defining and redefining the war goals, which became harsher as the war went on; luring neutral nations (Italy, Ottoman Empire, Bulgaria, Romania) into the coalition by offering slices of enemy territory; and encouragement by the Allies of nationalistic minority movements inside the Central Powers, especially among Czechs, Poles, and Arabs. In addition, multiple peace proposals were coming from neutrals, or one side or the other; none of them progressed very far.[329][330][page needed][331][page needed]
.Memorials were built in thousands of villages and towns. Close to battlefields, those buried in improvised burial grounds were gradually moved to formal graveyards under the care of organisations such as the Commonwealth War Graves Commission, the American Battle Monuments Commission, the German War Graves Commission, and Le Souvenir français. Many of these graveyards also have monuments to the missing or unidentified dead, such as the Menin Gate Memorial to the Missing and the Thiepval Memorial to the Missing of the Somme.[332][333]
.In 1915, John McCrae, a Canadian army doctor, wrote the poem In Flanders Fields as a salute to those who perished in the war. It is still recited today, especially on Remembrance Day and Memorial Day.[334][335]
.National World War I Museum and Memorial in Kansas City, Missouri, is a memorial dedicated to all Americans who served in World War I. The Liberty Memorial was dedicated on 1 November 1921.[336]
.The British government budgeted substantial resources to the commemoration of the war during the period 2014 to 2018. The lead body is the Imperial War Museum.[337] On 3 August 2014, French President François Hollande and German President Joachim Gauck together marked the centenary of Germany's declaration of war on France by laying the first stone of a memorial in Vieil Armand, known in German as Hartmannswillerkopf, for French and German soldiers killed in the war.[338] As part of commemorations for the centenary of the 1918 Armistice, French President Emmanuel Macron and German Chancellor Angela Merkel visited the site of the signing of the Armistice of Compiègne and unveiled a plaque to reconciliation.[339]
.... "Strange, friend," I said, "Here is no cause to mourn."
"None," said the other, "Save the undone years"... 
.The first efforts to comprehend the meaning and consequences of modern warfare began during the initial phases of the war and are still underway more than a century later. Teaching World War I has presented special challenges. When compared with World War II, the First World War is often thought to be "a wrong war fought for the wrong reasons"; it lacks the metanarrative of good versus evil that characterises retellings of the Second World War. Lacking recognizable heroes and villains, it is often taught thematically, invoking simplified tropes that obscure the complexity of the conflict.[341]
.Historian Heather Jones argues that the historiography has been reinvigorated by a cultural turn in the 21st century. Scholars have raised entirely new questions regarding military occupation, radicalisation of politics, race, medical science, gender and mental health. Among the major subjects that historians have long debated regarding the war include: Why the war began; why the Allies won; whether generals were responsible for high casualty rates; how soldiers endured the poor conditions of trench warfare; and to what extent the civilian home front accepted and endorsed the war effort.[342][343]
.As late as 2007, unexploded ordnance at battlefield sites like Verdun and Somme continued to pose a danger. In France and Belgium, locals who discover caches of unexploded munitions are assisted by weapons disposal units. In some places, plant life has still not recovered from the effects of the war.[341]
.Lawson, Eric; Lawson, Jane (2002). The First Air Campaign: August 1914 – November 1918. Da Capo Press. ISBN 978-0-306-81213-2.
.Lieberman, Benjamin (2013). The Holocaust and Genocides in Europe. New York: Continuum Publishing Corporation. ISBN 978-1-4411-9478-7.
.
.Asia-Pacific
.Mediterranean and Middle East
.Other campaigns
.Coups
.Resistance movements
.World War II[b] or the Second World War (1 September 1939 – 2 September 1945) was a global conflict between two coalitions: the Allies and the Axis powers. Nearly all of the world's countries participated, with many nations mobilising all resources in pursuit of total war. Tanks and aircraft played major roles, enabling the strategic bombing of cities and delivery of the first and only nuclear weapons ever used in war. World War II was the deadliest conflict in history, causing the death of 70 to 85 million people, more than half of whom were civilians. Millions died in genocides, including the Holocaust, and by massacres, starvation, and disease. After the Allied victory, Germany, Austria, Japan, and Korea were occupied, and German and Japanese leaders were tried for war crimes.
.The causes of World War II included unresolved tensions in the aftermath of World War I and the rises of fascism in Europe and militarism in Japan. Key events preceding the war included Japan's invasion of Manchuria in 1931, the Spanish Civil War, the outbreak of the Second Sino-Japanese War in 1937, and Germany's annexations of Austria and the Sudetenland. World War II is generally considered to have begun on 1 September 1939, when Nazi Germany, under Adolf Hitler, invaded Poland, after which the United Kingdom and France declared war on Germany. Poland was divided between Germany and the Soviet Union under the Molotov–Ribbentrop Pact. In 1940, the Soviet Union annexed the Baltic states and parts of Finland and Romania. After the fall of France in June 1940, the war continued mainly between Germany and the British Empire, with fighting in the Balkans, Mediterranean, and Middle East, the aerial Battle of Britain and the Blitz, and the naval Battle of the Atlantic. Through campaigns and treaties, Germany gained control of much of continental Europe and formed the Axis alliance with Italy, Japan, and other countries. In June 1941, Germany invaded the Soviet Union, opening the Eastern Front and initially making large territorial gains.
.In December 1941, Japan attacked American and British territories in Asia and the Pacific, including at Pearl Harbor in Hawaii, leading the United States to enter the war against Japan and Germany. Japan conquered much of coastal China and Southeast Asia, but its advances in the Pacific were halted in June 1942 at the Battle of Midway. In early 1943, Axis forces were defeated in North Africa and at Stalingrad in the Soviet Union, and that year their continued defeats on the Eastern Front, an Allied invasion of Italy, and Allied offensives in the Pacific forced them into retreat on all fronts. In 1944, the Western Allies invaded France at Normandy as the Soviet Union recaptured its pre-war territory and the US crippled Japan's navy and captured key Pacific islands. The war in Europe concluded with the liberation of German-occupied territories; invasions of Germany by the Western Allies and the Soviet Union, which culminated in the fall of Berlin to Soviet troops; and Germany's unconditional surrender on 8 May 1945. On 6 and 9 August, the US dropped atomic bombs on Hiroshima and Nagasaki in Japan. Faced with an imminent Allied invasion, the prospect of further atomic bombings, and a Soviet declaration of war and invasion of Manchuria, Japan announced its unconditional surrender on 15 August, and signed a surrender document on 2 September 1945.
.World War II transformed the political, economic, and social structures of the world, and established the foundation of international relations for the rest of the 20th century and into the 21st century. The United Nations was created to foster international cooperation and prevent future conflicts, with the victorious great powers—China, France, the Soviet Union, the UK, and the US—becoming the permanent members of its security council. The Soviet Union and US emerged as rival global superpowers, setting the stage for the half-century Cold War. In the wake of Europe's devastation, the influence of its great powers waned, triggering the decolonisation of Africa and Asia. Many countries whose industries had been damaged moved towards economic recovery and expansion.
.Most historians date the beginning of World War II to the German invasion of Poland on 1 September 1939[1][2] and the United Kingdom and France's declaration of war on Germany two days later. Dates for the beginning of the Pacific War include the start of the Second Sino-Japanese War on 7 July 1937,[3][4] or the earlier Japanese invasion of Manchuria, on 19 September 1931.[5][6] Others follow the British historian A. J. P. Taylor, who stated that the Sino-Japanese War and war in Europe and its colonies occurred simultaneously, and the two wars became World War II in 1941.[7] Other proposed starting dates for World War II include the Italian invasion of Abyssinia on 3 October 1935.[8] The British historian Antony Beevor views the beginning of World War II as the Battles of Khalkhin Gol fought between Japan and the forces of Mongolia and the Soviet Union from May to September 1939.[9] Others view the Spanish Civil War as the start or prelude to World War II.[10][11]
.The exact date of the war's end also is not universally agreed upon. It was generally accepted at the time that the war ended with the armistice of 15 August 1945 (V-J Day), rather than with the formal surrender of Japan on 2 September 1945, which officially ended the war in Asia. A peace treaty between Japan and the Allies was signed in 1951.[12] A 1990 treaty regarding Germany's future allowed the reunification of East and West Germany to take place.[13] No formal peace treaty between Japan and the Soviet Union was ever signed,[14] although the state of war between the two countries was terminated by the Soviet–Japanese Joint Declaration of 1956, which also restored full diplomatic relations between them.[15]
.World War I had radically altered the political European map with the defeat of the Central Powers—including Austria-Hungary, Germany, Bulgaria, and the Ottoman Empire—and the 1917 Bolshevik seizure of power in Russia, which led to the founding of the Soviet Union. Meanwhile, the victorious Allies of World War I, such as France, Belgium, Italy, Romania, and Greece, gained territory, and new nation-states were created out of the dissolution of the Austro-Hungarian, Ottoman, and Russian Empires.[16]
.To prevent a future world war, the League of Nations was established in 1920 by the Paris Peace Conference. The organisation's primary goals were to prevent armed conflict through collective security, military, and naval disarmament, as well as settling international disputes through peaceful negotiations and arbitration.[17]
.Despite strong pacifist sentiment after World War I,[18] irredentist and revanchist nationalism had emerged in several European states. These sentiments were especially marked in Germany because of the significant territorial, colonial, and financial losses imposed by the Treaty of Versailles. Under the treaty, Germany lost around 13 percent of its home territory and all its overseas possessions, while German annexation of other states was prohibited, reparations were imposed, and limits were placed on the size and capability of the country's armed forces.[19]
.The German Empire was dissolved in the German revolution of 1918–1919, and a democratic government, later known as the Weimar Republic, was created. The interwar period saw strife between supporters of the new republic and hardline opponents on both the political right and left. Italy, as an Entente ally, had made some post-war territorial gains; however, Italian nationalists were angered that the promises made by the United Kingdom and France to secure Italian entrance into the war were not fulfilled in the peace settlement. From 1922 to 1925, the Fascist movement led by Benito Mussolini seized power in Italy with a nationalist, totalitarian, and class collaborationist agenda that abolished representative democracy, repressed socialist, left-wing, and liberal forces, and pursued an aggressive expansionist foreign policy aimed at making Italy a world power, promising the creation of a "New Roman Empire".[20]
.Adolf Hitler, after an unsuccessful attempt to overthrow the German government in 1923, eventually became the chancellor of Germany in 1933 when President Paul von Hindenburg and the Reichstag appointed him. Following Hindenburg's death in 1934, Hitler proclaimed himself Führer of Germany and abolished democracy, espousing a radical, racially motivated revision of the world order, and soon began a massive rearmament campaign.[21] France, seeking to secure its alliance with Italy, allowed Italy a free hand in Ethiopia, which Italy desired as a colonial possession. The situation was aggravated in early 1935 when the Territory of the Saar Basin was legally reunited with Germany, and Hitler repudiated the Treaty of Versailles, accelerated his rearmament programme, and introduced conscription.[22]
.The United Kingdom, France and Italy formed the Stresa Front in April 1935 in order to contain Germany, a key step towards military globalisation; however, that June, the United Kingdom made an independent naval agreement with Germany, easing prior restrictions. The Soviet Union, concerned by Germany's goals of capturing vast areas of Eastern Europe, drafted a treaty of mutual assistance with France. Before taking effect, though, the Franco-Soviet pact was required to go through the bureaucracy of the League of Nations, which rendered it essentially toothless.[23] The United States, concerned with events in Europe and Asia, passed the Neutrality Act in August of the same year.[24]
.Hitler defied the Versailles and Locarno Treaties by remilitarising the Rhineland in March 1936, encountering little opposition due to the policy of appeasement.[25] In October 1936, Germany and Italy formed the Rome–Berlin Axis. A month later, Germany and Japan signed the Anti-Comintern Pact, which Italy joined the following year.[26]
.The Kuomintang party in China launched a unification campaign against regional warlords and nominally unified China in the mid-1920s, but was soon embroiled in a civil war against its former Chinese Communist Party (CCP) allies[27] and new regional warlords. In 1931, an increasingly militaristic Empire of Japan, which had long sought influence in China[28] as the first step of what its government saw as the country's right to rule Asia, staged the Mukden incident as a pretext to invade Manchuria and establish the puppet state of Manchukuo.[29]
.China appealed to the League of Nations to stop the Japanese invasion of Manchuria. Japan withdrew from the League of Nations after being condemned for its incursion into Manchuria. The two nations then fought several battles, in Shanghai, Rehe and Hebei, until the Tanggu Truce was signed in 1933. Thereafter, Chinese volunteer forces continued the resistance to Japanese aggression in Manchuria, and Chahar and Suiyuan.[30] After the 1936 Xi'an Incident, the Kuomintang and CCP forces agreed on a ceasefire to present a united front to oppose Japan.[31]
.The Second Italo-Ethiopian War was a brief colonial war that began in October 1935 and ended in May 1936. The war began with the invasion of the Ethiopian Empire (also known as Abyssinia) by the armed forces of the Kingdom of Italy (Regno d'Italia), which was launched from Italian Somaliland and Eritrea.[32] The war resulted in the military occupation of Ethiopia and its annexation into the newly created colony of Italian East Africa (Africa Orientale Italiana); in addition it exposed the weakness of the League of Nations as a force to preserve peace. Both Italy and Ethiopia were member nations, but the League did little when the former clearly violated Article X of the League's Covenant.[33] The United Kingdom and France supported imposing sanctions on Italy for the invasion, but the sanctions were not fully enforced and failed to end the Italian invasion.[34] Italy subsequently dropped its objections to Germany's goal of absorbing Austria.[35]
.When civil war broke out in Spain, Hitler and Mussolini lent military support to the Nationalist rebels, led by General Francisco Franco. Italy supported the Nationalists to a greater extent than the Nazis: Mussolini sent more than 70,000 ground troops, 6,000 aviation personnel, and 720 aircraft to Spain.[36] The Soviet Union supported the existing government of the Spanish Republic. More than 30,000 foreign volunteers, known as the International Brigades, also fought against the Nationalists. Both Germany and the Soviet Union used this proxy war as an opportunity to test in combat their most advanced weapons and tactics. The Nationalists won the civil war in April 1939; Franco, now dictator, remained officially neutral during World War II but generally favoured the Axis.[37] His greatest collaboration with Germany was the sending of volunteers to fight on the Eastern Front.[38]
.In July 1937, Japan captured the former Chinese imperial capital of Peking after instigating the Marco Polo Bridge incident, which culminated in the Japanese campaign to invade all of China.[39] The Soviets quickly signed a non-aggression pact with China to lend materiel support, effectively ending China's prior cooperation with Germany. From September to November, the Japanese attacked Taiyuan, engaged the Kuomintang Army around Xinkou,[40] and fought Communist forces in Pingxingguan.[41][42] Generalissimo Chiang Kai-shek deployed his best army to defend Shanghai, but after three months of fighting, Shanghai fell. The Japanese continued to push Chinese forces back, capturing the capital Nanking in December 1937. After the fall of Nanking, tens or hundreds of thousands of Chinese civilians and disarmed combatants were murdered by the Japanese.[43][44]
.In March 1938, Nationalist Chinese forces won their first major victory at Taierzhuang, but then the city of Xuzhou was taken by the Japanese in May.[45] In June 1938, Chinese forces stalled the Japanese advance by flooding the Yellow River; this manoeuvre bought time for the Chinese to prepare their defences at Wuhan, but the city was taken by October.[46] Japanese military victories did not bring about the collapse of Chinese resistance that Japan had hoped to achieve; instead, the Chinese government relocated inland to Chongqing and continued the war.[47][48]
.In the mid-to-late 1930s, Japanese forces in Manchukuo had sporadic border clashes with the Soviet Union and Mongolia. The Japanese doctrine of Hokushin-ron, which emphasised Japan's expansion northward, was favoured by the Imperial Army during this time. This policy would prove difficult to maintain in light of the Japanese defeat at Khalkin Gol in 1939, the ongoing Second Sino-Japanese War[49] and ally Nazi Germany pursuing neutrality with the Soviets. Japan and the Soviet Union eventually signed a Neutrality Pact in April 1941, and Japan adopted the doctrine of Nanshin-ron, promoted by the Navy, which took its focus southward and eventually led to war with the United States and the Western Allies.[50][51]
.In Europe, Germany and Italy were becoming more aggressive. In March 1938, Germany annexed Austria, again provoking little response from other European powers.[52] Encouraged, Hitler began pressing German claims on the Sudetenland, an area of Czechoslovakia with a predominantly ethnic German population. Soon the United Kingdom and France followed the appeasement policy of British Prime Minister Neville Chamberlain and conceded this territory to Germany in the Munich Agreement, which was made against the wishes of the Czechoslovak government, in exchange for a promise of no further territorial demands.[53] Soon afterwards, Germany and Italy forced Czechoslovakia to cede additional territory to Hungary, and Poland annexed the Trans-Olza region of Czechoslovakia.[54]
.Although all of Germany's stated demands had been satisfied by the agreement, privately Hitler was furious that British interference had prevented him from seizing all of Czechoslovakia in one operation. In subsequent speeches Hitler attacked British and Jewish "war-mongers" and in January 1939 secretly ordered a major build-up of the German navy to challenge British naval supremacy. In March 1939, Germany invaded the remainder of Czechoslovakia and subsequently split it into the German Protectorate of Bohemia and Moravia and a pro-German client state, the Slovak Republic.[55] Hitler also delivered an ultimatum to Lithuania on 20 March 1939, forcing the concession of the Klaipėda Region, formerly the German Memelland.[56]
.Greatly alarmed and with Hitler making further demands on the Free City of Danzig, the United Kingdom and France guaranteed their support for Polish independence; when Italy conquered Albania in April 1939, the same guarantee was extended to the Kingdoms of Romania and Greece.[57] Shortly after the Franco-British pledge to Poland, Germany and Italy formalised their own alliance with the Pact of Steel.[58] Hitler accused the United Kingdom and Poland of trying to "encircle" Germany and renounced the Anglo-German Naval Agreement and the German–Polish declaration of non-aggression.[59]
.The situation became a crisis in late August as German troops continued to mobilise against the Polish border. On 23 August the Soviet Union signed a non-aggression pact with Germany,[60] after tripartite negotiations for a military alliance between France, the United Kingdom, and Soviet Union had stalled.[61] This pact had a secret protocol that defined German and Soviet "spheres of influence" (western Poland and Lithuania for Germany; eastern Poland, Finland, Estonia, Latvia and Bessarabia for the Soviet Union), and raised the question of continuing Polish independence.[62] The pact neutralised the possibility of Soviet opposition to a campaign against Poland and assured that Germany would not have to face the prospect of a two-front war, as it had in World War I. Immediately afterwards, Hitler ordered the attack to proceed on 26 August, but upon hearing that the United Kingdom had concluded a formal mutual assistance pact with Poland and that Italy would maintain neutrality, he decided to delay it.[63]
.In response to British requests for direct negotiations to avoid war, Germany made demands on Poland, which served as a pretext to worsen relations.[64] On 29 August, Hitler demanded that a Polish plenipotentiary immediately travel to Berlin to negotiate the handover of Danzig, and to allow a plebiscite in the Polish Corridor in which the German minority would vote on secession.[64] The Poles refused to comply with the German demands, and on the night of 30–31 August in a confrontational meeting with the British ambassador Nevile Henderson, Ribbentrop declared that Germany considered its claims rejected.[65]
.On 1 September 1939, Germany invaded Poland after having staged several false flag border incidents as a pretext to initiate the invasion.[67] The first German attack of the war came against the Polish defences at Westerplatte.[68] The United Kingdom responded with an ultimatum for Germany to cease military operations, and on 3 September, after the ultimatum was ignored, Britain and France declared war on Germany.[c] During the Phoney War period, the alliance provided no direct military support to Poland, outside of a cautious French probe into the Saarland.[69] The Western Allies also began a naval blockade of Germany, which aimed to damage the country's economy and war effort.[70] Germany responded by ordering U-boat warfare against Allied merchant and warships, which would later escalate into the Battle of the Atlantic.[71]
On 8 September, German troops reached the suburbs of Warsaw. The Polish counter-offensive to the west halted the German advance for several days, but it was outflanked and encircled by the Wehrmacht. Remnants of the Polish army broke through to besieged Warsaw. On 17 September 1939, two days after signing a cease-fire with Japan, the Soviet Union invaded Poland[72] under the supposed pretext that the Polish state had ceased to exist.[73] On 27 September, the Warsaw garrison surrendered to the Germans, and the last large operational unit of the Polish Army surrendered on 6 October. Despite the military defeat, Poland never surrendered; instead, it formed the Polish government-in-exile and a clandestine state apparatus remained in occupied Poland.[74] A significant part of Polish military personnel evacuated to Romania and Latvia; many of them later fought against the Axis in other theatres of the war.[75]
.Germany annexed western Poland and occupied central Poland; the Soviet Union annexed eastern Poland; small shares of Polish territory were transferred to Lithuania and Slovakia. On 6 October, Hitler made a public peace overture to the United Kingdom and France but said that the future of Poland was to be determined exclusively by Germany and the Soviet Union. The proposal was rejected[65] and Hitler ordered an immediate offensive against France,[76] which was postponed until the spring of 1940 due to bad weather.[77][78][79]
.After the outbreak of war in Poland, Stalin threatened Estonia, Latvia, and Lithuania with military invasion, forcing the three Baltic countries to sign pacts allowing the creation of Soviet military bases in these countries; in October 1939, significant Soviet military contingents were moved there.[80][81][82] Finland refused to sign a similar pact and rejected ceding part of its territory to the Soviet Union. The Soviet Union invaded Finland in November 1939,[83] and was subsequently expelled from the League of Nations for this crime of aggression.[84] Despite overwhelming numerical superiority, Soviet military success during the Winter War was modest, and the Finno-Soviet war ended in March 1940 with some Finnish concessions of territory.[85]
.In June 1940, the Soviet Union occupied the entire territories of Estonia, Latvia and Lithuania,[81] as well as the Romanian regions of Bessarabia, Northern Bukovina, and the Hertsa region. In August 1940, Hitler imposed the Second Vienna Award on Romania which led to the transfer of Northern Transylvania to Hungary.[86] In September 1940, Bulgaria demanded Southern Dobruja from Romania with German and Italian support, leading to the Treaty of Craiova.[87] The loss of one-third of Romania's 1939 territory caused a coup against King Carol II, turning Romania into a fascist dictatorship under Marshal Ion Antonescu, with a course set towards the Axis in the hopes of a German guarantee.[88] Meanwhile, German-Soviet political relations and economic co-operation[89][90] gradually stalled,[91][92] and both states began preparations for war.[93]
.In April 1940, Germany invaded Denmark and Norway to protect shipments of iron ore from Sweden, which the Allies were attempting to cut off.[94] Denmark capitulated after six hours, and despite Allied support, Norway was conquered within two months.[95] British discontent over the Norwegian campaign led to the resignation of Prime Minister Neville Chamberlain, who was replaced by Winston Churchill on 10 May 1940.[96]
.On the same day, Germany launched an offensive against France. To circumvent the strong Maginot Line fortifications on the Franco-German border, Germany directed its attack at the neutral nations of Belgium, the Netherlands, and Luxembourg.[97] The Germans carried out a flanking manoeuvre through the Ardennes region,[98] which was mistakenly perceived by the Allies as an impenetrable natural barrier against armoured vehicles.[99][100] By successfully implementing new Blitzkrieg tactics, the Wehrmacht rapidly advanced to the Channel and cut off the Allied forces in Belgium, trapping the bulk of the Allied armies in a cauldron on the Franco-Belgian border near Lille. The United Kingdom was able to evacuate a significant number of Allied troops from the continent by early June, although they had to abandon almost all their equipment.[101]
.On 10 June, Italy invaded France, declaring war on both France and the United Kingdom.[102] The Germans turned south against the weakened French army, and Paris fell to them on 14 June. Eight days later France signed an armistice with Germany; it was divided into German and Italian occupation zones,[103] and an unoccupied rump state under the Vichy Regime, which, though officially neutral, was generally aligned with Germany. France kept its fleet, which the United Kingdom attacked on 3 July in an attempt to prevent its seizure by Germany.[104]
.The air Battle of Britain[105] began in early July with Luftwaffe attacks on shipping and harbours.[106] The German campaign for air superiority started in August but its failure to defeat RAF Fighter Command forced the indefinite postponement of the proposed German invasion of Britain. The German strategic bombing offensive intensified with night attacks on London and other cities in the Blitz, but largely ended in May 1941[107] after failing to significantly disrupt the British war effort.[106]
.Using newly captured French ports, the German Navy enjoyed success against an over-extended Royal Navy, using U-boats against British shipping in the Atlantic.[108] The British Home Fleet scored a significant victory on 27 May 1941 by sinking the German battleship Bismarck.[109]
.In November 1939, the United States was assisting China and the Western Allies, and had amended the Neutrality Act to allow "cash and carry" purchases by the Allies.[110] In 1940, following the German capture of Paris, the size of the United States Navy was significantly increased. In September the United States further agreed to a trade of American destroyers for British bases.[111] Still, a large majority of the American public continued to oppose any direct military intervention in the conflict well into 1941.[112] In December 1940, Roosevelt accused Hitler of planning world conquest and ruled out any negotiations as useless, calling for the United States to become an "arsenal of democracy" and promoting Lend-Lease programmes of military and humanitarian aid to support the British war effort; Lend-Lease was later extended to the other Allies, including the Soviet Union after it was invaded by Germany.[113] The United States started strategic planning to prepare for a full-scale offensive against Germany.[114]
.At the end of September 1940, the Tripartite Pact formally united Japan, Italy, and Germany as the Axis powers. The Tripartite Pact stipulated that any country—with the exception of the Soviet Union—that attacked any Axis Power would be forced to go to war against all three.[115] The Axis expanded in November 1940 when Hungary, Slovakia, and Romania joined.[116] Romania and Hungary later made major contributions to the Axis war against the Soviet Union, in Romania's case partially to recapture territory ceded to the Soviet Union.[117]
.In early June 1940, the Italian Regia Aeronautica attacked and besieged Malta, a British possession. From late summer to early autumn, Italy conquered British Somaliland and made an incursion into British-held Egypt. In October, Italy attacked Greece, but the attack was repulsed with heavy Italian casualties; the campaign ended within months with minor territorial changes.[118] To assist Italy and prevent Britain from gaining a foothold, Germany prepared to invade the Balkans, which would threaten Romanian oil fields and strike against British dominance of the Mediterranean.[119]
.In December 1940, British Empire forces began counter-offensives against Italian forces in Egypt and Italian East Africa.[120] The offensives were successful; by early February 1941, Italy had lost control of eastern Libya, and large numbers of Italian troops had been taken prisoner. The Italian Navy also suffered significant defeats, with the Royal Navy putting three Italian battleships out of commission after a carrier attack at Taranto, and neutralising several more warships at the Battle of Cape Matapan.[121]
.Italian defeats prompted Germany to deploy an expeditionary force to North Africa; at the end of March 1941, Rommel's Afrika Korps launched an offensive which drove back Commonwealth forces.[122] In less than a month, Axis forces advanced to western Egypt and besieged the port of Tobruk.[123]
.By late March 1941, Bulgaria and Yugoslavia signed the Tripartite Pact; however, the Yugoslav government was overthrown two days later by pro-British nationalists. Germany and Italy responded with simultaneous invasions of both Yugoslavia and Greece, commencing on 6 April 1941; both nations were forced to surrender within the month.[124] The airborne invasion of the Greek island of Crete at the end of May completed the German conquest of the Balkans.[125] Partisan warfare subsequently broke out against the Axis occupation of Yugoslavia, which continued until the end of the war.[126]
.In the Middle East in May, Commonwealth forces quashed an uprising in Iraq which had been supported by German aircraft from bases within Vichy-controlled Syria.[127] Between June and July, British-led forces invaded and occupied the French possessions of Syria and Lebanon, assisted by the Free French.[128]
.With the situation in Europe and Asia relatively stable, Germany, Japan, and the Soviet Union made preparations for war. With the Soviets wary of mounting tensions with Germany, and the Japanese planning to take advantage of the European War by seizing resource-rich European possessions in Southeast Asia, the two powers signed the Soviet–Japanese Neutrality Pact in April 1941.[129] By contrast, the Germans were steadily making preparations for an attack on the Soviet Union, massing forces on the Soviet border.[130]
.Hitler believed that the United Kingdom's refusal to end the war was based on the hope that the United States and the Soviet Union would enter the war against Germany sooner or later.[131] On 31 July 1940, Hitler decided that the Soviet Union should be eliminated and aimed for the conquest of Ukraine, the Baltic states and Byelorussia.[132] However, other senior German officials like Ribbentrop saw an opportunity to create a Euro-Asian bloc against the British Empire by inviting the Soviet Union into the Tripartite Pact.[133] In November 1940, negotiations took place to determine if the Soviet Union would join the pact. The Soviets showed some interest but asked for concessions from Finland, Bulgaria, Turkey, and Japan that Germany considered unacceptable. On 18 December 1940, Hitler issued the directive to prepare for an invasion of the Soviet Union.[134]
.On 22 June 1941, Germany, supported by Italy and Romania, invaded the Soviet Union in Operation Barbarossa, with Germany accusing the Soviets of plotting against them; they were joined shortly by Finland and Hungary.[135] The primary targets of this surprise offensive[136] were the Baltic region, Moscow and Ukraine, with the ultimate goal of ending the 1941 campaign near the Arkhangelsk-Astrakhan line—from the Caspian to the White Seas. Hitler's objectives were to eliminate the Soviet Union as a military power, exterminate Communism, generate Lebensraum ("living space")[137] by dispossessing the native population,[138] and guarantee access to the strategic resources needed to defeat Germany's remaining rivals.[139]
.Although the Red Army was preparing for strategic counter-offensives before the war,[140] Operation Barbarossa forced the Soviet supreme command to adopt strategic defence. During the summer, the Axis made significant gains into Soviet territory, inflicting immense losses in both personnel and materiel. By mid-August, however, the German Army High Command decided to suspend the offensive of a considerably depleted Army Group Centre, and to divert the 2nd Panzer Group to reinforce troops advancing towards central Ukraine and Leningrad.[141] The Kiev offensive was overwhelmingly successful, resulting in encirclement and elimination of four Soviet armies, and made possible further advance into Crimea and industrially-developed Eastern Ukraine (the First Battle of Kharkov).[142]
.The diversion of three-quarters of the Axis troops and the majority of their air forces from France and the central Mediterranean to the Eastern Front[143] prompted the United Kingdom to reconsider its grand strategy.[144] In July, the UK and the Soviet Union formed a military alliance against Germany[145] and in August, the United Kingdom and the United States jointly issued the Atlantic Charter, which outlined British and American goals for the post-war world.[146] In late August the British and Soviets invaded neutral Iran to secure the Persian Corridor, Iran's oil fields, and preempt any Axis advances through Iran toward the Baku oil fields or India.[147]
.By October, Axis powers had achieved operational objectives in Ukraine and the Baltic region, with only the sieges of Leningrad[148] and Sevastopol continuing.[149] A major offensive against Moscow was renewed; after two months of fierce battles in increasingly harsh weather, the German army almost reached the outer suburbs of Moscow, where the exhausted troops[150] were forced to suspend the offensive.[151] Large territorial gains were made by Axis forces, but their campaign had failed to achieve its main objectives: two key cities remained in Soviet hands, the Soviet capability to resist was not broken, and the Soviet Union retained a considerable part of its military potential. The blitzkrieg phase of the war in Europe had ended.[152]
.By early December, freshly mobilised reserves[153] allowed the Soviets to achieve numerical parity with Axis troops.[154] This, as well as intelligence data which established that a minimal number of Soviet troops in the East would be sufficient to deter any attack by the Japanese Kwantung Army,[155] allowed the Soviets to begin a massive counter-offensive that started on 5 December all along the front and pushed German troops 100–250 kilometres (62–155 mi) west.[156]
.Following the Japanese false flag Mukden incident in 1931, the Japanese shelling of the American gunboat USS Panay in 1937, and the 1937–1938 Nanjing Massacre, Japanese-American relations deteriorated. In 1939, the United States notified Japan that it would not be extending its trade treaty and American public opinion opposing Japanese expansionism led to a series of economic sanctions—the Export Control Acts—which banned US exports of chemicals, minerals and military parts to Japan, and increased economic pressure on the Japanese regime.[113][157][158] During 1939 Japan launched its first attack against Changsha, but was repulsed by late September.[159] Despite several offensives by both sides, by 1940 the war between China and Japan was at a stalemate. To increase pressure on China by blocking supply routes, and to better position Japanese forces in the event of a war with the Western powers, Japan invaded and occupied northern Indochina in September 1940.[160]
.Chinese nationalist forces launched a large-scale counter-offensive in early 1940. In August, Chinese communists launched an offensive in Central China; in retaliation, Japan instituted harsh measures in occupied areas to reduce human and material resources for the communists.[161] Continued antipathy between Chinese communist and nationalist forces culminated in armed clashes in January 1941, effectively ending their co-operation.[162] In March, the Japanese 11th army attacked the headquarters of the Chinese 19th army but was repulsed during the Battle of Shanggao.[163] In September, Japan attempted to take the city of Changsha again and clashed with Chinese nationalist forces.[164]
.German successes in Europe prompted Japan to increase pressure on European governments in Southeast Asia. The Dutch government agreed to provide Japan with oil supplies from the Dutch East Indies, but negotiations for additional access to their resources ended in failure in June 1941.[165] In July 1941 Japan sent troops to southern Indochina, thus threatening British and Dutch possessions in the Far East. The United States, the United Kingdom, and other Western governments reacted to this move with a freeze on Japanese assets and a total oil embargo.[166][167] At the same time, Japan was planning an invasion of the Soviet Far East, intending to take advantage of the German invasion in the west, but abandoned the operation after the sanctions.[168]
.Since early 1941, the United States and Japan had been engaged in negotiations in an attempt to improve their strained relations and end the war in China. During these negotiations, Japan advanced a number of proposals which were dismissed by the Americans as inadequate.[169] At the same time the United States, the United Kingdom, and the Netherlands engaged in secret discussions for the joint defence of their territories, in the event of a Japanese attack against any of them.[170] Roosevelt reinforced the Philippines (an American protectorate scheduled for independence in 1946) and warned Japan that the United States would react to Japanese attacks against any "neighboring countries".[170]
.Frustrated at the lack of progress and feeling the pinch of the American–British–Dutch sanctions, Japan prepared for war. Emperor Hirohito, after initial hesitation about Japan's chances of victory,[171] began to favour Japan's entry into the war.[172] As a result, Prime Minister Fumimaro Konoe resigned.[173][174] Hirohito refused the recommendation to appoint Prince Naruhiko Higashikuni in his place, choosing War Minister Hideki Tojo instead.[175] On 3 November, Nagano explained in detail the plan of the attack on Pearl Harbor to the Emperor.[176] On 5 November, Hirohito approved in imperial conference the operations plan for the war.[177] On 20 November, the new government presented an interim proposal as its final offer. It called for the end of American aid to China and for lifting the embargo on the supply of oil and other resources to Japan. In exchange, Japan promised not to launch any attacks in Southeast Asia and to withdraw its forces from southern Indochina.[169] The American counter-proposal of 26 November required that Japan evacuate all of China without conditions and conclude non-aggression pacts with all Pacific powers.[178] That meant Japan was essentially forced to choose between abandoning its ambitions in China, or seizing the natural resources it needed in the Dutch East Indies by force;[179][180] the Japanese military did not consider the former an option, and many officers considered the oil embargo an unspoken declaration of war.[181]
.Japan planned to seize European colonies in Asia to create a large defensive perimeter stretching into the Central Pacific. The Japanese would then be free to exploit the resources of Southeast Asia while exhausting the over-stretched Allies by fighting a defensive war.[182][183] To prevent American intervention while securing the perimeter, it was further planned to neutralise the United States Pacific Fleet and the American military presence in the Philippines from the outset.[184] On 7 December 1941 (8 December in Asian time zones), Japan attacked British and American holdings with near-simultaneous offensives against Southeast Asia and the Central Pacific.[185] These included an attack on the American fleets at Pearl Harbor and the Philippines, as well as invasions of Guam, Wake Island, Malaya,[185] Thailand, and Hong Kong.[186]
.These attacks led the United States, United Kingdom, China, Australia, and several other states to formally declare war on Japan, whereas the Soviet Union, being heavily involved in large-scale hostilities with European Axis countries, maintained its neutrality agreement with Japan.[187] Germany, followed by the other Axis states, declared war on the United States[188] in solidarity with Japan, citing as justification the American attacks on German war vessels that had been ordered by Roosevelt.[135][189]
.On 1 January 1942, the Allied Big Four[190]—the Soviet Union, China, the United Kingdom, and the United States—and 22 smaller or exiled governments issued the Declaration by United Nations, thereby affirming the Atlantic Charter[191] and agreeing not to sign a separate peace with the Axis powers.[192]
.During 1942, Allied officials debated on the appropriate grand strategy to pursue. All agreed that defeating Germany was the primary objective. The Americans favoured a straightforward, large-scale attack on Germany through France. The Soviets demanded a second front. The British argued that military operations should target peripheral areas to wear out German strength, leading to increasing demoralisation, and bolstering resistance forces; Germany itself would be subject to a heavy bombing campaign. An offensive against Germany would then be launched primarily by Allied armour, without using large-scale armies.[193] Eventually, the British persuaded the Americans that a landing in France was infeasible in 1942 and they should instead focus on driving the Axis out of North Africa.[194]
.At the Casablanca Conference in early 1943, the Allies reiterated the statements issued in the 1942 Declaration and demanded the unconditional surrender of their enemies. The British and Americans agreed to continue to press the initiative in the Mediterranean by invading Sicily to fully secure the Mediterranean supply routes.[195] Although the British argued for further operations in the Balkans to bring Turkey into the war, in May 1943, the Americans extracted a British commitment to limit Allied operations in the Mediterranean to an invasion of the Italian mainland, and to invade France in 1944.[196]
.By the end of April 1942, Japan and its ally Thailand had almost conquered Burma, Malaya, the Dutch East Indies, Singapore, and Rabaul, inflicting severe losses on Allied troops and taking a large number of prisoners.[197] Despite stubborn resistance by Filipino and US forces, the Philippine Commonwealth was eventually captured in May 1942, forcing its government into exile.[198] On 16 April, in Burma, 7,000 British soldiers were encircled by the Japanese 33rd Division during the Battle of Yenangyaung and rescued by the Chinese 38th Division.[199] Japanese forces also achieved naval victories in the South China Sea, Java Sea, and Indian Ocean,[200] and bombed the Allied naval base at Darwin, Australia. In January 1942, the only Allied success against Japan was a Chinese victory at Changsha.[201] These easy victories over the unprepared US and European opponents left Japan overconfident, and overextended.[202]
.In early May 1942, Japan initiated operations to capture Port Moresby by amphibious assault and thus sever communications and supply lines between the United States and Australia. The planned invasion was thwarted when an Allied task force, centred on two American fleet carriers, fought Japanese naval forces to a draw in the Battle of the Coral Sea.[203] Japan's next plan, motivated by the earlier Doolittle Raid, was to seize Midway Atoll and lure American carriers into battle to be eliminated; as a diversion, Japan would also send forces to occupy the Aleutian Islands in Alaska.[204] In mid-May, Japan started the Zhejiang-Jiangxi campaign in China, with the goal of inflicting retribution on the Chinese who aided the surviving American airmen in the Doolittle Raid by destroying Chinese air bases and fighting against the Chinese 23rd and 32nd Army Groups.[205][206] In early June, Japan put its operations into action, but the Americans had broken Japanese naval codes in late May and were fully aware of the plans and order of battle, and used this knowledge to achieve a decisive victory at Midway over the Imperial Japanese Navy.[207]
.With its capacity for aggressive action greatly diminished as a result of the Midway battle, Japan attempted to capture Port Moresby by an overland campaign in the Territory of Papua.[208] The Americans planned a counterattack against Japanese positions in the southern Solomon Islands, primarily Guadalcanal, as a first step towards capturing Rabaul, the main Japanese base in Southeast Asia.[209]
.Both plans started in July, but by mid-September, the Battle for Guadalcanal took priority for the Japanese, and troops in New Guinea were ordered to withdraw from the Port Moresby area to the northern part of the island, where they faced Australian and United States troops in the Battle of Buna–Gona.[210] Guadalcanal soon became a focal point for both sides with heavy commitments of troops and ships in the battle for Guadalcanal. By the start of 1943, the Japanese were defeated on the island and withdrew their troops.[211] In Burma, Commonwealth forces mounted two operations. The first was a disastrous offensive into the Arakan region in late 1942 that forced a retreat back to India by May 1943.[212] The second was the insertion of irregular forces behind Japanese frontlines in February which, by the end of April, had achieved mixed results.[213]
.Despite considerable losses, in early 1942 Germany and its allies stopped a major Soviet offensive in central and southern Russia, keeping most territorial gains they had achieved during the previous year.[214] In May, the Germans defeated Soviet offensives in the Kerch Peninsula and at Kharkov,[215] and then in June 1942 launched their main summer offensive against southern Russia, to seize the oil fields of the Caucasus and occupy the Kuban steppe, while maintaining positions on the northern and central areas of the front. The Germans split Army Group South into two groups: Army Group A advanced to the lower Don River and struck south-east to the Caucasus, while Army Group B headed towards the Volga River. The Soviets decided to make their stand at Stalingrad on the Volga.[216]
.By mid-November, the Germans had nearly taken Stalingrad in bitter street fighting. The Soviets began their second winter counter-offensive, starting with an encirclement of German forces at Stalingrad,[217] and an assault on the Rzhev salient near Moscow, though the latter failed disastrously.[218] By early February 1943, the German Army had taken tremendous losses; German troops at Stalingrad had been defeated,[219] and the front-line had been pushed back beyond its position before the summer offensive. In mid-February, after the Soviet push had tapered off, the Germans launched another attack on Kharkov, creating a salient in their front line around the Soviet city of Kursk.[220]
.Exploiting poor American naval command decisions, the German navy ravaged Allied shipping off the American Atlantic coast.[221] By November 1941, Commonwealth forces had launched a counter-offensive in North Africa, Operation Crusader, and reclaimed all the gains the Germans and Italians had made.[222] The Germans also launched a North African offensive in January, pushing the British back to positions at the Gazala line by early February,[223] followed by a temporary lull in combat which Germany used to prepare for their upcoming offensives.[224] Concerns that the Japanese might use bases in Vichy-held Madagascar caused the British to invade the island in early May 1942.[225] An Axis offensive in Libya forced an Allied retreat deep inside Egypt until Axis forces were stopped at El Alamein.[226] On the Continent, raids of Allied commandos on strategic targets, culminating in the failed Dieppe Raid,[227] demonstrated the Western Allies' inability to launch an invasion of continental Europe without much better preparation, equipment, and operational security.[228]
.In August 1942, the Allies succeeded in repelling a second attack against El Alamein[229] and, at a high cost, managed to deliver desperately needed supplies to the besieged Malta.[230] A few months later, the Allies commenced an attack of their own in Egypt, dislodging the Axis forces and beginning a drive west across Libya.[231] This attack was followed up shortly after by Anglo-American landings in French North Africa, which resulted in the region joining the Allies.[232] Hitler responded to the French colony's defection by ordering the occupation of Vichy France;[232] although Vichy forces did not resist this violation of the armistice, they managed to scuttle their fleet to prevent its capture by German forces.[232][233] Axis forces in Africa withdrew into Tunisia, which was conquered by the Allies in May 1943.[232][234]
.In June 1943, the British and Americans began a strategic bombing campaign against Germany with a goal to disrupt the war economy, reduce morale, and "de-house" the civilian population.[235] The firebombing of Hamburg was among the first attacks in this campaign, inflicting significant casualties and considerable losses on infrastructure of this important industrial centre.[236]
.After the Guadalcanal campaign, the Allies initiated several operations against Japan in the Pacific. In May 1943, Canadian and US forces were sent to eliminate Japanese forces from the Aleutians.[237] Soon after, the United States, with support from Australia, New Zealand and Pacific Islander forces, began major ground, sea and air operations to isolate Rabaul by capturing surrounding islands, and breach the Japanese Central Pacific perimeter at the Gilbert and Marshall Islands.[238] By the end of March 1944, the Allies had completed both of these objectives and had also neutralised the major Japanese base at Truk in the Caroline Islands. In April, the Allies launched an operation to retake Western New Guinea.[239]
.In the Soviet Union, both the Germans and the Soviets spent the spring and early summer of 1943 preparing for large offensives in central Russia. On 5 July 1943, Germany attacked Soviet forces around the Kursk Bulge. Within a week, German forces had exhausted themselves against the Soviets' well-constructed defences,[240] and for the first time in the war, Hitler cancelled an operation before it had achieved tactical or operational success.[241] This decision was partially affected by the Western Allies' invasion of Sicily launched on 9 July, which, combined with previous Italian failures, resulted in the ousting and arrest of Mussolini later that month.[242]
.On 12 July 1943, the Soviets launched their own counter-offensives, thereby dispelling any chance of German victory or even stalemate in the east. The Soviet victory at Kursk marked the end of German superiority,[243] giving the Soviet Union the initiative on the Eastern Front.[244][245] The Germans tried to stabilise their eastern front along the hastily fortified Panther–Wotan line, but the Soviets broke through it at Smolensk and the Lower Dnieper Offensive.[246]
.On 3 September 1943, the Western Allies invaded the Italian mainland, following Italy's armistice with the Allies and the ensuing German occupation of Italy.[247] Germany, with the help of fascists, responded to the armistice by disarming Italian forces that were in many places without superior orders, seizing military control of Italian areas,[248] and creating a series of defensive lines.[249] German special forces then rescued Mussolini, who then soon established a new client state in German-occupied Italy named the Italian Social Republic,[250] causing an Italian civil war. The Western Allies fought through several lines until reaching the main German defensive line in mid-November.[251]
.German operations in the Atlantic also suffered. By May 1943, as Allied counter-measures became increasingly effective, the resulting sizeable German submarine losses forced a temporary halt of the German Atlantic naval campaign.[252] In November 1943, Franklin D. Roosevelt and Winston Churchill met with Chiang Kai-shek in Cairo and then with Joseph Stalin in Tehran.[253] The former conference determined the post-war return of Japanese territory[254] and the military planning for the Burma campaign,[255] while the latter included agreement that the Western Allies would invade Europe in 1944 and that the Soviet Union would declare war on Japan within three months of Germany's defeat.[256]
.From November 1943, during the seven-week Battle of Changde, the Chinese awaited allied relief as they forced Japan to fight a costly war of attrition.[257][258][259] In January 1944, the Allies launched a series of attacks in Italy against the line at Monte Cassino and tried to outflank it with landings at Anzio.[260]
.On 27 January 1944, Soviet troops launched a major offensive that expelled German forces from the Leningrad region, thereby ending the most lethal siege in history.[261] The following Soviet offensive was halted on the pre-war Estonian border by the German Army Group North aided by Estonians hoping to re-establish national independence. This delay slowed subsequent Soviet operations in the Baltic Sea region.[262] By late May 1944, the Soviets had liberated Crimea, largely expelled Axis forces from Ukraine, and made incursions into Romania, which were repulsed by the Axis troops.[263] The Allied offensives in Italy had succeeded and, at the expense of allowing several German divisions to retreat, Rome was captured on 4 June.[264]
.The Allies had mixed success in mainland Asia. In March 1944, the Japanese launched the first of two invasions, an operation against Allied positions in Assam, India,[265] and soon besieged Commonwealth positions at Imphal and Kohima.[266] In May 1944, British and Indian forces mounted a counter-offensive that drove Japanese troops back to Burma by July,[266] and Chinese forces that had invaded northern Burma in late 1943 besieged Japanese troops in Myitkyina.[267] The second Japanese invasion of China aimed to destroy China's main fighting forces, secure railways between Japanese-held territory and capture Allied airfields.[268] By June, the Japanese had conquered the province of Henan and begun a new attack on Changsha.[269]
.On 6 June 1944 (commonly known as D-Day), after three years of Soviet pressure,[270] the Western Allies invaded northern France. After reassigning several Allied divisions from Italy, they also attacked southern France.[271] These landings were successful and led to the defeat of the German Army units in France. Paris was liberated on 25 August by the local resistance assisted by the Free French Forces, both led by General Charles de Gaulle,[272] and the Western Allies continued to push back German forces in western Europe during the latter part of the year. An attempt to advance into northern Germany spearheaded by a major airborne operation in the Netherlands failed.[273] After that, the Western Allies slowly pushed into Germany, but failed to cross the Ruhr river. In Italy, the Allied advance slowed due to the last major German defensive line.[274]
.On 22 June, the Soviets launched a strategic offensive in Belarus ("Operation Bagration") that nearly destroyed the German Army Group Centre.[275] Soon after that, another Soviet strategic offensive forced German troops from Western Ukraine and Eastern Poland. The Soviets formed the Polish Committee of National Liberation to control territory in Poland and combat the Polish Armia Krajowa; the Soviet Red Army remained in the Praga district on the other side of the Vistula and watched passively as the Germans quelled the Warsaw Uprising initiated by the Armia Krajowa.[276] The national uprising in Slovakia was also quelled by the Germans.[277] The Soviet Red Army's strategic offensive in eastern Romania cut off and destroyed the considerable German troops there and triggered a successful coup d'état in Romania and in Bulgaria, followed by those countries' shift to the Allied side.[278]
.In September 1944, Soviet troops advanced into Yugoslavia and forced the rapid withdrawal of German Army Groups E and F in Greece, Albania and Yugoslavia to rescue them from being cut off.[279] By this point, the communist-led Partisans under Marshal Josip Broz Tito, who had led an increasingly successful guerrilla campaign against the occupation since 1941, controlled much of the territory of Yugoslavia and engaged in delaying efforts against German forces further south. In northern Serbia, the Soviet Red Army, with limited support from Bulgarian forces, assisted the Partisans in a joint liberation of the capital city of Belgrade on 20 October. A few days later, the Soviets launched a massive assault against German-occupied Hungary that lasted until the fall of Budapest in February 1945.[280] Unlike impressive Soviet victories in the Balkans, bitter Finnish resistance to the Soviet offensive in the Karelian Isthmus denied the Soviets occupation of Finland and led to a Soviet-Finnish armistice on relatively mild conditions,[281] although Finland was forced to fight their former German allies.[282]
.By the start of July 1944, Commonwealth forces in Southeast Asia had repelled the Japanese sieges in Assam, pushing the Japanese back to the Chindwin River[283] while the Chinese captured Myitkyina. In September 1944, Chinese forces captured Mount Song and reopened the Burma Road.[284] In China, the Japanese had more successes, having finally captured Changsha in mid-June and the city of Hengyang by early August.[285] Soon after, they invaded the province of Guangxi, winning major engagements against Chinese forces at Guilin and Liuzhou by the end of November[286] and successfully linking up their forces in China and Indochina by mid-December.[287]
.In the Pacific, US forces continued to push back the Japanese perimeter. In mid-June 1944, they began their offensive against the Mariana and Palau islands and decisively defeated Japanese forces in the Battle of the Philippine Sea. These defeats led to the resignation of the Japanese Prime Minister, Hideki Tojo, and provided the United States with air bases to launch intensive heavy bomber attacks on the Japanese home islands. In late October, American forces invaded the Filipino island of Leyte; soon after, Allied naval forces scored another large victory in the Battle of Leyte Gulf, one of the largest naval battles in history.[288]
.On 16 December 1944, Germany made a last attempt to split the Allies on the Western Front by using most of its remaining reserves to launch a massive counter-offensive in the Ardennes and along the French-German border, hoping to encircle large portions of Western Allied troops and prompt a political settlement after capturing their primary supply port at Antwerp. By 16 January 1945, this offensive had been repulsed with no strategic objectives fulfilled.[289] In Italy, the Western Allies remained stalemated at the German defensive line. In mid-January 1945, the Red Army attacked in Poland, pushing from the Vistula to the Oder river in Germany, and overran East Prussia.[290] On 4 February Soviet, British, and US leaders met for the Yalta Conference. They agreed on the occupation of post-war Germany, and on when the Soviet Union would join the war against Japan.[291]
.In February, the Soviets entered Silesia and Pomerania, while the Western Allies entered western Germany and closed to the Rhine river. By March, the Western Allies crossed the Rhine north and south of the Ruhr, encircling the German Army Group B.[292] In early March, in an attempt to protect its last oil reserves in Hungary and retake Budapest, Germany launched its last major offensive against Soviet troops near Lake Balaton. Within two weeks, the offensive had been repulsed, the Soviets advanced to Vienna, and captured the city. In early April, Soviet troops captured Königsberg, while the Western Allies finally pushed forward in Italy and swept across western Germany capturing Hamburg and Nuremberg. American and Soviet forces met at the Elbe river on 25 April, leaving unoccupied pockets in southern Germany and around Berlin.
.Soviet troops stormed and captured Berlin in late April.[293] In Italy, German forces surrendered on 29 April, while the Italian Social Republic capitulated two days later. On 30 April, the Reichstag was captured, signalling the military defeat of Nazi Germany.[294]
.Major changes in leadership occurred on both sides during this period. On 12 April, President Roosevelt died and was succeeded by his vice president, Harry S. Truman.[295] Benito Mussolini was killed by Italian partisans on 28 April.[296] On 30 April, Hitler committed suicide in his headquarters, and was succeeded by Grand Admiral Karl Dönitz (as President of the Reich) and Joseph Goebbels (as Chancellor of the Reich); Goebbels also committed suicide on the following day and was replaced by Lutz Graf Schwerin von Krosigk, in what would later be known as the Flensburg Government. Total and unconditional surrender in Europe was signed on 7 and 8 May, to be effective by the end of 8 May.[297] German Army Group Centre resisted in Prague until 11 May.[298] On 23 May all remaining members of the German government were arrested by the Allied Forces in Flensburg, while on 5 June all German political and military institutions were transferred under the control of the Allies through the Berlin Declaration.[299]
.In the Pacific theatre, American forces accompanied by the forces of the Philippine Commonwealth advanced in the Philippines, clearing Leyte by the end of April 1945. They landed on Luzon in January 1945 and recaptured Manila in March. Fighting continued on Luzon, Mindanao, and other islands of the Philippines until the end of the war.[300] Meanwhile, the United States Army Air Forces launched a massive firebombing campaign of strategic cities in Japan in an effort to destroy Japanese war industry and civilian morale. A devastating bombing raid on Tokyo of 9–10 March was the deadliest conventional bombing raid in history.[301]
.In May 1945, Australian troops landed in Borneo, overrunning the oilfields there. British, American, and Chinese forces defeated the Japanese in northern Burma in March, and the British pushed on to reach Rangoon by 3 May.[302] Chinese forces started a counterattack in the Battle of West Hunan that occurred between 6 April and 7 June 1945. American naval and amphibious forces also moved towards Japan, taking Iwo Jima by March, and Okinawa by the end of June.[303] At the same time, a naval blockade by submarines was strangling Japan's economy and drastically reducing its ability to supply overseas forces.[304][305]
.On 11 July, Allied leaders met in Potsdam, Germany. They confirmed earlier agreements about Germany,[306] and the American, British and Chinese governments reiterated the demand for unconditional surrender of Japan, specifically stating that "the alternative for Japan is prompt and utter destruction".[307] During this conference, the United Kingdom held its general election, and Clement Attlee replaced Churchill as Prime Minister.[308]
.The call for unconditional surrender was rejected by the Japanese government, which believed it would be capable of negotiating for more favourable surrender terms.[309] In early August, the United States dropped atomic bombs on the Japanese cities of Hiroshima and Nagasaki. Between the two bombings, the Soviets, pursuant to the Yalta agreement, declared war on Japan, invaded Japanese-held Manchuria and quickly defeated the Kwantung Army, which was the largest Japanese fighting force.[310] These two events persuaded previously adamant Imperial Army leaders to accept surrender terms.[311] The Red Army also captured the southern part of Sakhalin Island and the Kuril Islands. On the night of 9–10 August 1945, Emperor Hirohito announced his decision to accept the terms demanded by the Allies in the Potsdam Declaration.[312] On 15 August, the Emperor communicated this decision to the Japanese people through a speech broadcast on the radio (Gyokuon-hōsō, literally "broadcast in the Emperor's voice").[313] On 15 August 1945, Japan surrendered, with the surrender documents finally signed at Tokyo Bay on the deck of the American battleship USS Missouri on 2 September 1945, ending the war.[314]
.The Allies established occupation administrations in Austria and Germany, both initially divided between western and eastern occupation zones controlled by the Western Allies and the Soviet Union, respectively. However, their paths soon diverged. In Germany, the western and eastern occupation zones controlled by the Western Allies and the Soviet Union officially ended in 1949, with the respective zones becoming separate countries, West Germany and East Germany.[315] In Austria, however, occupation continued until 1955, when a joint settlement between the Western Allies and the Soviet Union permitted the reunification of Austria as a democratic state officially non-aligned with any political bloc (although in practice having better relations with the Western Allies). A denazification program in Germany led to the prosecution of Nazi war criminals in the Nuremberg trials and the removal of ex-Nazis from power, although this policy moved towards amnesty and re-integration of ex-Nazis into West German society.[316]
.Germany lost a quarter of its pre-war (1937) territory. Among the eastern territories, Silesia, Neumark and most of Pomerania were taken over by Poland,[317] and East Prussia was divided between Poland and the Soviet Union, followed by the expulsion to Germany of the nine million Germans from these provinces,[318][319] as well as three million Germans from the Sudetenland in Czechoslovakia. By the 1950s, one-fifth of West Germans were refugees from the east. The Soviet Union also took over the Polish provinces east of the Curzon Line,[320] from which two million Poles were expelled.[319][321] North-east Romania,[322][323] parts of eastern Finland,[324] and the Baltic states were annexed into the Soviet Union.[325][326] Italy lost its monarchy, colonial empire and some European territories.[327]
.In an effort to maintain world peace,[328] the Allies formed the United Nations,[329] which officially came into existence on 24 October 1945,[330] and adopted the Universal Declaration of Human Rights in 1948 as a common standard for all member nations.[331] The great powers that were the victors of the war—France, China, the United Kingdom, the Soviet Union and the United States—became the permanent members of the UN's Security Council.[332] The five permanent members remain so to the present, although there have been two seat changes, between the Republic of China and the People's Republic of China in 1971, and between the Soviet Union and its successor state, the Russian Federation, following the dissolution of the Soviet Union in 1991. The alliance between the Western Allies and the Soviet Union had begun to deteriorate even before the war was over.[333]
.Besides Germany, the rest of Europe was also divided into Western and Soviet spheres of influence.[334] Most eastern and central European countries fell into the Soviet sphere, which led to establishment of Communist-led regimes, with full or partial support of the Soviet occupation authorities. As a result, East Germany,[335] Poland, Hungary, Romania, Bulgaria, Czechoslovakia, and Albania[336] became Soviet satellite states. Communist Yugoslavia conducted a fully independent policy, causing tension with the Soviet Union.[337] A Communist uprising in Greece was put down with Anglo-American support and the country remained aligned with the West.[338]
.Post-war division of the world was formalised by two international military alliances, the United States-led NATO and the Soviet-led Warsaw Pact.[339] The long period of political tensions and military competition between them—the Cold War—would be accompanied by an unprecedented arms race and number of proxy wars throughout the world.[340]
.In Asia, the United States led the occupation of Japan and administered Japan's former islands in the Western Pacific, while the Soviets annexed South Sakhalin and the Kuril Islands.[341] Korea, formerly under Japanese colonial rule, was divided and occupied by the Soviet Union in the North and the United States in the South between 1945 and 1948. Separate republics emerged on both sides of the 38th parallel in 1948, each claiming to be the legitimate government for all of Korea, which led ultimately to the Korean War.[342]
.In China, nationalist and communist forces resumed the civil war in June 1946. Communist forces were victorious and established the People's Republic of China on the mainland, while nationalist forces retreated to Taiwan in 1949.[343] In the Middle East, the Arab rejection of the United Nations Partition Plan for Palestine and the creation of Israel marked the escalation of the Arab–Israeli conflict. While European powers attempted to retain some or all of their colonial empires, their losses of prestige and resources during the war rendered this unsuccessful, leading to decolonisation.[344][345]
.The global economy suffered heavily from the war, although participating nations were affected differently. The United States emerged much richer than any other nation, leading to a baby boom, and by 1950 its gross domestic product per person was much higher than that of any of the other powers, and it dominated the world economy.[346] The Allied occupational authorities pursued a policy of industrial disarmament in Western Germany from 1945 to 1948.[347] Due to international trade interdependencies, this policy led to an economic stagnation in Europe and delayed European recovery from the war for several years.[348][349]
.At the Bretton Woods Conference in July 1944, the Allied nations drew up an economic framework for the post-war world. The agreement created the International Monetary Fund (IMF) and the International Bank for Reconstruction and Development (IBRD), which later became part of the World Bank Group. The Bretton Woods system lasted until 1973.[350] Recovery began with the mid-1948 currency reform in Western Germany, and was sped up by the liberalisation of European economic policy that the US Marshall Plan economic aid (1948–1951) both directly and indirectly caused.[351][352] The post-1948 West German recovery has been called the German economic miracle.[353] Italy also experienced an economic boom[354] and the French economy rebounded.[355] By contrast, the United Kingdom was in a state of economic ruin,[356] and although receiving a quarter of the total Marshall Plan assistance, more than any other European country,[357] it continued in relative economic decline for decades.[358] The Soviet Union, despite enormous human and material losses, also experienced rapid increase in production in the immediate post-war era,[359] having seized and transferred most of Germany's industrial plants and exacted war reparations from its satellite states.[d][360] Japan recovered much later.[361] China returned to its pre-war industrial production by 1952.[362]
.Estimates for the total number of casualties in the war vary, because many deaths went unrecorded.[363] Most suggest 60 million people died, about 20 million military personnel and 40 million civilians.[364][365]
.The Soviet Union alone lost around 27 million people during the war,[366] including 8.7 million military and 19 million civilian deaths.[367] A quarter of the total people in the Soviet Union were wounded or killed.[368] Germany sustained 5.3 million military losses, mostly on the Eastern Front and during the final battles in Germany.[369]
.An estimated 11[370] to 17 million[371] civilians died as a direct or as an indirect result of Hitler's racist policies, including mass killing of around 6 million Jews, along with Roma, homosexuals, at least 1.9 million ethnic Poles[372][373] and millions of other Slavs (including Russians, Ukrainians and Belarusians), and other ethnic and minority groups.[374][371] Between 1941 and 1945, more than 200,000 ethnic Serbs, along with Roma and Jews, were persecuted and murdered by the Axis-aligned Croatian Ustaše in Yugoslavia.[375] Concurrently, Muslims and Croats were persecuted and killed by Serb nationalist Chetniks,[376] with an estimated 50,000–68,000 victims (of which 41,000 were civilians).[377] Also, more than 100,000 Poles were massacred by the Ukrainian Insurgent Army in the Volhynia massacres, between 1943 and 1945.[378] At the same time, about 10,000–15,000 Ukrainians were killed by the Polish Home Army and other Polish units, in reprisal attacks.[379]
.In Asia and the Pacific, the number of people killed by Japanese troops remains contested. According to R.J. Rummel, the Japanese killed between 3 million and more than 10 million people, with the most probable case of almost 6,000,000 people.[380] According to the British historian M. R. D. Foot, civilian deaths are between 10 million and 20 million, whereas Chinese military casualties (killed and wounded) are estimated to be over five million.[381] Other estimates say that up to 30 million people, most of them civilians, were killed.[382][383] The most infamous Japanese atrocity was the Nanjing Massacre, in which fifty to three hundred thousand Chinese civilians were raped and murdered.[384] Mitsuyoshi Himeta reported that 2.7 million casualties occurred during the Three Alls policy. General Yasuji Okamura implemented the policy in Hebei and Shandong.[385]
.Axis forces employed biological and chemical weapons. The Imperial Japanese Army used a variety of such weapons during its invasion and occupation of China (see Unit 731)[386][387] and in early conflicts against the Soviets.[388] Both the Germans and the Japanese tested such weapons against civilians,[389] and sometimes on prisoners of war.[390]
.The Soviet Union was responsible for the Katyn massacre of 22,000 Polish officers,[391] and the imprisonment or execution of hundreds of thousands of political prisoners by the NKVD secret police, along with mass civilian deportations to Siberia, in the Baltic states and eastern Poland annexed by the Red Army.[392] Soviet soldiers committed mass rapes in occupied territories, especially in Germany.[393][394] The exact number of German women and girls raped by Soviet troops during the war and occupation is uncertain, but historians estimate their numbers are likely in the hundreds of thousands, and possibly as many as two million,[395] while figures for women raped by German soldiers in the Soviet Union go as far as ten million.[396][397]
.The mass bombing of cities in Europe and Asia has often been called a war crime, although no positive or specific customary international humanitarian law with respect to aerial warfare existed before or during World War II.[398] The USAAF bombed a total of 67 Japanese cities, killing 393,000 civilians, including the atomic bombings of Hiroshima and Nagasaki, and destroying 65% of built-up areas.[399]
.Nazi Germany, under the dictatorship of Adolf Hitler, was responsible for murdering about 6 million Jews in what is now known as the Holocaust. They also murdered an additional 4 million others who were deemed "unworthy of life" (including the disabled and mentally ill, Soviet prisoners of war, Romani, homosexuals, Freemasons, and Jehovah's Witnesses) as part of a program of deliberate extermination, in effect becoming a "genocidal state".[400] Soviet POWs were kept in especially unbearable conditions, and 3.6 million Soviet POWs out of 5.7 million died in Nazi camps during the war.[401][402] In addition to concentration camps, death camps were created in Nazi Germany to exterminate people on an industrial scale. Nazi Germany extensively used forced labourers; about 12 million Europeans from German-occupied countries were abducted and used as a slave work force in German industry, agriculture and war economy.[403]
.The Soviet Gulag became a de facto system of deadly camps during 1942–1943, when wartime privation and hunger caused numerous deaths of inmates,[405] including foreign citizens of Poland and other countries occupied in 1939–1940 by the Soviet Union, as well as Axis POWs.[406] By the end of the war, most Soviet POWs liberated from Nazi camps and many repatriated civilians were detained in special filtration camps where they were subjected to NKVD evaluation, and 226,127 were sent to the Gulag as real or perceived Nazi collaborators.[407]
.Japanese prisoner-of-war camps, many of which were used as labour camps, also had high death rates. The International Military Tribunal for the Far East found the death rate of Western prisoners was 27 percent (for American POWs, 37 percent),[408] seven times that of POWs under the Germans and Italians.[409] While 37,583 prisoners from the UK, 28,500 from the Netherlands, and 14,473 from the United States were released after the surrender of Japan, the number of Chinese released was only 56.[410]
.At least five million Chinese civilians from northern China and Manchukuo were enslaved between 1935 and 1941 by the East Asia Development Board, or Kōain, for work in mines and war industries. After 1942, the number reached 10 million.[411] In Java, between 4 and 10 million rōmusha (Japanese: "manual labourers"), were forced to work by the Japanese military. About 270,000 of these Javanese labourers were sent to other Japanese-held areas in Southeast Asia, and only 52,000 were repatriated to Java.[412]
.In Europe, occupation came under two forms. In Western, Northern, and Central Europe (France, Norway, Denmark, the Low Countries, and the annexed portions of Czechoslovakia) Germany established economic policies through which it collected roughly 69.5 billion reichsmarks (27.8 billion US dollars) by the end of the war; this figure does not include the plunder of industrial products, military equipment, raw materials and other goods.[413] Thus, the income from occupied nations was over 40 percent of the income Germany collected from taxation, a figure which increased to nearly 40 percent of total German income as the war went on.[414]
.In the East, the intended gains of Lebensraum were never attained as fluctuating front-lines and Soviet scorched earth policies denied resources to the German invaders.[415] Unlike in the West, the Nazi racial policy encouraged extreme brutality against what it considered to be the "inferior people" of Slavic descent; most German advances were thus followed by mass atrocities and war crimes.[416] The Nazis killed an estimated 2.8 million ethnic Poles in addition to Polish-Jewish victims of the Holocaust.[417] Although resistance groups formed in most occupied territories, they did not significantly hamper German operations in either the East[418] or the West[419] until late 1943.
.In Asia, Japan termed nations under its occupation as being part of the Greater East Asia Co-Prosperity Sphere, essentially a Japanese hegemony which it claimed was for purposes of liberating colonised peoples.[420] Although Japanese forces were sometimes welcomed as liberators from European domination, Japanese war crimes frequently turned local public opinion against them.[421] During Japan's initial conquest, it captured 4,000,000 barrels (640,000 m3) of oil (~550,000 tonnes) left behind by retreating Allied forces; and by 1943, was able to get production in the Dutch East Indies up to 50 million barrels (7,900,000 m3) of oil (~6.8 million tonnes), 76 percent of its 1940 output rate.[421]
.In the 1930s Britain and the United States together controlled almost 75% of world mineral output—essential for projecting military power.[422]
.In Europe, before the outbreak of the war, the Allies had significant advantages in both population and economics. In 1938, the Western Allies (United Kingdom, France, Poland and the British Dominions) had a 30 percent larger population and a 30 percent higher gross domestic product than the European Axis powers (Germany and Italy); including colonies, the Allies had more than a 5:1 advantage in population and a nearly 2:1 advantage in GDP.[423] In Asia at the same time, China had roughly six times the population of Japan but only an 89 percent higher GDP; this reduces to three times the population and only a 38 percent higher GDP if Japanese colonies are included.[423]
.The United States produced about two-thirds of all munitions used by the Allies in World War II, including warships, transports, warplanes, artillery, tanks, trucks, and ammunition.[424] Although the Allies' economic and population advantages were largely mitigated during the initial rapid blitzkrieg attacks of Germany and Japan, they became the decisive factor by 1942, after the United States and Soviet Union joined the Allies and the war evolved into one of attrition.[425] While the Allies' ability to out-produce the Axis was partly due to more access to natural resources, other factors, such as Germany and Japan's reluctance to employ women in the labour force,[426] Allied strategic bombing,[427] and Germany's late shift to a war economy[428] contributed significantly. Additionally, neither Germany nor Japan planned to fight a protracted war, and had not equipped themselves to do so.[429] To improve their production, Germany and Japan used millions of slave labourers;[430] Germany enslaved about 12 million people, mostly from Eastern Europe,[403] while Japan used more than 18 million people in Far East Asia.[411][412]
.Aircraft were used for reconnaissance, as fighters, bombers, and ground-support, and each role developed considerably. Innovations included airlift (the capability to quickly move limited high-priority supplies, equipment, and personnel);[431] and strategic bombing (the bombing of enemy industrial and population centres to destroy the enemy's ability to wage war).[432] Anti-aircraft weaponry also advanced, including defences such as radar and surface-to-air artillery, in particular the introduction of the proximity fuze. The use of the jet aircraft was pioneered and led to jets becoming standard in air forces worldwide.[433]
.Advances were made in nearly every aspect of naval warfare, most notably with aircraft carriers and submarines. Although aeronautical warfare had relatively little success at the start of the war, actions at Taranto, Pearl Harbor, and the Coral Sea established the carrier as the dominant capital ship (in place of the battleship).[434][435][436] In the Atlantic, escort carriers became a vital part of Allied convoys, increasing the effective protection radius and helping to close the Mid-Atlantic gap.[437] Carriers were also more economical than battleships due to the relatively low cost of aircraft[438] and because they are not required to be as heavily armoured.[439] Submarines, which had proved to be an effective weapon during the First World War,[440] were expected by all combatants to be important in the second. The British focused development on anti-submarine weaponry and tactics, such as sonar and convoys, while Germany focused on improving its offensive capability, with designs such as the Type VII submarine and wolfpack tactics.[441] Gradually, improving Allied technologies such as the Leigh Light, Hedgehog, Squid, and homing torpedoes proved effective against German submarines.[442]
.Land warfare changed from the static frontlines of trench warfare of World War I, which had relied on improved artillery that outmatched the speed of both infantry and cavalry, to increased mobility and combined arms. The tank, which had been used predominantly for infantry support in the First World War, had evolved into the primary weapon.[443] In the late 1930s, tank design was considerably more advanced than it had been during World War I,[444] and advances continued throughout the war with increases in speed, armour and firepower.[445][446] At the start of the war, most commanders thought enemy tanks should be met by tanks with superior specifications.[447] This idea was challenged by the poor performance of the relatively light early tank guns against armour, and German doctrine of avoiding tank-versus-tank combat. This, along with Germany's use of combined arms, were among the key elements of their highly successful blitzkrieg tactics across Poland and France.[443] Many means of destroying tanks, including indirect artillery, anti-tank guns (both towed and self-propelled), mines, short-ranged infantry antitank weapons, and other tanks were used.[447] Even with large-scale mechanisation, infantry remained the backbone of all forces,[448] and throughout the war, most infantry were equipped similarly to World War I.[449] The portable machine gun spread, a notable example being the German MG 34, and various submachine guns which were suited to close combat in urban and jungle settings.[449] The assault rifle, a late war development incorporating many features of the rifle and submachine gun, became the standard post-war infantry weapon for most armed forces.[450]
.Most major belligerents attempted to solve the problems of complexity and security involved in using large codebooks for cryptography by designing ciphering machines, the most well-known being the German Enigma machine.[451] Development of SIGINT (signals intelligence) and cryptanalysis enabled the countering process of decryption. Notable examples were the Allied decryption of Japanese naval codes[452] and British Ultra, a pioneering method for decoding Enigma that benefited from information given to the United Kingdom by the Polish Cipher Bureau, which had been decoding early versions of Enigma before the war.[453] Another component of military intelligence was deception, which the Allies used to great effect in operations such as Mincemeat and Bodyguard.[452][454]
.Other technological and engineering feats achieved during, or as a result of, the war include the world's first programmable computers (Z3, Colossus, and ENIAC), guided missiles and modern rockets, the Manhattan Project's development of nuclear weapons, operations research, the development of artificial harbours, and oil pipelines under the English Channel.[455] Penicillin was first developed, mass-produced, and used during the war.[456]
.
.
.The National Basketball Association (NBA) is a professional basketball league in North America composed of 30 teams (29 in the United States and 1 in Canada). The NBA is one of the major professional sports leagues in the United States and Canada and is considered the premier professional basketball league in the world.[3] The league is headquartered in Midtown Manhattan.
.The NBA was created on August 3, 1949, with the merger of the Basketball Association of America (BAA) and the National Basketball League (NBL). The league later adopted the BAA's history and considers its founding on June 6, 1946, as its own.[4][1][5] In 1976, the NBA and the American Basketball Association (ABA) merged, adding four franchises to the NBA. The NBA's regular season runs from October to April, with each team playing 82 games. The league's playoff tournament extends into June, culminating with the NBA Finals championship series.
.The NBA is an active member of USA Basketball (USAB),[6] which is recognized by the International Basketball Federation (FIBA) as the governing body for basketball in the United States. The NBA is the second-wealthiest professional sports league in the world by revenue after the National Football League (NFL).[7] As of 2020[update], NBA players are the world's highest paid athletes by average annual salary per player.[8][9][10]
.The Boston Celtics have the most NBA championships with 18 and are the reigning league champions, having defeated the Dallas Mavericks in the 2024 NBA Finals.
.The NBA traces its roots to the Basketball Association of America which was founded in 1946 by owners of the major ice hockey arenas in the Northeastern and Midwestern United States and Canada. On November 1, 1946, in Toronto, Ontario, Canada, the Toronto Huskies hosted the New York Knickerbockers at Maple Leaf Gardens, in a game the NBA now refers to as the first game played in NBA history.[11] The first basket was made by Ossie Schectman of the Knickerbockers.[12]
.Although there had been earlier attempts at professional basketball leagues, including the American Basketball League (ABL) and the NBL, the BAA was the first league to attempt to play primarily in large arenas in major cities. During its early years, the quality of play in the BAA was not significantly better than in competing leagues or among leading independent clubs such as the Harlem Globetrotters. For instance, the 1947 ABL finalist Baltimore Bullets moved to the BAA and won that league's 1948 title,[13][14] and the 1948 NBL champion Minneapolis Lakers won the 1949 BAA title.[15]
.Prior to the 1948–49 season, the BAA lured away the Fort Wayne Pistons, Indianapolis Kautskys, Minneapolis Lakers, and Rochester Royals from the NBL with the prospect of playing in major venues such as Boston Garden and Madison Square Garden.[16] The NBL hit back by outbidding the BAA for the services of several players, including Al Cervi, rookie Dolph Schayes and five stars from the University of Kentucky while also gaining the upper hand in Indianapolis with the creation of the Indianapolis Olympians while the Kautskys folded.[4][17] With several teams facing financial difficulties,[18] the BAA and the NBL agreed on a merger on August 3, 1949, to create the National Basketball Association. Maurice Podoloff, the president of BAA, became the president of the NBA while Ike Duffey, president of the NBL, became the chairman.[19][20] The NBA later adopted the BAA's history and statistics as its own but did not do the same for NBL records and statistics.[21]
.The new league had seventeen franchises located in a mix of large and small cities,[22] as well as large arenas and smaller gymnasiums and armories. In 1950, the NBA consolidated[23] to eleven franchises, a process that continued until 1954–55, when the league reached its smallest size of eight franchises: the New York Knicks, Boston Celtics, Philadelphia Warriors, Minneapolis Lakers, Rochester Royals, Fort Wayne Pistons, Milwaukee Hawks, and Syracuse Nationals, all of which remain in the league today, although the latter six all did eventually relocate. The process of contraction saw the league's smaller-city franchises move to larger cities. The Hawks had shifted from the Tri-Cities to Milwaukee in 1951, and later shifted to St. Louis in 1955. In 1957, the Rochester Royals moved from Rochester, New York, to Cincinnati and the Pistons moved from Fort Wayne, Indiana, to Detroit.[24]
.Japanese-American Wataru Misaka is considered to have broken the NBA color barrier in the 1947–48 season when he played for the New York Knicks in the BAA. He remained the only non-white player in league history prior to the first African-American, Harold Hunter, signing with the Washington Capitols in 1950.[25][26] Hunter was cut from the team during training camp,[25][27] but several African-American players did play in the league later that year, including Chuck Cooper with the Celtics, Nathaniel "Sweetwater" Clifton with the Knicks, and Earl Lloyd with the Washington Capitols. During this period, the Minneapolis Lakers won five NBA championships and established themselves as the league's first dynasty;[28] their squad was led by center George Mikan who was the NBA's first superstar.[29] To encourage shooting and discourage stalling, the league introduced the 24-second shot clock in 1954.[30]
.In 1957, rookie center Bill Russell joined the Boston Celtics, which already featured guard Bob Cousy and coach Red Auerbach, and went on to lead the franchise to eleven NBA titles in thirteen seasons.[31] Center Wilt Chamberlain entered the league with the Warriors in 1959 and became a dominant individual star of the 1960s, setting new single-game records in scoring (100) and rebounding (55). Russell's rivalry with Chamberlain became one of the greatest rivalries in the history of American team sports.[32]
.The 1960s were dominated by the Celtics. Led by Russell, Cousy, and Auerbach, Boston won eight straight championships in the NBA from 1959 to 1966. This championship streak is the longest in the history of American professional sports.[33] They did not win the title in 1966–67, but regained it in the 1967–68 season and repeated in 1969. The domination totaled nine of the ten championship banners of the 1960s.[34]
.Through this period, the NBA continued to evolve with the shift of the Minneapolis Lakers to Los Angeles, the Philadelphia Warriors to San Francisco, the Syracuse Nationals to Philadelphia to become the Philadelphia 76ers, and the St. Louis Hawks moving to Atlanta, as well as the addition of its first expansion franchises. The Chicago Packers (now Washington Wizards) became the ninth NBA team in 1961.[35] From 1966 to 1968, the league expanded from 9 to 14 teams, introducing the Chicago Bulls, Seattle SuperSonics (now Oklahoma City Thunder), San Diego Rockets (who moved to Houston four years later), Milwaukee Bucks, and Phoenix Suns.
.In 1967, the league faced a new external threat with the formation of the American Basketball Association (ABA). The leagues engaged in a bidding war.[36][37] The NBA landed the most important college star of the era, Kareem Abdul-Jabbar (then known as Lew Alcindor), who went on to become the league's best player of the 1970s.[38] However, the NBA's leading scorer, Rick Barry, jumped to the ABA, as did four veteran referees—Norm Drucker, Earl Strom, John Vanak, and Joe Gushue.[39]
.In 1969, Alan Siegel, who oversaw the design of Jerry Dior's Major League Baseball logo a year prior, created the modern NBA logo inspired by the MLB's. It incorporates the silhouette of Jerry West, based on a photo by Wen Roberts. The NBA would not confirm that a particular player was used because, according to Siegel, "They want to institutionalize it rather than individualize it. It's become such a ubiquitous, classic symbol and focal point of their identity and their licensing program that they don't necessarily want to identify it with one player." The logo debuted in 1971 (with a small change to the typeface on the NBA wordmark in 2017) and would remain a fixture of the NBA brand.[40]
.The ABA succeeded in signing a number of major stars in the 1970s, including Julius Erving of the Virginia Squires, in part because it allowed teams to sign college undergraduates. The NBA expanded rapidly during this period. From 1966 to 1974, the NBA grew from nine franchises to 18.[36] In 1970, the Portland Trail Blazers, Cleveland Cavaliers, and Buffalo Braves (now the Los Angeles Clippers) all made their debuts expanding the league to 17.[41] The New Orleans Jazz (now in Utah) came aboard in 1974 bringing the total to 18. Following the 1976 season, the leagues reached a settlement that provided for the addition of four ABA franchises to the NBA, raising the number of franchises in the league at that time to 22. The franchises added were the San Antonio Spurs, Denver Nuggets, Indiana Pacers, and New York Nets (now the Brooklyn Nets).[42] Some of the biggest stars of this era were Abdul-Jabbar, Barry, Dave Cowens, Erving, Elvin Hayes, Walt Frazier, Moses Malone, Artis Gilmore, George Gervin, Dan Issel, and Pete Maravich. The end of the decade, however, saw declining television ratings, low attendance and drug-related player issues – both perceived and real – that threatened to derail the league.[43]
.The league added the ABA's three-point field goal beginning in 1979.[44] That same year, rookies Larry Bird and Magic Johnson joined the Boston Celtics and Los Angeles Lakers respectively, initiating a period of significant growth of fan interest in the NBA.[45] The two had faced each other in the 1979 NCAA Division I Basketball Championship Game, and they later played against each other in three NBA Finals (1984, 1985, and 1987).[45] In the 10 seasons of the 1980s, Johnson led the Lakers to five titles[46] while Bird led the Celtics to three titles.[47] Also in the early 1980s, the NBA added one more expansion franchise, the Dallas Mavericks,[48] bringing the total to 23 teams. Later on, Larry Bird won the first three three-point shooting contests.[49] On February 1, 1984 David Stern became commissioner of the NBA.[50] Stern has been recognized as playing a major role in the growth of the league during his career.[51][52]
.Michael Jordan entered the league in 1984 with the Chicago Bulls, spurring more interest in the league.[53] In 1988 and 1989, four cities got their wishes as the Charlotte Hornets, Miami Heat, Orlando Magic, and Minnesota Timberwolves made their NBA debuts, bringing the total to 27 teams.[54] The Detroit Pistons won back-to-back NBA championships in 1989 and 1990, led by coach Chuck Daly and guard Isiah Thomas.[55] Jordan and Scottie Pippen led the Bulls to two three-peats in eight years during the 1991–1998 seasons.[56][57] Hakeem Olajuwon won back-to-back titles with the Houston Rockets in 1994 and 1995.[58]
.The 1992 Olympic basketball Dream Team, the first to use current NBA stars, featured Michael Jordan as the anchor, along with Bird, Johnson, David Robinson, Patrick Ewing, Scottie Pippen, Clyde Drexler, Karl Malone, John Stockton, Chris Mullin, Charles Barkley, and star NCAA amateur Christian Laettner.[59] The team was elected to the Naismith Memorial Basketball Hall of Fame, while 11 of the 12 players (along with three out of four coaches) have been inducted as individuals in their own right.[60]
.In 1995, the NBA expanded to Canada with the addition of the Vancouver Grizzlies and the Toronto Raptors.[61][62] In 1996, the NBA created a women's league, the Women's National Basketball Association (WNBA).[63]
.In 1998, the NBA owners began a lockout that suspended all league business until a new labor agreement could be reached, which led to the season being shortened to 50 games.[64][65]
.After the breakup of the Chicago Bulls championship roster in the summer of 1998, the Western Conference dominated much of the next two decades.[66] The Los Angeles Lakers, coached by Phil Jackson, and the San Antonio Spurs, coached by Gregg Popovich, combined to make 13 Finals in 16 seasons, with 10 titles.[67] "Twin Towers" Tim Duncan and David Robinson won the 1999 championship with the Spurs, becoming the first former ABA team to win the NBA championship.[68] Shaquille O'Neal and Kobe Bryant started the 2000s with three consecutive championships for the Lakers.[69] The Spurs reclaimed the title in 2003 against the Nets.[70] In 2004, the Lakers returned to the Finals, only to lose in five games to the Detroit Pistons.[71]
.The league's image was marred by a violent incident between players and fans in a November 2004 game between the Indiana Pacers and Detroit Pistons.[72] In response, players were suspended for a total of 146 games with $11 million total lost in salary, and the league tightened security and limited the sale of alcohol.[72]
.On May 19, 2005, Commissioner Stern testified before the U.S. House of Representatives' Committee on Government Reform about the NBA's actions to combat the use of steroids and other performance-enhancing drugs. The NBA started its drug-testing program in 1983 and substantially improved it in 1999. In the 1999–2000 season, all players were randomly tested during training camp, and all rookies were additionally tested three more times during the regular season. Of the nearly 4,200 tests for steroids and performance-enhancing drugs conducted over six seasons, only three players were confirmed positive for NBA's drug program, all were immediately suspended, and as of the time of the testimony, none were playing in the NBA.[73]
.After the Spurs won the championship again in 2005, the 2006 Finals featured two franchises making their inaugural Finals appearances.[74] The Miami Heat, led by their star shooting guard, Dwyane Wade, and Shaquille O'Neal, who had been traded from the Lakers during the summer of 2004,[75] won the series over the Dallas Mavericks.[76] The Lakers/Spurs dominance continued in 2007 with a four-game sweep by the Spurs over the LeBron James-led Cleveland Cavaliers.[77] The 2008 Finals saw a rematch of the league's highest profile rivalry, the Boston Celtics and Los Angeles Lakers, with the Celtics winning their 17th championship.[78] The Lakers won back-to-back championships in 2009 and 2010, against the Orlando Magic and the Celtics.[79][80] The 2010 NBA All-Star Game was held at Cowboys Stadium in front of the largest crowd ever, 108,713.[81]
.A referee lockout began on September 1, 2009, when the contract between the NBA and its referees expired. The first preseason games were played on October 1, 2009, and replacement referees from the WNBA and NBA Development League were used, the first time replacement referees had been used since the beginning of the 1995–96 season. The NBA and the regular referees reached a deal on October 23, 2009.[82][83]
.At the start of the 2010–11 season, free agents LeBron James and Chris Bosh signed with the Miami Heat, joining Dwyane Wade to form the "Big Three".[84] The Heat dominated the league,[85] reaching the Finals for four straight years.[86] In 2011, they faced a re-match with the Dallas Mavericks but lost to the Dirk Nowitzki-led team.[87] They won back-to-back titles in 2012 and 2013 against the Oklahoma City Thunder and the Spurs,[88] and lost in a re-match with the Spurs in the 2014 Finals.[89]
.The 2011–12 season began with another lockout, the league's fourth.[90] After the first few weeks of the season were canceled, the players and owners ratified a new collective bargaining agreement on December 8, 2011, setting up a shortened 66-game season.[91] On February 1, 2014, commissioner David Stern retired after 30 years in the position, and was succeeded by his deputy, Adam Silver.[92]
.After four seasons with the Miami Heat, LeBron James returned to the Cleveland Cavaliers for the 2014–15 season.[93] He led the team to their second Finals appearance with the help of Kyrie Irving and Kevin Love. The Golden State Warriors defeated the Cavaliers in six games, led by the "Splash Brothers" Stephen Curry and Klay Thompson. The Cavaliers and the Warriors faced each other in the Finals a record four consecutive times.[94] In the 2015–16 season, the Warriors finished the season 73–9, the best season record in NBA history.[95] However, the Cavaliers overcame a 3–1 deficit in the Finals to win their first championship that season,[96] and end a 52-year professional sports championship drought for the city of Cleveland.[97] In the 2016–17 season, the Warriors recruited free agent Kevin Durant[98] and went on to win the 2017 and 2018 Finals against the Cavaliers.[99]
.After the departure of James in free agency in 2018, the Cavaliers' streak of playoff and Finals appearances ended. The Warriors returned for a fifth consecutive Finals appearance in 2019 but lost to the Toronto Raptors, who won their first championship after acquiring Kawhi Leonard in a trade.[100]
.The 2019–20 season was suspended indefinitely on March 11, 2020, due to the COVID-19 pandemic, after Utah Jazz center Rudy Gobert tested positive for the coronavirus.[101][102] On June 4, 2020, the NBA Board of Governors voted to resume the season in a 22-team format with 8 seeding games per team and a regular playoffs format, with all games played in a "bubble" in Walt Disney World without any fans present.[103][104][105]
.This era also saw the continuous near year-over-year decline in NBA viewership. Between 2012 and 2019, the league lost 40 to 45 percent of its viewership. While some of it can be attributed to "cable-cutting", other professional leagues, like the NFL and MLB have retained stable viewership demographics. The opening game of the 2020 Finals between the Los Angeles Lakers and Miami Heat brought in only 7.41 million viewers to ABC, according to The Hollywood Reporter. That is reportedly the lowest viewership seen for the Finals since at least 1994, when total viewers began to be regularly recorded and is a 45 percent decline from game one between the Golden State Warriors and Toronto Raptors, which had 13.51 million viewers a year earlier. Some attribute this decline to the political stances the league and its players are taking, while others consider load management, the uneven talent distribution between the conferences and the cord-cutting of younger viewers as the main reason for the decline.[106][107][108][109][110]
.During the 2020–21 and 2021–22 seasons, the Milwaukee Bucks would defeat the Phoenix Suns in the 2021 NBA Finals, securing their second NBA championship since 1971, and the Golden State Warriors made their sixth appearance in the finals defeating the Boston Celtics in the 2022 NBA Finals, their fourth championship in eight years.[111][112]
.The 2022–23 season saw the Denver Nuggets, led by center Nikola Jokić, make the franchise's first NBA Finals appearance and defeat the Miami Heat in five games to win their first NBA championship.[113]
.The 2023–24 NBA season saw the star-studded Boston Celtics, winning a championship over the Dallas Mavericks, after five conference finals appearances, and a finals appearance marking their 18th championship, their first since 2008.[114]
.The 2025 NBA playoffs features four teams (the Knicks, Pacers, Thunder and Timberwolves) who have championship droughts spanning several decades or who have never won a championship. 2019–2025 is the longest continuous period in the NBA's history where a different team has won the Finals each season, leading numerous outlets to dub this the "parity era" in contrast to the dynasties which dominated previous decades.[115][116]
.Following pioneers like Vlade Divac (Serbia) and Dražen Petrović (Croatia), who joined the NBA in the late 1980s, an increasing number of international players have moved directly from playing elsewhere in the world to starring in the NBA.[117][118] Since 2006, the NBA has faced EuroLeague teams in exhibition matches in the NBA Europe Live Tour,[119] and since 2009, in the EuroLeague American Tour. On November 9, 2007, when the Houston Rockets with Yao Ming faced off against the Milwaukee Bucks with Yi Jianlian, over 200 million people in China watched on 19 different networks, making it the most-viewed game in NBA history.[120]
.The 2013–14 season opened with a record 92 international players on the opening night rosters, representing 39 countries and comprising over 20 percent of the league.[121] The NBA defines "international" players as those born outside the 50 United States and Washington, D.C. This means that:
.The beginning of the 2017–18 season saw a record 108 international players representing 42 countries marking 4 consecutive years of at least 100 international players and each team having at least one international player.[122] In 2018, the Phoenix Suns hired Serbian coach Igor Kokoškov as their new head coach, replacing Canadian interim coach Jay Triano, making Kokoškov the first European coach to become a head coach for a team in the NBA.[123]
.In the 2023–24 season, the Mavericks and the Thunder each had eight international players on their roster.[124] For seven consecutive seasons from 2018–19 to 2024–25, the league's MVP award has been given to an international player.[125]
[126]
.In 2001, an affiliated minor league, the National Basketball Development League, now called the NBA G League, was created.[127]
.Two years after the Hornets' move to New Orleans, the NBA returned to North Carolina, as the Charlotte Bobcats were formed as an expansion team in 2004.[128]
.The Hornets temporarily moved to Oklahoma City in 2005 for two seasons because of damage caused by Hurricane Katrina.[129] The team returned to New Orleans in 2007.[130]
.A new official game ball was introduced on June 28, 2006, for the 2006–07 season, marking the first change to the ball in over 35 years and only the second ball in 60 seasons.[131] Manufactured by Spalding, the new ball featured a new design and new synthetic material that Spalding claimed offered a better grip, feel, and consistency than the original ball. However, many players were vocal in their disdain for the new ball, saying that it was too sticky when dry, and too slippery when wet.
.Commissioner Stern announced on December 11, 2006, that beginning January 1, 2007, the NBA would return to the traditional leather basketball in use prior to the 2006–07 season. The change was influenced by frequent player complaints and confirmed hand injuries (cuts) caused by the microfiber ball.[132] The Players' Association had filed a suit on behalf of the players against the NBA over the new ball.[133] As of the 2017–18 season[update], the NBA team jerseys are manufactured by Nike, replacing the previous supplier, Adidas. All teams will wear jerseys with the Nike logo except the Charlotte Hornets, whose jerseys will instead have the Jumpman logo associated with longtime Nike endorser Michael Jordan, who owns the Hornets.[134]
.The Federal Bureau of Investigation (FBI) began an investigation on July 19, 2007, over allegations that veteran NBA referee Tim Donaghy bet on basketball games he officiated over the past two seasons and that he made calls affecting the point spread in those games.[135] On August 15, 2007, Donaghy pleaded guilty to two federal charges related to the investigation. Donaghy claimed in 2008 that certain referees were friendly with players and "company men" for the NBA, and he alleged that referees influenced the outcome of certain playoff and finals games in 2002 and 2005. NBA commissioner David Stern denied the allegations and said Donaghy was a convicted felon and a "singing, cooperating witness".[136] Donaghy served 15 months in prison and was released in November 2009.[137] According to an independent study by Ronald Beech of Game 6 of the 2002 Western Conference Finals between the Los Angeles Lakers and Sacramento Kings, although the refs increased the Lakers' chances of winning through foul calls during the game, there was no collusion to fix the game. On alleged "star treatment" during Game 6 by the referees toward certain players, Beech claimed, "there does seem to be issues with different standards and allowances for different players."[138]
.The NBA Board of Governors approved the request of the Seattle SuperSonics to move to Oklahoma City on April 18, 2008.[139] The team, however, could not move until it had settled a lawsuit filed by the city of Seattle, which was intended to keep the SuperSonics in Seattle for the remaining two seasons of the team's lease at KeyArena. Following a court case, the city of Seattle settled with the ownership group of the SuperSonics on July 2, 2008, allowing the team to move to Oklahoma City immediately in exchange for terminating the final two seasons of the team's lease at KeyArena.[140] The Oklahoma City Thunder began playing in the 2008–09 season.
.The first outdoor game in the modern era of the league was played at the Indian Wells Tennis Garden on October 11, 2008, between the Phoenix Suns and the Denver Nuggets.[141]
.The first official NBA league games on European ground took place in 2011. In two matchups, the New Jersey Nets faced the Toronto Raptors at the O2 Arena in London in front of over 20,000 fans.
.After the 2012–13 season, the New Orleans Hornets were renamed the Pelicans.[142] During the 2013–14 season, Stern retired as commissioner after 30 years, and deputy commissioner Adam Silver ascended to the position of commissioner. During that season's playoffs, the Bobcats officially reclaimed the Hornets name, and by agreement with the league and the Pelicans, also received sole ownership of all history, records, and statistics from the Pelicans' time in Charlotte. As a result, the Hornets are now officially considered to have been founded in 1988, suspended operations in 2002, and resumed in 2004 as the Bobcats, while the Pelicans are officially treated as a 2002 expansion team.[143] (This is somewhat similar to the relationship between the Cleveland Browns and Baltimore Ravens in the NFL.)
.Donald Sterling, who was then-owner of the Los Angeles Clippers, received a lifetime ban from the NBA on April 29, 2014, after racist remarks he made became public. Sterling was also fined US$2.5 million, the maximum allowed under the NBA Constitution.[144]
.Becky Hammon was hired by the San Antonio Spurs on August 5, 2014, as an assistant coach, becoming the second female coach in NBA history but the first full-time coach.[145][146] This also makes her the first full-time female coach in any of the four major professional sports in North America.[146]
.The NBA announced on April 15, 2016, that it would allow all 30 of its teams to sell corporate sponsor advertisement patches on official game uniforms, beginning with the 2017–18 season. The sponsorship advertisement patches would appear on the left front of jerseys, opposite Nike's logo, marking the first time a manufacturer's logo would appear on NBA jerseys, and would measure approximately 2.5 by 2.5 inches. The NBA would become the first major North American professional sports league to allow corporate sponsorship logos on official team uniforms, and the last to have a uniform manufacturer logo appear on its team uniforms.[147] The first team to announce a jersey sponsorship was the Philadelphia 76ers, who agreed to a deal with StubHub.[148]
.On July 6, 2017, the NBA unveiled an updated rendition of its logo; it was largely identical to the previous design, except with revised typography and a "richer" color scheme. The league began to phase in the updated logo across its properties during the 2017 NBA Summer League.[149]
.The NBA also officially released new Nike uniforms for all 30 teams beginning with the 2017–18 season. The league eliminated "home" and "away" uniform designations. Instead, each team would have four or six uniforms: the "Association" edition, which is the team's white uniform, the "Icon" edition, which is the team's color uniform, and the "Statement" and "City" uniforms, which most teams use as an alternate uniform.[150] In 2018, the NBA also released the "Earned" uniform.[151]
.In 2018, Adam Silver showed support in the Supreme Court's decision to overturn a federal ban on sports betting. Silver thought it would bring greater transparency and integrity as well as business opportunities.[152] Before naming DraftKings and FanDuel co-official sports betting partners of the NBA in 2021, the NBA first named MGM as the exclusive official gaming partner of the NBA and WNBA—the first major American sports league to do so.[153][154] With a deal between the 76ers and then-sportsbook FOX Bet as the first agreement between an NBA team and a sportsbook app, more teams partnered with operators thereafter.[155] This early acceptance of sports betting translated to basketball being the most bet-on sport in the United States over football in 2023.[156]
.As a part of its November 2021 multi-year partnership deal with the United Arab Emirates (UAE), the NBA hosted two preseason games in Abu Dhabi on October 4 and 6, 2024, marking its third annual trip to the country. However, the Human Rights Watch (HRW) raised concerns, citing the UAE's pattern of using high-profile events to enhance its image. HRW accused the league of being complicit in "sportswashing" the UAE's poor human rights record, while the country seeks to display itself as open country, without addressing the abuses. On September 30, HRW wrote a letter to the NBA, urging it to implement a human rights risk mitigation strategy, and to ensure that the preseason games were not used as a distraction from the UAE's human rights abuses. The rights organization also pointed out that the UAE hosted the games amidst the reports of the country being directly involved in fuelling the Sudanese civil war. A coalition of human rights groups called upon the NBA to cancel the games in Abu Dhabi in solidarity with Sudanese.[157]
.On March 10, 2025, NBA and Australia's National Basketball League (NBL) announced that in October 2025, the New Orleans Pelicans would play two preseason games at Rod Laver Arena in Melbourne as part of the NBA x NBL: Melbourne Series.[158][159]
.Download coordinates as:
.The NBA originated in 1946 with 11 teams, and through a sequence of team expansions, reductions and relocations consists of 30 teams – 29 in the United States and 1 in Canada.
.The current league organization divides 30 teams into two 15-team conferences[160] of three divisions with five teams each. The current divisional alignment was introduced in the 2004–05 season.[161] Reflecting the population distribution of the United States and Canada as a whole, most teams are in the eastern half of the country: 13 teams are in the Eastern Time Zone, nine in the Central, three in the Mountain, and five in the Pacific.
.Notes:
.Following the summer break, teams begin training camps in late September.[162] Training camps allow the coaching staff to evaluate players (especially rookies), scout the team's strengths and weaknesses, prepare the players for the rigorous regular season and determine the 12-man active roster (and a 3-man inactive list) with which they will begin the regular season. Teams have the ability to assign players with less than two years of experience to the NBA G League. After training camp, a series of preseason exhibition games are held. Preseason matches are sometimes held in non-NBA cities, both in the United States and overseas. The NBA regular season begins in mid-October.[160]
.During the regular season, each team plays 82 games, 41 each home and away.[163] A team faces opponents in its own division four times a year (16 games).[163] Each team plays six of the teams from the other two divisions in its conference four times (24 games), and the remaining four teams three times (12 games).[163] Finally, each team plays all the teams in the other conference twice apiece (30 games).[163] This asymmetrical structure means the strength of schedule will vary between teams (but not as significantly as the NFL or MLB). Over five seasons, each team will have played 80 games against their division (20 games against each opponent, 10 at home, 10 on the road), 180 games against the rest of their conference (18 games against each opponent, 9 at home, 9 on the road), and 150 games against the other conference (10 games against each team, 5 at home, 5 on the road).
.Starting with the 2023–24 season, the regular season includes an in-season tournament, in which all games in the tournament (except for the final) count towards the regular season.[164]
.The NBA is also the only league that regularly schedules games on Christmas Day.[165][original research] The league has been playing games regularly on the holiday since 1947,[166] though the first Christmas Day games were not televised until 1983–84.[167] Games played on this day have featured some of the best teams and players.[165][166][167] Christmas is also notable for NBA on television, as the holiday is when the first NBA games air on network television each season.[166][167] Games played on this day have been some of the highest-rated games during a particular season.
.The NBA has also played games on Martin Luther King Jr. Day (MLK Day) every year since the holiday was first observed in 1986.[168]
.In February, the regular season pauses to celebrate the annual NBA All-Star Game.[169] Fans vote throughout the United States, Canada, and on the Internet, and the top vote-getters in each conference are named captains. Fan votes determine the rest of the All-Star starters. Coaches vote to choose the remaining 14 All-Stars. The player with the best performance during the game is rewarded with a Game MVP award. Other attractions of the All-Star break include the Rising Stars Challenge (originally Rookie Challenge), where the top rookies and second-year players in the NBA play in a 5-on-5 basketball game, with the current format pitting U.S. players against those from the rest of the world; the Skills Challenge, where players compete to finish an obstacle course consisting of shooting, passing, and dribbling in the fastest time; the Three Point Contest, where players compete to score the highest number of three-point field goals in a given time; and the NBA Slam Dunk Contest, where players compete to dunk the ball in the most entertaining way according to the judges. These other attractions have varying names which include the names of the various sponsors who have paid for naming rights.
.Shortly after the All-Star break is the trade deadline, which is set to fall on the 16th Thursday of the season (usually in February) at 3 pm Eastern Time.[170] After this date, teams are not allowed to exchange players with each other for the remainder of the season, although they may still sign and release players. Major trades are often completed right before the trading deadline, making that day a hectic time for general managers.
.Around the middle of April, the regular season ends.[160] It is during this time that voting begins for individual awards, as well as the selection of the honorary, league-wide, postseason teams. The Sixth Man of the Year Award is given to the best player coming off the bench (must have more games coming off the bench than actual games started). The Rookie of the Year Award is awarded to the most outstanding first-year player. The Most Improved Player Award is awarded to the player who is deemed to have shown the most improvement from the previous season. The Defensive Player of the Year Award is awarded to the league's best defender. The Coach of the Year Award is awarded to the coach that has made the most positive difference to a team. The Most Valuable Player Award is given to the player deemed the most valuable for (his team) that season. Additionally, Sporting News awards an unofficial (but widely recognized) Executive of the Year Award to the general manager who is adjudged to have performed the best job for the benefit of his franchise.
.The postseason teams are the All-NBA Team, the All-Defensive Team, and the All-Rookie Team; each consists of five players. There are three All-NBA teams, consisting of the top players at each position, with first-team status being the most desirable. There are two All-Defensive teams, consisting of the top defenders at each position. There are also two All-Rookie teams, consisting of the top first-year players regardless of position.[171][172]
.The NBA playoffs begin in April after the conclusion of the regular season and play-in tournament with the top eight teams in each conference,[160] regardless of divisional alignment, competing for the league's championship title, the Larry O'Brien Championship Trophy. Seeds are awarded in strict order of regular season record (with a tiebreaker system used as needed).
.Having a higher seed offers several advantages. Since the first seed begins the playoffs playing against the eighth seed, the second seed plays the seventh seed, the third seed plays the sixth seed, and the fourth seed plays the fifth seed, having a higher seed typically means a team faces a weaker opponent in the first round. The team in each series with the better record has home-court advantage, including the First Round.
.The league began using its current format, with the top eight teams in each conference advancing regardless of divisional alignment, in the 2015–16 season. Previously, the top three seeds went to the division winners.[173]
.The playoffs follow a tournament format. Each team plays an opponent in a best-of-seven series, with the first team to win four games advancing into the next round, while the other team is eliminated from the playoffs. In the next round, the successful team plays against another advancing team of the same conference. All but one team in each conference are eliminated from the playoffs. Since the NBA does not re-seed teams, the playoff bracket in each conference uses a traditional design, with the winner of the series matching the first- and eighth-seeded teams playing the winner of the series matching the fourth- and fifth-seeded teams, and the winner of the series matching the second- and seventh-seeded teams playing the winner of the series matching the third- and sixth-seeded teams. In every round, the best-of-7 series follows a 2–2–1–1–1 home-court pattern, meaning that one team will have home court in games 1, 2, 5, and 7, while the other plays at home in games 3, 4, and 6. From 1985 to 2013, the NBA Finals followed a 2–3–2 pattern, meaning that one team had home court in games 1, 2, 6, and 7, while the other played at home in games 3, 4, and 5.[174]
.The final playoff round, a best-of-seven series between the victors of both conferences, is known as the NBA Finals and is held annually in June (sometimes, the series will start in late May). The winner of the NBA Finals receives the Larry O'Brien Championship Trophy. Each player and major contributor—including coaches and the general manager—on the winning team receive a championship ring. In addition, the league awards the Bill Russell NBA Finals Most Valuable Player Award to the best performing player of the series.
.The Boston Celtics have the most championships, with 18 NBA Finals wins.[175] The Los Angeles Lakers have the second-most with 17; the Golden State Warriors and Chicago Bulls have the third- and fourth-most, respectively, with seven and six titles.
.Current teams that have no NBA Finals appearances:[176]
.As one of the major sports leagues in North America, the NBA has a long history of partnerships with television networks in the United States. The NBA signed a contract with DuMont Television Network in its eighth season, the 1953–54 season, marking the first year the NBA had a national television broadcaster.[177] Similar to the National Football League, the lack of television stations led to NBC taking over the rights from the 1954–55 season until April 1962–NBC's first tenure with the NBA.[178] As of 2024[update] in the United States, the NBA has a contract with ESPN (and ABC) and TNT through the 2024–25 season.[179] Games that are not broadcast nationally are usually aired over regional sports networks specific to the area where the teams are located.
.The National Basketball Association has sporadically participated in international club competitions. The first international competition involving the NBA was a 1978 exhibition game in Tel Aviv, Israel between the Washington Bullets and Israeli club Maccabi Tel Aviv.[180] From 1987 to 1999 an NBA team played against championship club teams from Asia, Europe and South America in the McDonald's Championship. This tournament was won by the NBA invitee every year it was held.[181]
.In 2022, an average ticket cost $77.75.[182] Depending on the market and stage of the season—preseason, regular season, postseason—a ticket can range from $10 to $100,000.[a][183][184]
.In 2020, ticket prices for the NBA All Star Game became more expensive than ever before, averaging around $2,600, and even more on the secondary market.[185]
.According to Nielsen's survey, in 2013 the NBA had the youngest audience, with 45 percent of its viewers under 35. As of 2022[update], the league remains the least likely to be watched by women, who make up only 30% of the viewership.[186] As of 2014[update], 45 percent of its viewers were black, while 40 percent of viewers were white, making it the only top North American sport that does not have a white majority audience.[187]
.As of 2017[update], the NBA's popularity further declined among white Americans, who during the 2016–17 season, made up only 34% of the viewership. At the same time, the black viewership increased to 47 percent, while Hispanic (of any race) stood at 11% and Asian viewership stood at 8%. According to the same poll, the NBA was favored more strongly by Democrats than Republicans.[188]
.Outside the U.S., the NBA's biggest international market is in China,[189][190] where an estimated 800 million viewers watched the 2017–18 season.[191] NBA China is worth approximately $4 billion.[189][190]
.Following pioneers like Vlade Divac (Serbia) and Dražen Petrović (Croatia), who joined the NBA in the late 1980s, an increasing number of international players have moved directly from playing elsewhere in the world to starring in the NBA. Below is a list of foreign players who have won NBA awards or have otherwise been recognized for their contributions to basketball, either currently or formerly active in the league:
.On some occasions, young players, most but not all from the English-speaking world, have attended U.S. colleges before playing in the NBA. Notable examples are:
.The league has a global social responsibility program, NBA Cares, that is responsible for the league's stated mission of addressing important social issues worldwide.[204]
.1961
.1966
.1967
.1968
.1970
.1974
.1980
.1988
.1989
.1995
.2004
.
.Basketball is a team sport in which two teams, most commonly of five players each, opposing one another on a rectangular court, compete with the primary objective of shooting a basketball (approximately 9.4 inches (24 cm) in diameter) through the defender's hoop (a basket 18 inches (46 cm) in diameter mounted 10 feet (3.05 m) high to a backboard at each end of the court), while preventing the opposing team from shooting through their own hoop. A field goal is worth two points, unless made from behind the three-point line, when it is worth three. After a foul, timed play stops and the player fouled or designated to shoot a technical foul is given one, two or three one-point free throws. The team with the most points at the end of the game wins, but if regulation play expires with the score tied, an additional period of play (overtime) is mandated.
.Players advance the ball by bouncing it while walking or running (dribbling) or by passing it to a teammate, both of which require considerable skill. On offense, players may use a variety of shots – the layup, the jump shot, or a dunk; on defense, they may steal the ball from a dribbler, intercept passes, or block shots; either offense or defense may collect a rebound, that is, a missed shot that bounces from rim or backboard. It is a violation to lift or drag one's pivot foot without dribbling the ball, to carry it, or to hold the ball with both hands then resume dribbling.
.The five players on each side fall into five playing positions. The tallest player is usually the center, the second-tallest and strongest is the power forward, a slightly shorter but more agile player is the small forward, and the shortest players or the best ball handlers are the shooting guard and the point guard, who implement the coach's game plan by managing the execution of offensive and defensive plays (player positioning). Informally, players may play three-on-three, two-on-two, and one-on-one.
.Invented in 1891 by Canadian-American gym teacher James Naismith in Springfield, Massachusetts, in the United States, basketball has evolved to become one of the world's most popular and widely viewed sports.[1][2] The National Basketball Association (NBA) is the most significant professional basketball league in the world in terms of popularity, salaries, talent, and level of competition[3][4] (drawing most of its talent from U.S. college basketball). Outside North America, the top clubs from national leagues qualify to continental championships such as the EuroLeague and the Basketball Champions League Americas. The FIBA Basketball World Cup and Men's Olympic Basketball Tournament are the major international events of the sport and attract top national teams from around the world. Each continent hosts regional competitions for national teams, like EuroBasket and FIBA AmeriCup.
.The FIBA Women's Basketball World Cup and women's Olympic basketball tournament feature top national teams from continental championships. The main North American league is the WNBA (NCAA Women's Division I Basketball Championship is also popular), whereas the strongest European clubs participate in the EuroLeague Women.
.A game similar to basketball is mentioned in a 1591 book published in Frankfurt am Main that reports on the lifestyles and customs of coastal North American residents, Wahrhafftige Abconterfaytung der Wilden (German; translates as Truthful Depictions of the Savages:[5] "Among other things, a game of skill is described in which balls must be thrown against a target woven from twigs, mounted high on a pole. There's a small reward for the player if the target is being hit."[6]
.In December 1891, James Naismith, a Canadian-American professor of physical education and instructor at the International Young Men's Christian Association Training School (now Springfield College) in Springfield, Massachusetts,[7] was trying to keep his gym class active on a rainy day.[8] He sought a vigorous indoor game to keep his students occupied and at proper levels of fitness during the long New England winters. After rejecting other ideas as either too rough or poorly suited to walled-in gymnasiums, he invented a new game in which players would pass a ball to teammates and try to score points by tossing the ball into a basket mounted on a wall. 
.Naismith wrote the basic rules and nailed a peach basket onto an elevated track. Naismith initially set up the peach basket with its bottom intact, which meant that the ball had to be retrieved manually after each "basket" or point scored. This quickly proved tedious, so Naismith removed the bottom of the basket to allow the balls to be poked out with a long dowel after each scored basket.
.Shortly after, Senda Berenson, instructor of physical culture at the nearby Smith College, went to Naismith to learn more about the game.[9] Fascinated by the new sport and the values it could teach, she started to organize games with her pupils, following adjusted rules.[10] The first official women's interinstitutional game was played barely 11 months later, between the University of California and Miss Head's School.[11] In 1899, a committee was established at the Conference of Physical Training in Springfield to draw up general rules for women's basketball. Thus, the sport quickly spread throughout America's schools, colleges and universities with uniform rules for both sexes.[12]
.Basketball was originally played with a soccer ball. These round balls from "association football" were made, at the time, with a set of laces to close off the hole needed for inserting the inflatable bladder after the other sewn-together segments of the ball's cover had been flipped outside-in.[13][14] These laces could cause bounce passes and dribbling to be unpredictable.[15] Eventually a lace-free ball construction method was invented, and this change to the game was endorsed by Naismith (whereas in American football, the lace construction proved to be advantageous for gripping and remains to this day). The first balls made specifically for basketball were brown, and it was only in the late 1950s that Tony Hinkle, searching for a ball that would be more visible to players and spectators alike, introduced the orange ball that is now in common use. Dribbling was not part of the original game except for the "bounce pass" to teammates. Passing the ball was the primary means of ball movement. Dribbling was eventually introduced but limited by the asymmetric shape of early balls.[dubious – discuss] Dribbling was common by 1896, with a rule against the double dribble by 1898.[16]
.The peach baskets were used until 1906 when they were finally replaced by metal hoops with backboards. A further change was soon made, so the ball merely passed through. Whenever a person got the ball in the basket, their team would gain a point. Whichever team got the most points won the game.[17] The baskets were originally nailed to the mezzanine balcony of the playing court, but this proved impractical when spectators in the balcony began to interfere with shots. The backboard was introduced to prevent this interference; it had the additional effect of allowing rebound shots.[18] Naismith's handwritten diaries, discovered by his granddaughter in early 2006, indicate that he was nervous about the new game he had invented, which incorporated rules from a children's game called duck on a rock, as many had failed before it.[19]
.Frank Mahan, one of the players from the original first game, approached Naismith after the Christmas break, in early 1892, asking him what he intended to call his new game. Naismith replied that he had not thought of it because he had been focused on just getting the game started. Mahan suggested that it be called "Naismith ball", at which he laughed, saying that a name like that would kill any game. Mahan then said, "Why not call it basketball?" Naismith replied, "We have a basket and a ball, and it seems to me that would be a good name for it."[20][21] The first official game was played in the YMCA gymnasium in Albany, New York, on January 20, 1892, with nine players. The game ended at 1–0; the shot was made from 25 feet (7.6 m), on a court just half the size of a present-day Streetball or National Basketball Association (NBA) court.
.At the time, soccer was being played with 10 to a team (which was increased to 11). When winter weather got too icy to play soccer, teams were taken indoors, and it was convenient to have them split in half and play basketball with five on each side. By 1897–98, teams of five became standard.
.Basketball's early adherents were dispatched to YMCAs throughout the United States, and it quickly spread through the United States and Canada. By 1895, it was well established at several women's high schools. While YMCA was responsible for initially developing and spreading the game, within a decade it discouraged the new sport, as rough play and rowdy crowds began to detract from YMCA's primary mission. However, other amateur sports clubs, colleges, and professional clubs quickly filled the void. In the years before World War I, the Amateur Athletic Union and the Intercollegiate Athletic Association of the United States (forerunner of the NCAA) vied for control over the rules for the game. The first pro league, the National Basketball League, was formed in 1898 to protect players from exploitation and to promote a less rough game. This league only lasted five years.
.James Naismith was instrumental in establishing college basketball. His colleague C. O. Beamis fielded the first college basketball team just a year after the Springfield YMCA game at the suburban Pittsburgh Geneva College.[22] Naismith himself later coached at the University of Kansas for six years, before handing the reins to renowned coach Forrest "Phog" Allen. Naismith's disciple Amos Alonzo Stagg brought basketball to the University of Chicago, while Adolph Rupp, a student of Naismith's at Kansas, enjoyed great success as coach at the University of Kentucky. On February 9, 1895, the first intercollegiate 5-on-5 game was played at Hamline University between Hamline and the School of Agriculture, which was affiliated with the University of Minnesota.[23][24][25] The School of Agriculture won in a 9–3 game.
.In 1901, colleges, including the University of Chicago, Columbia University, Cornell University, Dartmouth College, the University of Minnesota, the U.S. Naval Academy, the University of Colorado and Yale University began sponsoring men's games. In 1905, frequent injuries on the football field prompted President Theodore Roosevelt to suggest that colleges form a governing body, resulting in the creation of the Intercollegiate Athletic Association of the United States (IAAUS). In 1910, that body changed its name to the National Collegiate Athletic Association (NCAA). The first Canadian interuniversity basketball game was played at YMCA in Kingston, Ontario on February 6, 1904, when McGill University – Naismith's alma mater – visited Queen's University. McGill won 9–7 in overtime; the score was 7–7 at the end of regulation play, and a ten-minute overtime period settled the outcome. A good turnout of spectators watched the game.[26]
.The first men's national championship tournament, the National Association of Intercollegiate Basketball tournament, which still exists as the National Association of Intercollegiate Athletics (NAIA) tournament, was organized in 1937. The first national championship for NCAA teams, the National Invitation Tournament (NIT) in New York, was organized in 1938; the NCAA national tournament began one year later. College basketball was rocked by gambling scandals from 1948 to 1951, when dozens of players from top teams were implicated in game-fixing and point shaving. Partially spurred by an association with cheating, the NIT lost support to the NCAA tournament.
.Before widespread school district consolidation, most American high schools were far smaller than their present-day counterparts. During the first decades of the 20th century, basketball quickly became the ideal interscholastic sport due to its modest equipment and personnel requirements. In the days before widespread television coverage of professional and college sports, the popularity of high school basketball was unrivaled in many parts of America. Perhaps the most legendary of high school teams was Indiana's Franklin Wonder Five, which took the nation by storm during the 1920s, dominating Indiana basketball and earning national recognition.
.Today virtually every high school in the United States fields a basketball team in varsity competition.[27] Basketball's popularity remains high, both in rural areas where they carry the identification of the entire community, as well as at some larger schools known for their basketball teams where many players go on to participate at higher levels of competition after graduation. In the 2016–17 season, 980,673 boys and girls represented their schools in interscholastic basketball competition, according to the National Federation of State High School Associations.[28] The states of Illinois, Indiana and Kentucky are particularly well known for their residents' devotion to high school basketball, commonly called Hoosier Hysteria in Indiana; the critically acclaimed film Hoosiers shows high school basketball's depth of meaning to these communities.
.⁣There is currently no tournament to determine a national high school champion. The most serious effort was the National Interscholastic Basketball Tournament at the University of Chicago from 1917 to 1930. The event was organized by Amos Alonzo Stagg and sent invitations to state champion teams. The tournament started out as a mostly Midwest affair but grew. In 1929 it had 29 state champions. Faced with opposition from the National Federation of State High School Associations and North Central Association of Colleges and Schools that bore a threat of the schools losing their accreditation the last tournament was in 1930. The organizations said they were concerned that the tournament was being used to recruit professional players from the prep ranks.[29] The tournament did not invite minority schools or private/parochial schools.
.The National Catholic Interscholastic Basketball Tournament ran from 1924 to 1941 at Loyola University.[30] The National Catholic Invitational Basketball Tournament from 1954 to 1978 played at a series of venues, including Catholic University, Georgetown and George Mason.[31] The National Interscholastic Basketball Tournament for Black High Schools was held from 1929 to 1942 at Hampton Institute.[32] The National Invitational Interscholastic Basketball Tournament was held from 1941 to 1967 starting out at Tuskegee Institute. Following a pause during World War II it resumed at Tennessee State College in Nashville. The basis for the champion dwindled after 1954 when Brown v. Board of Education began an integration of schools. The last tournaments were held at Alabama State College from 1964 to 1967.[33]
.Teams abounded throughout the 1920s. There were hundreds of men's professional basketball teams in towns and cities all over the United States, and little organization of the professional game. Players jumped from team to team and teams played in armories and smoky dance halls. Leagues came and went. Barnstorming squads such as the Original Celtics and two all-African American teams, the New York Renaissance Five ("Rens") and the (still existing) Harlem Globetrotters played up to two hundred games a year on their national tours.
.In 1946, the Basketball Association of America (BAA) was formed. The first game was played in Toronto, Ontario, Canada between the Toronto Huskies and New York Knickerbockers on November 1, 1946. Three seasons later, in 1949, the BAA merged with the National Basketball League (NBL) to form the National Basketball Association (NBA). By the 1950s, basketball had become a major college sport, thus paving the way for a growth of interest in professional basketball. In 1959, a basketball hall of fame was founded in Springfield, Massachusetts, site of the first game. Its rosters include the names of great players, coaches, referees and people who have contributed significantly to the development of the game. The hall of fame has people who have accomplished many goals in their career in basketball. An upstart organization, the American Basketball Association, emerged in 1967 and briefly threatened the NBA's dominance until the ABA-NBA merger in 1976. Today the NBA is the top professional basketball league in the world in terms of popularity, salaries, talent, and level of competition.
.The NBA has featured many famous players, including George Mikan, the first dominating "big man"; ball-handling wizard Bob Cousy and defensive genius Bill Russell of the Boston Celtics; charismatic center Wilt Chamberlain, who originally played for the barnstorming Harlem Globetrotters; all-around stars Oscar Robertson and Jerry West; more recent big men Kareem Abdul-Jabbar, Shaquille O'Neal, Hakeem Olajuwon and Karl Malone; playmakers John Stockton, Isiah Thomas and Steve Nash; crowd-pleasing forwards Julius Erving and Charles Barkley; European stars Dirk Nowitzki, Pau Gasol, Nikola Jokić and Tony Parker; Latin American stars Manu Ginobili, more recent superstars, Allen Iverson, Kobe Bryant, Tim Duncan, LeBron James, Stephen Curry, Giannis Antetokounmpo, etc.; and the three players who many credit with ushering the professional game to its highest level of popularity during the 1980s and 1990s: Larry Bird, Earvin "Magic" Johnson, and Michael Jordan.
.In 2001, the NBA formed a developmental league, the National Basketball Development League (later known as the NBA D-League and then the NBA G League after a branding deal with Gatorade). As of the 2023–24 season, the G League has 31 teams.
.FIBA (International Basketball Federation) was formed in 1932 by eight founding nations: Argentina, Czechoslovakia, Greece, Italy, Latvia, Portugal, Romania and Switzerland. At this time, the organization only oversaw amateur players. Its acronym, derived from the French Fédération Internationale de Basket-ball Amateur, was thus "FIBA". Men's basketball was first included at the Berlin 1936 Summer Olympics, although a demonstration tournament was held in 1904. The United States defeated Canada in the first final, played outdoors. This competition has usually been dominated by the United States, whose team has won all but three titles. The first of these came in a controversial final game in Munich in 1972 against the Soviet Union, in which the ending of the game was replayed three times until the Soviet Union finally came out on top.[34] In 1950 the first FIBA World Championship for men, now known as the FIBA Basketball World Cup, was held in Argentina. Three years later, the first FIBA World Championship for women, now known as the FIBA Women's Basketball World Cup, was held in Chile. Women's basketball was added to the Olympics in 1976, which were held in Montreal, Quebec, Canada with teams such as the Soviet Union, Brazil and Australia rivaling the American squads.
.In 1989, FIBA allowed professional NBA players to participate in the Olympics for the first time. Prior to the 1992 Summer Olympics, only European and South American teams were allowed to field professionals in the Olympics. The United States' dominance continued with the introduction of the original Dream Team. In the 2004 Athens Olympics, the United States suffered its first Olympic loss while using professional players, falling to Puerto Rico (in a 19-point loss) and Lithuania in group games, and being eliminated in the semifinals by Argentina. It eventually won the bronze medal defeating Lithuania, finishing behind Argentina and Italy. The Redeem Team, won gold at the 2008 Olympics, and the B-Team, won gold at the 2010 FIBA World Championship in Turkey despite featuring no players from the 2008 squad. The United States continued its dominance as they won gold at the 2012 Olympics, 2014 FIBA World Cup and the 2016 Olympics.
.Worldwide, basketball tournaments are held for boys and girls of all age levels. The global popularity of the sport is reflected in the nationalities represented in the NBA. Players from all six inhabited continents currently play in the NBA. Top international players began coming into the NBA in the mid-1990s, including Croatians Dražen Petrović and Toni Kukoč, Serbian Vlade Divac, Lithuanians Arvydas Sabonis and Šarūnas Marčiulionis, Dutchman Rik Smits and German Detlef Schrempf.
.In the Philippines, the Philippine Basketball Association's first game was played on April 9, 1975, at the Araneta Coliseum in Cubao, Quezon City, Philippines. It was founded as a "rebellion" of several teams from the now-defunct Manila Industrial and Commercial Athletic Association, which was tightly controlled by the Basketball Association of the Philippines (now defunct), the then-FIBA recognized national association. Nine teams from the MICAA participated in the league's first season that opened on April 9, 1975. The NBL is Australia's pre-eminent men's professional basketball league. The league commenced in 1979, playing a winter season (April–September) and did so until the completion of the 20th season in 1998. The 1998–99 season, which commenced only months later, was the first season after the shift to the current summer season format (October–April). This shift was an attempt to avoid competing directly against Australia's various football codes. It features 8 teams from around Australia and one in New Zealand. A few players including Luc Longley, Andrew Gaze, Shane Heal, Chris Anstey and Andrew Bogut made it big internationally, becoming poster figures for the sport in Australia. The Women's National Basketball League began in 1981.
.Women began to play basketball in the fall of 1892 at Smith College through Senda Berenson, substitute director of the newly opened gymnasium and physical education teacher, after having modified the rules for women. Shortly after Berenson was hired at Smith, she visited Naismith to learn more about the game.[9] Fascinated by the new sport and the values it could teach, she instantly introduced the game as a class exercise and soon after teams were organized. The first women's collegiate basketball game was played on March 21, 1893, when her Smith freshmen and sophomores played against one another.[10][35] The first official women's interinstitutional game was played later that year between the University of California and the Miss Head's School.[11] In 1899, a committee was established at the Conference of Physical Training in Springfield to draw up general rules for women's basketball.[36] These rules, designed by Berenson, were published in 1899.[12] In 1902 Berenson became the editor of A. G. Spalding's first Women's Basketball Guide.[10] The same year women of Mount Holyoke and Sophie Newcomb College (coached by Clara Gregory Baer), began playing basketball. By 1895, the game had spread to colleges across the country, including Wellesley, Vassar, and Bryn Mawr. The first intercollegiate women's game was on April 4, 1896. Stanford women played Berkeley, 9-on-9, ending in a 2–1 Stanford victory.
.Women's basketball development was more structured than that for men in the early years. In 1905, the executive committee on Basket Ball Rules (National Women's Basketball Committee) was created by the American Physical Education Association.[37] These rules called for six to nine players per team and 11 officials. The International Women's Sports Federation (1924) included a women's basketball competition. 37 women's high school varsity basketball or state tournaments were held by 1925. And in 1926, the Amateur Athletic Union backed the first national women's basketball championship, complete with men's rules.[37] The Edmonton Grads, a touring Canadian women's team based in Edmonton, Alberta, operated between 1915 and 1940. The Grads toured all over North America, and were exceptionally successful. They posted a record of 522 wins and only 20 losses over that span, as they met any team that wanted to challenge them, funding their tours from gate receipts.[38] The Grads also shone on several exhibition trips to Europe, and won four consecutive exhibition Olympics tournaments, in 1924, 1928, 1932, and 1936; however, women's basketball was not an official Olympic sport until 1976. The Grads' players were unpaid, and had to remain single. The Grads' style focused on team play, without overly emphasizing skills of individual players. The first women's AAU All-America team was chosen in 1929.[37] Women's industrial leagues sprang up throughout the United States, producing famous athletes, including Babe Didrikson of the Golden Cyclones, and the All American Red Heads Team, which competed against men's teams, using men's rules. By 1938, the women's national championship changed from a three-court game to two-court game with six players per team.[37]
.The NBA-backed Women's National Basketball Association (WNBA) began in 1997. Though it had shaky attendance figures, several marquee players (Lisa Leslie, Diana Taurasi, and Candace Parker among others) have helped the league's popularity and level of competition. Other professional women's basketball leagues in the United States, such as the American Basketball League (1996–98), have folded in part because of the popularity of the WNBA. The WNBA has been looked at by many as a niche league. However, the league has recently taken steps forward. In June 2007, the WNBA signed a contract extension with ESPN. The new television deal ran from 2009 to 2016. Along with this deal, came the first-ever rights fees to be paid to a women's professional sports league. Over the eight years of the contract, "millions and millions of dollars" were "dispersed to the league's teams." In a March 12, 2009, article, NBA commissioner David Stern said that in the bad economy, "the NBA is far less profitable than the WNBA. We're losing a lot of money among a large number of teams. We're budgeting the WNBA to break even this year."[39]
.Measurements and time limits discussed in this section often vary among tournaments and organizations; international and NBA rules are used in this section.
.The object of the game is to outscore one's opponents by throwing the ball through the opponents' basket from above while preventing the opponents from doing so on their own. An attempt to score in this way is called a shot. A successful shot is worth two points, or three points if it is taken from beyond the three-point arc 6.75 meters (22 ft 2 in) from the basket in international games[40] and 23 feet 9 inches (7.24 m) in NBA games.[41] A one-point shot can be earned when shooting from the foul line after a foul is made. After a team has scored from a field goal or free throw, play is resumed with a throw-in awarded to the non-scoring team taken from a point beyond the endline of the court where the points were scored.[42]
.Games are played in four quarters of 10 (FIBA)[43] or 12 minutes (NBA).[44] College men's games use two 20-minute halves,[45] college women's games use 10-minute quarters,[46] and most United States high school varsity games use 8-minute quarters; however, this varies from state to state.[47][48] 15 minutes are allowed for a half-time break under FIBA, NBA, and NCAA rules[45][49][50] and 10 minutes in United States high schools.[47] Overtime periods are five minutes in length[45][51][52] except for high school, which is four minutes in length.[47] Teams exchange baskets for the second half. The time allowed is actual playing time; the clock is stopped while the play is not active. Therefore, games generally take much longer to complete than the allotted game time, typically about two hours.
.Five players from each team may be on the court at one time.[53][54][55][56] Substitutions are unlimited but can only be done when play is stopped. Teams also have a coach, who oversees the development and strategies of the team, and other team personnel such as assistant coaches, managers, statisticians, doctors and trainers.
.For both men's and women's teams, a standard uniform consists of a pair of shorts and a jersey with a clearly visible number, unique within the team, printed on both the front and back. Players wear high-top sneakers that provide extra ankle support. Typically, team names, players' names and, outside of North America, sponsors are printed on the uniforms.
.A limited number of time-outs, clock stoppages requested by a coach (or sometimes mandated in the NBA) for a short meeting with the players, are allowed. They generally last no longer than one minute (100 seconds in the NBA) unless, for televised games, a commercial break is needed.
.The game is controlled by the officials consisting of the referee (referred to as crew chief in the NBA), one or two umpires (referred to as referees in the NBA) and the table officials. For college, the NBA, and many high schools, there are a total of three referees on the court. The table officials are responsible for keeping track of each team's scoring, timekeeping, individual and team fouls, player substitutions, team possession arrow, and the shot clock.
.The only essential equipment in a basketball game is the ball and the court: a flat, rectangular surface with baskets at opposite ends. Competitive levels require the use of more equipment such as clocks, score sheets, scoreboards, alternating possession arrows, and whistle-operated stop-clock systems.
.A regulation basketball court in international games is 28 meters (92 feet) long and 15 meters (49 feet) wide. In the NBA and NCAA the court is 94 by 50 feet (29 by 15 meters).[41] Most courts have wood flooring, usually constructed from maple planks running in the same direction as the longer court dimension.[57][58] The name and logo of the home team is usually painted on or around the center circle.
.The basket is a steel rim 18 inches (46 cm) diameter with an attached net affixed to a backboard that measures 6 by 3.5 feet (1.8 by 1.1 meters) and one basket is at each end of the court. The white outlined box on the backboard is 18 inches (46 cm) high and 2 feet (61 cm) wide. At almost all levels of competition, the top of the rim is exactly 10 feet (3.05 meters) above the court and 4 feet (1.22 meters) inside the baseline. While variation is possible in the dimensions of the court and backboard, it is considered important for the basket to be of the correct height – a rim that is off by just a few inches can have an adverse effect on shooting. The net must "check the ball momentarily as it passes through the basket" to aid the visual confirmation that the ball went through.[59] The act of checking the ball has the further advantage of slowing down the ball so the rebound does not go as far.[60]
.The size of the basketball is also regulated. For men, the official ball is 29.5 inches (75 cm) in circumference (size 7, or a "295 ball") and weighs 22 oz (620 g). If women are playing, the official basketball size is 28.5 inches (72 cm) in circumference (size 6, or a "285 ball") with a weight of 20 oz (570 g). In 3x3, a formalized version of the halfcourt 3-on-3 game, a dedicated ball with the circumference of a size 6 ball but the weight of a size 7 ball is used in all competitions (men's, women's, and mixed teams).[61]
.The ball may be advanced toward the basket by being shot, passed between players, thrown, tapped, rolled or dribbled (bouncing the ball while running).
.The ball must stay within the court; the last team to touch the ball before it travels out of bounds forfeits possession. The ball is out of bounds if it touches a boundary line, or touches any player or object that is out of bounds.
.There are limits placed on the steps a player may take without dribbling, which commonly results in an infraction known as traveling. Nor may a player stop their dribble and then resume dribbling. A dribble that touches both hands is considered stopping the dribble, giving this infraction the name double dribble. Within a dribble, the player cannot carry the ball by placing their hand on the bottom of the ball; doing so is known as carrying the ball. A team, once having established ball control in the front half of their court, may not return the ball to the backcourt and be the first to touch it. A violation of these rules results in loss of possession.
.The ball may not be kicked, nor be struck with the fist. For the offense, a violation of these rules results in loss of possession; for the defense, most leagues reset the shot clock and the offensive team is given possession of the ball out of bounds.
.There are limits imposed on the time taken before progressing the ball past halfway (8 seconds in FIBA and the NBA; 10 seconds in NCAA and high school for both sexes), before attempting a shot (24 seconds in FIBA, the NBA, and U Sports (Canadian universities) play for both sexes, and 30 seconds in NCAA play for both sexes), holding the ball while closely guarded (5 seconds), and remaining in the restricted area known as the free-throw lane, (or the "key") (3 seconds). These rules are designed to promote more offense.
.There are also limits on how players may block an opponent's field goal attempt or help a teammate's field goal attempt. Goaltending is a defender's touching of a ball that is on a downward flight toward the basket, while the related violation of basket interference is the touching of a ball that is on the rim or above the basket, or by a player reaching through the basket from below. Goaltending and basket interference committed by a defender result in awarding the basket to the offense, while basket interference committed by an offensive player results in cancelling the basket if one is scored. The defense gains possession in all cases of goaltending or basket interference.
.An attempt to unfairly disadvantage an opponent through certain types of physical contact is illegal and is called a personal foul. These are most commonly committed by defensive players; however, they can be committed by offensive players as well. Players who are fouled either receive the ball to pass inbounds again, or receive one or more free throws if they are fouled in the act of shooting, depending on whether the shot was successful. One point is awarded for making a free throw, which is attempted from a line 15 feet (4.6 m) from the basket.
.The referee is responsible for judging whether contact is illegal, sometimes resulting in controversy. The calling of fouls can vary between games, leagues and referees.
.There is a second category of fouls called technical fouls, which may be charged for various rules violations including failure to properly record a player in the scorebook, or for unsportsmanlike conduct. These infractions result in one or two free throws, which may be taken by any of the five players on the court at the time. Repeated incidents can result in disqualification. A blatant foul involving physical contact that is either excessive or unnecessary is called an intentional foul (flagrant foul in the NBA). In FIBA and NCAA women's basketball, a foul resulting in ejection is called a disqualifying foul, while in leagues other than the NBA, such a foul is referred to as flagrant.
.If a team exceeds a certain limit of team fouls in a given period (quarter or half) – four for NBA, NCAA women's, and international games – the opposing team is awarded one or two free throws on all subsequent non-shooting fouls for that period, the number depending on the league. In the US college men's game and high school games for both sexes, if a team reaches 7 fouls in a half, the opposing team is awarded one free throw, along with a second shot if the first is made. This is called shooting "one-and-one". If a team exceeds 10 fouls in the half, the opposing team is awarded two free throws on all subsequent fouls for the half.
.When a team shoots foul shots, the opponents may not interfere with the shooter, nor may they try to regain possession until the last or potentially last free throw is in the air.
.After a team has committed a specified number of fouls, the other team is said to be "in the bonus". On scoreboards, this is usually signified with an indicator light reading "Bonus" or "Penalty" with an illuminated directional arrow or dot indicating that team is to receive free throws when fouled by the opposing team. (Some scoreboards also indicate the number of fouls committed.)
.If a team misses the first shot of a two-shot situation, the opposing team must wait for the completion of the second shot before attempting to reclaim possession of the ball and continuing play.
.If a player is fouled while attempting a shot and the shot is unsuccessful, the player is awarded a number of free throws equal to the value of the attempted shot. A player fouled while attempting a regular two-point shot thus receives two shots, and a player fouled while attempting a three-point shot receives three shots.
.If a player is fouled while attempting a shot and the shot is successful, typically the player will be awarded one additional free throw for one point. In combination with a regular shot, this is called a "three-point play" or "four-point play" (or more colloquially, an "and one") because of the basket made at the time of the foul (2 or 3 points) and the additional free throw (1 point).
.Although the rules do not specify any positions whatsoever, they have evolved as part of basketball. During the early years of basketball's evolution, two guards, two forwards, and one center were used. In more recent times specific positions evolved, but the current trend, advocated by many top coaches including Mike Krzyzewski, is towards positionless basketball, where big players are free to shoot from outside and dribble if their skill allows it.[62] Popular descriptions of positions include:
.Point guard (often called the "1") : usually the fastest player on the team, organizes the team's offense by controlling the ball and making sure that it gets to the right player at the right time.
.Shooting guard (the "2") : creates a high volume of shots on offense, mainly long-ranged; and guards the opponent's best perimeter player on defense.
.Small forward (the "3") : often primarily responsible for scoring points via cuts to the basket and dribble penetration; on defense seeks rebounds and steals, but sometimes plays more actively.
.Power forward (the "4"): plays offensively often with their back to the basket; on defense, plays under the basket (in a zone defense) or against the opposing power forward (in man-to-man defense).
.Center (the "5"): uses height and size to score (on offense), to protect the basket closely (on defense), or to rebound.
.The above descriptions are flexible. For most teams today, the shooting guard and small forward have very similar responsibilities and are often called the wings, as do the power forward and center, who are often called post players. While most teams describe two players as guards, two as forwards, and one as a center, on some occasions teams choose to call them by different designations.
.There are two main defensive strategies: zone defense and man-to-man defense. In a zone defense, each player is assigned to guard a specific area of the court. Zone defenses often allow the defense to double team the ball, a manoeuver known as a trap. In a man-to-man defense, each defensive player guards a specific opponent.
.Offensive plays are more varied, normally involving planned passes and movement by players without the ball. A quick movement by an offensive player without the ball to gain an advantageous position is known as a cut. A legal attempt by an offensive player to stop an opponent from guarding a teammate, by standing in the defender's way such that the teammate cuts next to him, is a screen or pick. The two plays are combined in the pick and roll, in which a player sets a pick and then "rolls" away from the pick towards the basket. Screens and cuts are very important in offensive plays; these allow the quick passes and teamwork, which can lead to a successful basket. Teams almost always have several offensive plays planned to ensure their movement is not predictable. On court, the point guard is usually responsible for indicating which play will occur.
.Shooting is the act of attempting to score points by throwing the ball through the basket, methods varying with players and situations.
.Typically, a player faces the basket with both feet facing the basket. A player will rest the ball on the fingertips of the dominant hand (the shooting arm) slightly above the head, with the other hand supporting the side of the ball. The ball is usually shot by jumping (though not always) and extending the shooting arm. The shooting arm, fully extended with the wrist fully bent, is held stationary for a moment following the release of the ball, known as a follow-through. Players often try to put a steady backspin on the ball to absorb its impact with the rim. The ideal trajectory of the shot is somewhat controversial, but generally a proper arc is recommended. Players may shoot directly into the basket or may use the backboard to redirect the ball into the basket.
.The two most common shots that use the above described setup are the set shot and the jump shot. Both are preceded by a crouching action which preloads the muscles and increases the power of the shot. In a set shot, the shooter straightens up and throws from a standing position with neither foot leaving the floor; this is typically used for free throws. For a jump shot, the throw is taken in mid-air with the ball being released near the top of the jump. This provides much greater power and range, and it also allows the player to elevate over the defender. Failure to release the ball before the feet return to the floor is considered a traveling violation.
.Another common shot is called the layup. This shot requires the player to be in motion toward the basket, and to "lay" the ball "up" and into the basket, typically off the backboard (the backboard-free, underhand version is called a finger roll). The most crowd-pleasing and typically highest-percentage accuracy shot is the slam dunk, in which the player jumps very high and throws the ball downward, through the basket while touching it.
.Another shot that is less common than the layup, is the "circus shot". The circus shot is a low-percentage shot that is flipped, heaved, scooped, or flung toward the hoop while the shooter is off-balance, airborne, falling down or facing away from the basket. A back-shot is a shot taken when the player is facing away from the basket, and may be shot with the dominant hand, or both; but there is a very low chance that the shot will be successful.[63]
.A shot that misses both the rim and the backboard completely is referred to as an air ball. A particularly bad shot, or one that only hits the backboard, is jocularly called a brick. The hang time is the length of time a player stays in the air after jumping, either to make a slam dunk, layup or jump shot.
.The objective of rebounding is to successfully gain possession of the basketball after a missed field goal or free throw, as it rebounds from the hoop or backboard. This plays a major role in the game, as most possessions end when a team misses a shot. There are two categories of rebounds: offensive rebounds, in which the ball is recovered by the offensive side and does not change possession, and defensive rebounds, in which the defending team gains possession of the loose ball. The majority of rebounds are defensive, as the team on defense tends to be in better position to recover missed shots; for example, about 75% of rebounds in the NBA are defensive.[64]
.A pass is a method of moving the ball between players. Most passes are accompanied by a step forward to increase power and are followed through with the hands to ensure accuracy.
.A staple pass is the chest pass. The ball is passed directly from the passer's chest to the receiver's chest. A proper chest pass involves an outward snap of the thumbs to add velocity and leaves the defence little time to react.
.Another type of pass is the bounce pass. Here, the passer bounces the ball crisply about two-thirds of the way from his own chest to the receiver. The ball strikes the court and bounces up toward the receiver. The bounce pass takes longer to complete than the chest pass, but it is also harder for the opposing team to intercept (kicking the ball deliberately is a violation). Thus, players often use the bounce pass in crowded moments, or to pass around a defender.
.The overhead pass is used to pass the ball over a defender. The ball is released while over the passer's head.
.The outlet pass occurs after a team gets a defensive rebound. The next pass after the rebound is the outlet pass.
.The crucial aspect of any good pass is it being difficult to intercept. Good passers can pass the ball with great accuracy and they know exactly where each of their other teammates prefers to receive the ball. A special way of doing this is passing the ball without looking at the receiving teammate. This is called a no-look pass.
.Another advanced style of passing is the behind-the-back pass, which, as the description implies, involves throwing the ball behind the passer's back to a teammate. Although some players can perform such a pass effectively, many coaches discourage no-look or behind-the-back passes, believing them to be difficult to control and more likely to result in turnovers or violations.
.Dribbling is the act of bouncing the ball continuously with one hand and is a requirement for a player to take steps with the ball. To dribble, a player pushes the ball down towards the ground with the fingertips rather than patting it; this ensures greater control.
.When dribbling past an opponent, the dribbler should dribble with the hand farthest from the opponent, making it more difficult for the defensive player to get to the ball. It is therefore important for a player to be able to dribble competently with both hands.
.Good dribblers (or "ball handlers") tend to keep their dribbling hand low to the ground, reducing the distance of travel of the ball from the floor to the hand, making it more difficult for the defender to "steal" the ball. Good ball handlers frequently dribble behind their backs, between their legs, and switch directions suddenly, making a less predictable dribbling pattern that is more difficult to defend against. This is called a crossover, which is the most effective way to move past defenders while dribbling.
.A skilled player can dribble without watching the ball, using the dribbling motion or peripheral vision to keep track of the ball's location. By not having to focus on the ball, a player can look for teammates or scoring opportunities, as well as avoid the danger of having someone steal the ball away from him/her.
.A block is performed when, after a shot is attempted, a defender succeeds in altering the shot by touching the ball. In almost all variants of play, it is illegal to touch the ball after it is in the downward path of its arc; this is known as goaltending. It is also illegal under NBA and Men's NCAA basketball to block a shot after it has touched the backboard, or when any part of the ball is directly above the rim. Under international rules it is illegal to block a shot that is in the downward path of its arc or one that has touched the backboard until the ball has hit the rim. After the ball hits the rim, it is again legal to touch it even though it is no longer considered as a block performed.
.To block a shot, a player has to be able to reach a point higher than where the shot is released. Thus, height can be an advantage in blocking. Players who are taller and playing the power forward or center positions generally record more blocks than players who are shorter and playing the guard positions. However, with good timing and a sufficiently high vertical leap, even shorter players can be effective shot blockers.
.At the professional level, most male players are above 6 feet 3 inches (1.91 m) and most women above 5 feet 7 inches (1.70 m). Guards, for whom physical coordination and ball-handling skills are crucial, tend to be the smallest players. Almost all forwards in the top men's pro leagues are 6 feet 6 inches (1.98 m) or taller. Most centers are over 6 feet 10 inches (2.08 m) tall. According to a survey given to all NBA teams,[when?] the average height of all NBA players is just under 6 feet 7 inches (2.01 m), with the average weight being close to 222 pounds (101 kg). The tallest players ever in the NBA were Manute Bol and Gheorghe Mureșan, who were both 7 feet 7 inches (2.31 m) tall. At 7 feet 2 inches (2.18 m), Margo Dydek was the tallest player in the history of the WNBA.
.The shortest player ever to play in the NBA is Muggsy Bogues at 5 feet 3 inches (1.60 m).[65] Other average-height or relatively short players have thrived at the pro level, including Anthony "Spud" Webb, who was 5 feet 7 inches (1.70 m) tall, but had a 42-inch (1.1 m) vertical leap, giving him significant height when jumping, and Temeka Johnson, who won the WNBA Rookie of the Year Award and a championship with the Phoenix Mercury while standing only 5 feet 3 inches (1.60 m). While shorter players are often at a disadvantage in certain aspects of the game, their ability to navigate quickly through crowded areas of the court and steal the ball by reaching low are strengths.
.Players regularly inflate their height in high school or college. Many prospects exaggerate their height while in high school or college to make themselves more appealing to coaches and scouts, who prefer taller players. Charles Barkley stated; "I've been measured at 6–5, 6-4+3⁄4. But I started in college at 6–6." Sam Smith, a former writer from the Chicago Tribune, said: "We sort of know the heights, because after camp, the sheet comes out. But you use that height, and the player gets mad. And then you hear from his agent. Or you file your story with the right height, and the copy desk changes it because they have the 'official' N.B.A. media guide, which is wrong. So you sort of go along with the joke."[66]
.Since the 2019–20 NBA season heights of NBA players are recorded definitively by measuring players with their shoes off.[67]
.Variations of basketball are activities based on the game of basketball, using common basketball skills and equipment (primarily the ball and basket). Some variations only have superficial rule changes, while others are distinct games with varying degrees of influence from basketball. Other variations include children's games, contests or activities meant to help players reinforce skills.
.An earlier version of basketball, played primarily by women and girls, was six-on-six basketball. Horseball is a game played on horseback where a ball is handled and points are scored by shooting it through a high net (approximately 1.5m×1.5m). The sport is like a combination of polo, rugby, and basketball. There is even a form played on donkeys known as Donkey basketball, which has attracted criticism from animal rights groups.
.Perhaps the single most common variation of basketball is the half-court game, played in informal settings without referees or strict rules. Only one basket is used, and the ball must be "taken back" or "cleared" – passed or dribbled outside the three-point line each time possession of the ball changes from one team to the other. Half-court games require less cardiovascular stamina, since players need not run back and forth a full court. Half-court raises the number of players that can use a court or, conversely, can be played if there is an insufficient number to form full 5-on-5 teams.
.Half-court basketball is usually played 1-on-1, 2-on-2 or 3-on-3. The last of these variations is gradually gaining official recognition as 3x3, originally known as FIBA 33. It was first tested at the 2007 Asian Indoor Games in Macau and the first official tournaments were held at the 2009 Asian Youth Games and the 2010 Youth Olympics, both in Singapore. The first FIBA 3x3 Youth World Championships[68] were held in Rimini, Italy in 2011, with the first FIBA 3x3 World Championships for senior teams following a year later in Athens. The sport is highly tipped to become an Olympic sport as early as 2016.[69] In the summer of 2017, the BIG3 basketball league, a professional 3x3 half court basketball league that features former NBA players, began. The BIG3 features several rule variants including a four-point field goal.[70]
.Variations of basketball with their own page or subsection include:
.Spin-offs from basketball that are now separate sports include:
.Basketball as a social and communal sport features environments, rules and demographics different from those seen in professional and televised basketball.
.Basketball is played widely as an extracurricular, intramural or amateur sport in schools and colleges. Notable institutions of recreational basketball include:
.Fantasy basketball was popularized during the 1990s by ESPN Fantasy Sports, NBA.com, and Yahoo! Fantasy Sports. On the model of fantasy baseball and football, players create fictional teams, select professional basketball players to "play" on these teams through a mock draft or trades, then calculate points based on the players' real-world performance.
.
.The Bretton Woods system of monetary management established the rules for commercial relations among 44 countries, including the United States, Canada, Western European countries, and Australia,[1] after the 1944 Bretton Woods Agreement until the Jamaica Accords in 1976. The Bretton Woods system was the first example of a fully negotiated monetary order intended to govern monetary relations among independent states. The Bretton Woods system required countries to guarantee convertibility of their currencies into U.S. dollars to within 1% of fixed parity rates, with the dollar convertible to gold bullion for foreign governments and central banks at US$35 per troy ounce of fine gold (or 0.88867 gram fine gold per dollar). It also envisioned greater cooperation among countries in order to prevent future competitive devaluations, and thus established the International Monetary Fund (IMF) to monitor exchange rates and lend reserve currencies to countries with balance of payments deficits.[1]
.Preparing to rebuild the international economic system while World War II was still being fought, 730 delegates from all 44 Allied countries gathered at the Mount Washington Hotel in Bretton Woods, New Hampshire, United States, for the United Nations Monetary and Financial Conference, also known as the Bretton Woods Conference. The delegates deliberated from 1 to 22 July 1944, and signed the Bretton Woods agreement on its final day. Setting up a system of rules, institutions, and procedures to regulate the international monetary system, these accords established the IMF and the International Bank for Reconstruction and Development (IBRD), which today is part of the World Bank Group. The United States, which controlled two-thirds of the world's gold, insisted that the Bretton Woods system rest on both gold and the US dollar. Soviet representatives attended the conference but later declined to ratify the final agreements, charging that the institutions they had created were "branches of Wall Street".[2] These organizations became operational in 1945 after a sufficient number of countries had ratified the agreement. According to Barry Eichengreen, the Bretton Woods system operated successfully due to three factors: "low international capital mobility, tight financial regulation, and the dominant economic and financial position of the United States and the dollar."[3]
.Eurodollar growth increased capital flows, challenging regulation of capital movements.[4] On 15 August 1971, the United States ended the convertibility of the US dollar to gold, effectively bringing the Bretton Woods system to an end and rendering the dollar a fiat currency.[5] Shortly thereafter, many fixed currencies (such as the pound sterling) also became free-floating,[6] and the subsequent era has been characterized by floating exchange rates.[7] The end of Bretton Woods was formally ratified by the Jamaica Accords in 1976.
.The planners at Bretton Woods hoped to avoid a repetition of the Treaty of Versailles after World War I, which had created enough economic and political tension to lead to WWII. After World War I, Britain owed the U.S. substantial sums, which Britain could not repay because it had used the funds to support allies such as France during the War; the Allies could not pay back Britain, so Britain could not pay back the U.S. The solution at Versailles for the French, British, and Americans seemed to entail ultimately charging Germany for the debts. If the demands on Germany were unrealistic, then it was unrealistic for France to pay back Britain, and for Britain to pay back the US.[8] Thus, many "assets" on bank balance sheets internationally were actually unrecoverable loans, which culminated in the 1931 banking crisis. Intransigent insistence by creditor countries for the repayment of Allied war debts and reparations, combined with an inclination to isolationism, led to a breakdown of the international financial system and a worldwide economic depression.[9]
.The beggar thy neighbour policies that emerged as the crisis continued saw some trading countries using currency devaluations in an attempt to increase their competitiveness (i.e. raise exports and lower imports), though recent research[when?] suggests this de facto inflationary policy probably offset some of the contractionary forces in world price levels (see Eichengreen "How to Prevent a Currency war").
.In the 1920s, international flows of speculative financial capital increased, leading to extremes in balance of payments situations in various European countries and the US.[10] In the 1930s, world markets never broke through the barriers and restrictions on international trade and investment volume – barriers haphazardly constructed, nationally motivated and imposed. The various anarchic and often autarkic protectionist and neo-mercantilist national policies – often mutually inconsistent – that emerged over the first half of the decade worked inconsistently and self-defeatingly to promote national import substitution, increase national exports, divert foreign investment and trade flows, and even prevent certain categories of cross-border trade and investment outright. Global central bankers attempted to manage the situation by meeting with each other, but their understanding of the situation as well as difficulties in communicating internationally, hindered their abilities.[11] The lesson was that simply having responsible, hard-working central bankers was not enough.
.Britain in the 1930s had an exclusionary trade bloc with countries and territories of the British Empire known as the Sterling Area. If Britain imported more than it exported to such trading partners, recipients of pounds sterling within these countries tended to put them into London banks. This meant that though Britain was running a trade deficit, it had a financial account surplus, and payments balanced. Increasingly, Britain's positive balance of payments required keeping the wealth from across its Empire in British banks. One incentive for, say, South African holders of rand to park their wealth in London and to keep the money in Sterling, was a strongly valued pound sterling. In the 1920s, imports from the US threatened certain parts of the British domestic market for manufactured goods and the way out of the trade deficit was to devalue the currency. But Britain could not devalue, or the Empire surplus would leave its banking system.[12]
.Nazi Germany also worked with a bloc of controlled countries by 1940. Germany forced trading partners with a surplus to spend that surplus importing products from Germany.[13] Thus, Britain survived by keeping Sterling-using countries' surpluses in its banking system, and Germany survived by forcing trading partners to purchase its own products. The U.S. was concerned that a sudden drop-off in war spending might return its population to the unemployment levels of the 1930s, and so wanted Sterling-using countries and everyone in Europe to be able to import from the US, hence the U.S. supported free trade and international convertibility of currencies into gold or dollars.[14]
.When many of the same experts who observed the 1930s became the architects of a new, unified, post-war system at Bretton Woods, their guiding principles became "no more beggar thy neighbor" and "control flows of speculative financial capital". Preventing a repetition of this process of competitive devaluations was desired, but in a way that would not force debtor countries to contract their industrial bases by keeping interest rates at a level high enough to attract foreign bank deposits. John Maynard Keynes, wary of repeating the Great Depression, was behind Britain's proposal that surplus countries be forced by a "use-it-or-lose-it" mechanism, to either import from debtor states, build factories in debtor states or donate to debtor states.[15][16] The U.S. opposed Keynes' plan, and a senior official at the U.S. Treasury, Harry Dexter White, rejected Keynes' proposals, in favor of an International Monetary Fund with enough resources to counteract destabilizing flows of speculative finance.[17] However, unlike the modern IMF, White's proposed fund would have counteracted dangerous speculative flows automatically, with no political strings attached—i.e., no IMF conditionality.[18] Economic historian Brad Delong writes that on almost every point where he was overruled by the Americans, Keynes was later proved correct by events.[19][dubious – discuss]
.Today these key 1930s events look different to scholars of the era (see the work of Barry Eichengreen Golden Fetters: The Gold Standard and the Great Depression, 1919–1939 and How to Prevent a Currency War); in particular, devaluations today are viewed with more nuance. Ben Bernanke's opinion on the subject follows:
.... [T]he proximate cause of the world depression was a structurally flawed and poorly managed international gold standard. ... For a variety of reasons, including a desire of the Federal Reserve to curb the U.S. stock market boom, monetary policy in several major countries turned contractionary in the late 1920s—a contraction that was transmitted worldwide by the gold standard. What was initially a mild deflationary process began to snowball when the banking and currency crises of 1931 instigated an international "scramble for gold". Sterilization of gold inflows by surplus countries [the U.S. and France], substitution of gold for foreign exchange reserves, and runs on commercial banks all led to increases in the gold backing of money, and consequently to sharp unintended declines in national money supplies. Monetary contractions in turn were strongly associated with falling prices, output and employment. Effective international cooperation could in principle have permitted a worldwide monetary expansion despite gold standard constraints, but disputes over World War I reparations and war debts, and the insularity and inexperience of the Federal Reserve, among other factors, prevented this outcome. As a result, individual countries were able to escape the deflationary vortex only by unilaterally abandoning the gold standard and re-establishing domestic monetary stability, a process that dragged on in a halting and uncoordinated manner until France and the other Gold Bloc countries finally left gold in 1936. —Great Depression, B. Bernanke.In 1944 at Bretton Woods, as a result of the collective conventional wisdom of the time,[20] representatives from all the leading allied states collectively favored a regulated system of fixed exchange rates, indirectly disciplined by a US dollar tied to gold[21]—a system that relied on a regulated market economy with tight controls on the values of currencies. Flows of speculative international finance were curtailed by shunting them through and limiting them via central banks. This meant that international flows of investment went into foreign direct investment (FDI)—i.e., construction of factories overseas, rather than international currency manipulation or bond markets. Although the national experts disagreed to some degree on the specific implementation of this system, all agreed on the need for tight controls.
.Also based on experience of the inter-war years, U.S. planners developed a concept of economic security—that a liberal international economic system would enhance the possibilities of postwar peace. One of those who saw such a security link was Cordell Hull, the United States Secretary of State from 1933 to 1944.[Notes 1] Hull believed that the fundamental causes of the two world wars lay in economic discrimination and trade warfare. Hull argued
.[U]nhampered trade dovetailed with peace; high tariffs, trade barriers, and unfair economic competition, with war … if we could get a freer flow of trade…freer in the sense of fewer discriminations and obstructions…so that one country would not be deadly jealous of another and the living standards of all countries might rise, thereby eliminating the economic dissatisfaction that breeds war, we might have a reasonable chance of lasting peace.[22].The developed countries also agreed that the liberal international economic system required governmental intervention. In the aftermath of the Great Depression, public management of the economy had emerged as a primary activity of governments in the developed states. Employment, stability, and growth were now important subjects of public policy.
.In turn, the role of government in the national economy had become associated with the assumption by the state of the responsibility for assuring its citizens of a degree of economic well-being. The system of economic protection for at-risk citizens sometimes called the welfare state grew out of the Great Depression, which created a popular demand for governmental intervention in the economy, and out of the theoretical contributions of the Keynesian school of economics, which asserted the need for governmental intervention to counter market imperfections.
.However, increased government intervention in domestic economy brought with it isolationist sentiment that had a profoundly negative effect on international economics. The priority of national goals, independent national action in the interwar period, and the failure to perceive that those national goals could not be realized without some form of international collaboration—all resulted in "beggar-thy-neighbor" policies such as high tariffs, competitive devaluations that contributed to the breakdown of the gold-based international monetary system, domestic political instability, and international war. The lesson learned was, as the principal architect of the Bretton Woods system New Dealer Harry Dexter White put it:
.the absence of a high degree of economic collaboration among the leading nations will … inevitably result in economic warfare that will be but the prelude and instigator of military warfare on an even vaster scale..To ensure economic stability and political peace, states agreed to cooperate to closely regulate the production of their currencies to maintain fixed exchange rates between countries with the aim of more easily facilitating international trade. This was the foundation of the U.S. vision of postwar world free trade, which also involved lowering tariffs and, among other things, maintaining a balance of trade via fixed exchange rates that would be favorable to the capitalist system.
.Thus, the more developed market economies agreed with the U.S. vision of post-war international economic management, which intended to create and maintain an effective international monetary system and foster the reduction of barriers to trade and capital flows. In a sense, the new international monetary system was a return to a system similar to the pre-war gold standard, only using U.S. dollars as the world's new reserve currency until international trade reallocated the world's gold supply.
.Thus, the new system would be devoid (initially) of governments meddling with their currency supply as they had during the years of economic turmoil preceding WWII. Instead, governments would closely police the production of their currencies and ensure that they would not artificially manipulate their price levels. If anything, Bretton Woods was a return to a time devoid of increased governmental intervention in economies and currency systems.
.The Atlantic Charter, drafted during U.S. President Franklin D. Roosevelt's August 1941 meeting with British Prime Minister Winston Churchill on a ship in the North Atlantic, was the most notable precursor to the Bretton Woods Conference. Like Woodrow Wilson before him, whose "Fourteen Points" had outlined U.S. aims in the aftermath of the First World War, Roosevelt set forth a range of ambitious goals for the postwar world even before the U.S. had entered the Second World War.
.The Atlantic Charter affirmed the right of all states to equal access to trade and raw materials. Moreover, the charter called for freedom of the seas (a principal U.S. foreign policy aim since France and Britain had first threatened U.S. shipping in the 1790s), the disarmament of aggressors, and the "establishment of a wider and more permanent system of general security".
.As the war drew to a close, the Bretton Woods conference was the culmination of some two and a half years of planning for postwar reconstruction by the Treasuries of the U.S. and the UK. U.S. representatives studied with their British counterparts the reconstitution of what had been lacking between the two world wars: a system of international payments that would let countries trade without fear of sudden currency depreciation or wild exchange rate fluctuations—ailments that had nearly paralyzed world capitalism during the Great Depression.
.Without a strong European market for U.S. goods and services, most policymakers believed, the U.S. economy would be unable to sustain the prosperity it had achieved during the war.[23] In addition, U.S. unions had only grudgingly accepted government-imposed restraints on their demands during the war, but they were willing to wait no longer, particularly as inflation cut into the existing wage scales with painful force (by the end of 1945, there had already been major strikes in the automobile, electrical, and steel industries).[24]
.In early 1945, Bernard Baruch described the spirit of Bretton Woods as: if we can "stop subsidization of labor and sweated competition in the export markets", as well as prevent rebuilding of war machines, "oh boy, oh boy, what long term prosperity we will have."[25] The United States could therefore use its position of influence to reopen and control the rules of the world economy, so as to give unhindered access to all countries' markets and materials.
.United States allies—economically exhausted by the war—needed U.S. assistance to rebuild their domestic production and to finance their international trade; indeed, they needed it to survive.[14]
.Before the war, the French and the British realized that they could no longer compete with U.S. industries in an open marketplace. During the 1930s, the British created their own economic bloc to shut out U.S. goods. Churchill did not believe that he could surrender that protection after the war, so he watered down the Atlantic Charter's "free access" clause before agreeing to it.
.Yet U.S. officials were determined to open their access to the British empire. The combined value of British and U.S. trade was well over half of all the world's trade in goods. For the U.S. to open global markets, it first had to split the British (trade) empire. While Britain had economically dominated the 19th century, U.S. officials intended the second half of the 20th to be under U.S. hegemony.[26][27]
.A senior official of the Bank of England commented:
.One of the reasons Bretton Woods worked was that the U.S. was clearly the most powerful country at the table and so ultimately was able to impose its will on the others, including an often-dismayed Britain. At the time, one senior official at the Bank of England described the deal reached at Bretton Woods as "the greatest blow to Britain next to the war", largely because it underlined the way financial power had moved from the UK to the US.[28].A devastated Britain had little choice. Two world wars had destroyed the country's principal industries that paid for the importation of half of the nation's food and nearly all its raw materials except coal. The British had no choice but to ask for aid. Not until the United States signed an agreement on 6 December 1945 to grant Britain aid of $4.4 billion did the British Parliament ratify the Bretton Woods Agreements (which occurred later in December 1945).[29]
.Free trade relied on the free convertibility of currencies. Negotiators at the Bretton Woods conference, fresh from what they perceived as a disastrous experience with floating rates in the 1930s, concluded that major monetary fluctuations could stall the free flow of trade.
.The new economic system required an accepted vehicle for investment, trade, and payments. Unlike national economies, however, the international economy lacks a central government that can issue currency and manage its use. In the past this problem had been solved through the gold standard, but the architects of Bretton Woods did not consider this option feasible for the postwar political economy. Instead, they set up a system of fixed exchange rates managed by a series of newly created international institutions using the U.S. dollar (which was a gold standard currency for central banks) as a reserve currency.
.In the 19th and early 20th centuries gold played a key role in international monetary transactions. The gold standard was used to back currencies; the international value of currency was determined by its fixed relationship to gold; gold was used to settle international accounts. The gold standard maintained fixed exchange rates that were seen as desirable because they reduced the risk when trading with other countries.
.Imbalances in international trade were theoretically rectified automatically by the gold standard. A country with a deficit would have depleted gold reserves and would thus have to reduce its money supply. The resulting fall in demand would reduce imports and the lowering of prices would boost exports; thus, the deficit would be rectified. Any country experiencing inflation would lose gold and therefore would have a decrease in the amount of money available to spend.
.This decrease in the amount of money would act to reduce the inflationary pressure. Supplementing the use of gold in this period was the British pound. Based on the dominant British economy, the pound became a reserve, transaction, and intervention currency. But the pound was not up to the challenge of serving as the primary world currency, given the weakness of the British economy after the Second World War.
.The architects of Bretton Woods had conceived of a system wherein exchange rate stability was a prime goal. Yet, in an era of more activist economic policy, governments did not seriously consider permanently fixed rates on the model of the classical gold standard of the 19th century. Gold production was not even sufficient to meet the demands of growing international trade and investment. Further, a sizable share of the world's known gold reserves was located in the Soviet Union, which would later emerge as a Cold War rival to the United States and Western Europe.
.The only currency strong enough to meet the rising demands for international currency transactions was the U.S. dollar.[clarification needed] The strength of the U.S. economy, the fixed relationship of the dollar to gold ($35 an ounce), and the commitment of the U.S. government to convert dollars into gold at that price made the dollar as good as gold. In fact, the dollar was even better than gold: it earned interest and it was more flexible than gold.
.The rules of Bretton Woods, set forth in the articles of agreement of the International Monetary Fund (IMF) and the International Bank for Reconstruction and Development (IBRD), provided for a system of fixed exchange rates. The rules further sought to encourage an open system by committing members to the convertibility of their respective currencies into other currencies and to free trade.
.What emerged was the "pegged rate" currency regime. Members were required to establish a parity of their national currencies in terms of the reserve currency (a "peg") and to maintain exchange rates within plus or minus 1% of parity (a "band") by intervening in their foreign exchange markets (that is, buying or selling foreign money).
.In theory, the reserve currency would be the bancor (a World Currency Unit that was never implemented), proposed by John Maynard Keynes; however, the United States objected, and their request was granted, making the "reserve currency" the U.S. dollar. This meant that other countries would peg their currencies to the U.S. dollar, and—once convertibility was restored—would buy and sell U.S. dollars to keep market exchange rates within plus or minus 1% of parity. Thus, the U.S. dollar took over the role that gold had played under the gold standard in the international financial system.[30]
.Meanwhile, to bolster confidence in the dollar, the U.S. agreed separately to link the dollar to gold at the rate of $35 per ounce. At this rate, foreign governments and central banks could exchange dollars for gold. Bretton Woods established a system of payments based on the dollar, which defined all currencies in relation to the dollar, itself convertible into gold, and above all, "as good as gold" for trade. U.S. currency was now effectively the world currency, the standard to which every other currency was pegged.
.The U.S. dollar was the currency with the most purchasing power and it was the only currency that was backed by gold. Additionally, all European states that had been involved in World War II were highly in debt and transferred large amounts of gold into the United States, a fact that contributed to the supremacy of the United States. Thus, the U.S. dollar was strongly appreciated in the rest of the world and therefore became the key currency of the Bretton Woods system.
.Member countries could only change their par value by more than 10% with IMF approval, which was contingent on IMF determination that its balance of payments was in a "fundamental disequilibrium". The formal definition of fundamental disequilibrium was never determined, leading to uncertainty of approvals and attempts to repeatedly devalue by less than 10% instead.[31] Any country that changed without approval or after being denied approval was denied access to the IMF.
.Maintaining the fixed exchange system required countries to maintain sufficient foreign exchange reserves to intervene in markets and prevent fluctuations away from the pegged rate.[32]: 220  This also meant that international movement of capital could not be too large (because that might lead to large fluctuations in the exchange rate and necessitate costly market interventions that risked depleting a country's foreign exchange reserves).[32]: 220 
.The Bretton Woods Conference led to the establishment of the IMF and the IBRD (now the World Bank), which remain powerful forces in the world economy as of the 2020s.
.A major point of common ground at the Conference was the goal to avoid a recurrence of the closed markets and economic warfare that had characterized the 1930s. Thus, negotiators at Bretton Woods also agreed that there was a need for an institutional forum for international cooperation on monetary matters. Already in 1944, the British economist John Maynard Keynes emphasized "the importance of rule-based regimes to stabilize business expectations"—something he accepted in the Bretton Woods system of fixed exchange rates. Currency troubles in the interwar years, it was felt, had been greatly exacerbated by the absence of any established procedure or machinery for intergovernmental consultation.
.As a result of the establishment of agreed upon structures and rules of international economic interaction, conflict over economic issues was minimized, and the significance of the economic aspect of international relations seemed to recede.
.Officially established on 27 December 1945, when the 29 participating countries at the conference of Bretton Woods signed its Articles of Agreement, the IMF was to be the keeper of the rules and the main instrument of public international management. The Fund commenced its financial operations on 1 March 1947. IMF approval was necessary for any change in exchange rates in excess of 10%. It advised countries on policies affecting the monetary system and lent reserve currencies to countries that had incurred balance of payment debts.
.The big question at the Bretton Woods conference with respect to the institution that would emerge as the IMF was the issue of future access to international liquidity and whether that source should be akin to a world central bank able to create new reserves at will or a more limited borrowing mechanism.
.Although attended by 44 governments, discussions at the conference were dominated by two rival plans developed by the United States and Britain. Writing to the British Treasury, Keynes, who took the lead at the Conference, did not want many countries. He believed that those from the colonies and semi-colonies had "nothing to contribute and will merely encumber the ground".[33]
.As the chief international economist at the U.S. Treasury in 1942–44, Harry Dexter White drafted the U.S. blueprint for international access to liquidity, which competed with the plan drafted for the British Treasury by Keynes. Overall, White's scheme tended to favor incentives designed to create price stability within the world's economies, while Keynes wanted a system that encouraged economic growth. The "collective agreement was an enormous international undertaking" that took two years prior to the conference to prepare for. It consisted of numerous bilateral and multilateral meetings to reach common ground on what policies would make up the Bretton Woods system.
.At the time, gaps between the White and Keynes plans seemed enormous. White basically wanted a fund to reverse destabilizing flows of financial capital automatically. White proposed a new monetary institution called the Stabilization Fund that "would be funded with a finite pool of national currencies and gold… that would effectively limit the supply of reserve credit". Keynes wanted incentives for the U.S. to help Britain and the rest of Europe rebuild after WWII.[34] Outlining the difficulty of creating a system that every nation could accept in his speech at the closing plenary session of the Bretton Woods conference on 22 July 1944, Keynes stated:
.We, the delegates of this Conference, Mr President, have been trying to accomplish something very difficult to accomplish.[...] It has been our task to find a common measure, a common standard, a common rule acceptable to each and not irksome to any..Keynes' proposals would have established a world reserve currency (which he thought might be called "bancor") administered by a central bank vested with the power to create money and with the authority to take actions on a much larger scale.
.In the case of balance of payments imbalances, Keynes recommended that both debtors and creditors should change their policies. As outlined by Keynes, countries with payment surpluses should increase their imports from the deficit countries, build factories in debtor countries, or donate to them—and thereby create a foreign trade equilibrium.[15] Thus, Keynes was sensitive to the problem that placing too much of the burden on the deficit country would be deflationary.
.But the United States, as a likely creditor nation, and eager to take on the role of the world's economic powerhouse, used White's plan but targeted many of Keynes's concerns. White saw a role for global intervention in an imbalance only when it was caused by currency speculation.
.Although a compromise was reached on some points, because of the overwhelming economic and military power of the United States the participants at Bretton Woods largely agreed on White's plan. White’s plan was designed not merely to secure the rise and world economic domination of the United States, but to ensure that as the outgoing superpower Britain would be shuffled even further from centre stage.[35]
.What emerged largely reflected U.S. preferences: a system of subscriptions and quotas embedded in the IMF, which itself was to be no more than a fixed pool of national currencies and gold subscribed by each country, as opposed to a world central bank capable of creating money. The Fund was charged with managing various countries' trade deficits so that they would not produce currency devaluations that would trigger a decline in imports.
.The IMF is provided with a fund composed of contributions from member countries in gold and their own currencies. The original quotas were to total $8.8 billion. When joining the IMF, members are assigned "quotas" that reflect their relative economic power—and, as a sort of credit deposit, are obliged to pay a "subscription" of an amount commensurate with the quota. They pay the subscription as 25% in gold or currency convertible into gold (effectively the dollar, which at the founding, was the only currency then still directly gold convertible for central banks) and 75% in their own currency.
.Quota subscriptions form the largest source of money at the IMF's disposal. The IMF set out to use this money to grant loans to member countries with financial difficulties. Each member is then entitled to withdraw 25% of its quota immediately in case of payment problems. If this sum should be insufficient, each nation in the system is also able to request loans for foreign currency.
.In the event of a deficit in the current account, Fund members, when short of reserves, would be able to borrow foreign currency in amounts determined by the size of its quota. In other words, the higher the country's contribution was, the higher the sum of money it could borrow from the IMF.
.Members were required to pay back debts within a period of 18 months to five years. In turn, the IMF embarked on setting up rules and procedures to keep a country from going too deeply into debt year after year. The Fund would exercise "surveillance" over other economies for the U.S. Treasury in return for its loans to prop up national currencies.
.IMF loans were not comparable to loans issued by a conventional credit institution. Instead, they were effectively a chance to purchase a foreign currency with gold or the member's national currency.
.The U.S.-backed IMF plan sought to end restrictions on the transfer of goods and services from one country to another, eliminate currency blocs, and lift currency exchange controls.
.The IMF was designed to advance credits to countries with balance of payments deficits. Short-run balance of payment difficulties would be overcome by IMF loans, which would facilitate stable currency exchange rates. This flexibility meant a member state would not have to induce a depression to cut its national income down to such a low level that its imports would finally fall within its means. Thus, countries were to be spared the need to resort to the classical medicine of deflating themselves into drastic unemployment when faced with chronic balance of payments deficits. Before the Second World War, European governments—particularly Britain—often resorted to this.
.The IMF sought to provide for occasional discontinuous exchange-rate adjustments (changing a member's par value) by international agreement. Member states were permitted to adjust their currency exchange rate by 1%. This tended to restore equilibrium in their trade by expanding their exports and contracting imports. This would be allowed only if there was a fundamental disequilibrium. A decrease in the value of a country's money was called a devaluation, while an increase in the value of the country's money was called a revaluation.
.It was envisioned that these changes in exchange rates would be quite rare. However, the concept of fundamental disequilibrium, though key to the operation of the par value system, was never defined in detail.
.Never before had international monetary cooperation been attempted on a permanent institutional basis. Even more groundbreaking was the decision to allocate voting rights among governments, not on a one-state one-vote basis, but rather in proportion to quotas. Since the United States was contributing the most, U.S. leadership was the key. Under the system of weighted voting, the United States exerted a preponderant influence on the IMF. The United States held one-third of all IMF quotas at the outset, enough on its own to veto all changes to the IMF Charter.
.In addition, the IMF was based in Washington, D.C., and staffed mainly by U.S. economists. It regularly exchanged personnel with the U.S. Treasury. When the IMF began operations in 1946, President Harry S. Truman named White as its first U.S. Executive Director. Since no Deputy Managing Director post had yet been created, White served occasionally as Acting Managing Director and generally played a highly influential role during the IMF's first year. Truman had to abandon his original plan of naming White as IMF Executive Director when FBI Director J. Edgar Hoover submitted a report to the president, asserting that White was "a valuable adjunct to an underground Soviet espionage organization", who was placing individuals of high regard to Soviet intelligence inside the government.[36]
.The agreement made no provisions to create international reserves. It assumed new gold production would be sufficient. In the event of structural disequilibria, it expected that there would be national solutions, for example, an adjustment in the value of the currency or an improvement by other means of a country's competitive position. The IMF was left with few means, however, to encourage such national solutions.
.Economists and other planners recognized in 1944 that the new system could only commence after a return to normality following the disruption of World War II. It was expected that after a brief transition period of no more than five years, the international economy would recover, and the system would enter into operation.
.To promote growth of world trade and finance postwar reconstruction of Europe, the planners at Bretton Woods created another institution, the International Bank for Reconstruction and Development (IBRD), which is one of five agencies that make up the World Bank Group and is perhaps now the most important agency of the Group. The IBRD had an authorized capitalization of $10 billion and was expected to make loans of its own funds to underwrite private loans and to issue securities to raise new funds to make possible a speedy postwar recovery. The IBRD was to be a specialized agency of the United Nations, charged with making loans for economic development purposes.
.The Bretton Woods arrangements were largely adhered to and ratified by the participating governments. It was expected that national monetary reserves, supplemented with necessary IMF credits, would finance any temporary balance of payments disequilibria. But this did not prove sufficient to get Europe out of its conundrum.
.Postwar world capitalism suffered from a dollar shortage. The United States was running large balance of trade surpluses, and U.S. reserves were immense and growing. It was necessary to reverse this flow. Even though all countries wanted to buy U.S. exports, dollars had to leave the United States and become available for international use so they could do so. In other words, the United States would have to reverse the imbalances in global wealth by running a balance of trade deficit, financed by an outflow of U.S. reserves to other countries (a U.S. financial account deficit). The U.S. could run a financial deficit by either importing from, building plants in, or donating to foreign countries. Speculative investment was discouraged by the Bretton Woods agreement, and importing from other locations was not appealing in the 1950s, because U.S. technology was cutting edge at the time. So, multinational corporations and global aid that originated from the U.S. burgeoned.[37]
.The modest credit facilities of the IMF were clearly insufficient to deal with Western Europe's huge balance of payments deficits. The problem was further aggravated by the reaffirmation by the IMF Board of Governors of the provision in the Bretton Woods Articles of Agreement that the IMF could make loans only for current account deficits and not for capital and reconstruction purposes. Only the United States contribution of $570 million was actually available for IBRD lending. In addition, because the only available market for IBRD bonds was the conservative Wall Street banking market, the IBRD was forced to adopt a conservative lending policy, granting loans only when repayment was assured. Given these problems, by 1947 the IMF and the IBRD themselves were admitting that they could not deal with the international monetary system's economic problems.[38]
.The United States set up the European Recovery Program (Marshall Plan) to provide large-scale financial and economic aid for rebuilding Europe largely through grants rather than loans. Countries belonging to the Soviet bloc, e.g., Poland were invited to receive the grants, but were given a favorable agreement with the Soviet Union's COMECON.[39] In a speech at Harvard University on 5 June 1947, U.S. Secretary of State George Marshall stated:
.The breakdown of the business structure of Europe during the war was complete. … Europe's requirements for the next three or four years of foreign food and other essential products … principally from the United States … are so much greater than her present ability to pay that she must have substantial help or face economic, social and political deterioration of a very grave character..From 1947 until 1958, the U.S. deliberately encouraged an outflow of dollars, and, from 1950 on, the United States ran a balance of payments deficit with the intent of providing liquidity for the international economy. Dollars flowed out through various U.S. aid programs: the Truman Doctrine entailing aid to the pro-U.S. Greek and Turkish regimes, which were struggling to suppress communist revolution, aid to various pro-U.S. regimes in the Third World, and most importantly, the Marshall Plan. From 1948 to 1954 the United States provided 16 Western European countries with $17 billion in grants.
.To encourage long-term adjustment, the United States promoted European and Japanese trade competitiveness. Policies for economic controls on the defeated former Axis countries were scrapped. Aid to Europe and Japan was designed to rebuild productivity and export capacity. In the long run it was expected that such European and Japanese recovery would benefit the United States by widening markets for U.S. exports and providing locations for U.S. capital expansion.
.In 1945, Roosevelt and Churchill prepared the postwar era by negotiating with Joseph Stalin at Yalta about respective zones of influence; this same year Germany was divided into four occupation zones (Soviet, American, British, and French).
.Roosevelt and Henry Morgenthau insisted that the Big Five (United States, United Kingdom, France, the Soviet Union, and China) participate in the Bretton Woods conference in 1944,[40] but their plans were frustrated when the Soviet Union would not join the IMF. The reasons why the Soviet Union chose not to subscribe to the articles by December 1945 have been the subject of speculation.
.Facing the Soviet Union, whose power had also strengthened and whose territorial influence had expanded, the U.S. assumed the role of leader of the capitalist camp. The rise of the postwar U.S. as the world's leading industrial, monetary, and military power was rooted in the fact that the mainland U.S. was untouched by the war, in the instability of the nation states of postwar Europe, and the wartime devastation of the Soviet and European economies.
.Despite the economic cost implied by such a policy, being at the center of the international market gave the U.S. unprecedented freedom of action in pursuing its foreign affairs goals. A trade surplus made it easier to keep armies abroad and to invest outside the U.S., and because other countries could not sustain foreign deployments, the U.S. had the power to decide why, when and how to intervene in global crises. The dollar continued to function as a compass to guide the health of the world economy and exporting to the U.S. became the primary economic goal of developing or redeveloping economies. This arrangement came to be referred to as the Pax Americana, in analogy to the Pax Britannica of the late 19th century, the Pax Francica of the 17/18th centuries, the Pax Hispanica of the 16th century and the Pax Romana of the first. (See Globalism)
.After the end of World War II, the U.S. held $26 billion in gold reserves, of an estimated total of $40 billion (approx 65%). As world trade increased rapidly through the 1950s, the size of the gold base increased by only a few percentage points. In 1950, the U.S. balance of payments swung negative. The first U.S. response to the crisis was in the late 1950s when the Eisenhower administration placed import quotas on oil and other restrictions on trade outflows. More drastic measures were proposed, but not acted upon. However, with a mounting recession that began in 1958, this response alone was not sustainable. In 1960, with Kennedy's election, a decade-long effort to maintain the Bretton Woods System at the $35/ounce price began.
.The design of the Bretton Woods System was such that countries could only enforce convertibility to gold for the anchor currency—the United States dollar. Conversion of dollars to gold was allowed but was not required. Governments could forgo converting dollars to gold, and instead hold dollars. Rather than full convertibility, the system provided a fixed price for sales between central banks. However, there was still an open gold market. For the Bretton Woods system to remain workable, it would either have to alter the peg of the dollar to gold, or it would have to maintain the free market price for gold near the $35 per ounce official price. The greater the gap between free market gold prices and central bank gold prices, the greater the temptation to deal with internal economic issues by buying gold at the Bretton Woods price and selling it on the open market.
.In 1960 Robert Triffin, a Belgian-American economist, noticed that holding dollars was more valuable than gold because constant U.S. balance of payments deficits helped to keep the system liquid and fuel economic growth. What would later come to be known as Triffin's Dilemma was predicted when Triffin noted that if the U.S. failed to keep running deficits the system would lose its liquidity, not be able to keep up with the world's economic growth, and, thus, bring the system to a halt. But incurring such payment deficits also meant that, over time, the deficits would erode confidence in the dollar as the reserve currency created instability.[41]
.The first effort was the creation of the London Gold Pool on 1 November 1961 between eight governments. The theory behind the pool was that spikes in the free market price of gold, set by the morning gold fix in London, could be controlled by having a pool of gold to sell on the open market, that would then be recovered when the price of gold dropped. Gold's price spiked in response to events such as the Cuban Missile Crisis, and other less significant events, to as high as $40/ounce. The Kennedy administration drafted a radical change of the tax system to spur more production capacity and thus encourage exports. This culminated with the 1964 tax cut program, designed to maintain the $35 peg.
.In 1967, there was an attack on the pound and a run on gold in the sterling area, and on 18 November 1967, the British government was forced to devalue the pound.[42] U.S. President Lyndon Baines Johnson was faced with a difficult choice, either institute protectionist measures, including travel taxes, export subsidies and slashing the budget—or accept the risk of a "run on gold" and the dollar. From Johnson's perspective: "The world supply of gold is insufficient to make the present system workable—particularly as the use of the dollar as a reserve currency is essential to create the required international liquidity to sustain world trade and growth."[43]
.While West Germany agreed not to purchase gold from the U.S., and agreed to hold dollars instead, the pressure on both the dollar and the pound sterling continued. In January 1968 Johnson imposed a series of measures designed to end gold outflow, and to increase U.S. exports. This was unsuccessful, however, as in mid-March 1968 a dollar run on gold ensued through the free market in London, the London Gold Pool was dissolved, initially by the institution of ad hoc UK bank holidays at the request of the U.S. government. This was followed by a full closure of the London gold market, also at the request of the U.S. government, until a series of meetings were held that attempted to rescue or reform the existing system.[44]
.All attempts to maintain the peg collapsed in November 1968, and a new policy program attempted to convert the Bretton Woods system into an enforcement mechanism of floating the gold peg, which would be set by either fiat policy or by a restriction to honor foreign accounts. The collapse of the gold pool and the refusal of the pool members to trade gold with private entities—on 18 March 1968 the Congress of the United States repealed the 25% requirement of gold backing of the dollar[45]—as well as the U.S. pledge to suspend gold sales to governments that trade in the private markets,[46] led to the expansion of the private markets for international gold trade, in which the price of gold rose much higher than the official dollar price.[47][48]
U.S. gold reserves remained depleted due to the actions of some countries, notably France,[48] which continued to build up their own gold reserves.
.In the 1960s and 1970s, important structural changes eventually led to the breakdown of international monetary management. One change was the development of a high level of monetary interdependence. The stage was set for monetary interdependence by the return to convertibility of the Western European currencies at the end of 1958 and of the Japanese yen in 1964. Convertibility facilitated the vast expansion of international financial transactions, which deepened monetary interdependence.
.The Eurodollar market grew from zero before 1957 to $80 billion in 1972. Euromarkets increased international credit, liquidity, and volatility. Capital flows began to dwarf domestic and international efforts to regulate capital movements.[4]
.Another aspect of the internationalization of banking has been the emergence of international banking consortia. Since 1964 various banks had formed international syndicates, and by 1971 over three-quarters of the world's largest banks had become shareholders in such syndicates. Multinational banks can and do make large international transfers of capital not only for investment purposes but also for hedging and speculating against exchange rate fluctuations.
.These new forms of monetary interdependence made large capital flows possible. During the Bretton Woods era, countries were reluctant to alter exchange rates formally even in cases of structural disequilibria. Because such changes had a direct impact on certain domestic economic groups, they came to be seen as political risks for leaders. As a result, official exchange rates often became unrealistic in market terms, providing a virtually risk-free temptation for speculators. They could move from a weak to a strong currency hoping to reap profits when a revaluation occurred. If, however, monetary authorities managed to avoid revaluation, they could return to other currencies with no loss. The combination of risk-free speculation with the availability of large sums was highly destabilizing.
.A second structural change that undermined monetary management was the decline of U.S. hegemony. The U.S. was no longer the dominant economic power it had been for more than two decades. By the mid-1960s, the E.E.C. and Japan had become international economic powers in their own right. With total reserves exceeding those of the U.S., higher levels of growth and trade, and per capita income approaching that of the U.S., Europe and Japan were narrowing the gap between themselves and the United States.
.The shift toward a more pluralistic distribution of economic power led to increasing dissatisfaction with the privileged role of the U.S. dollar as the international currency. Acting effectively as the world's central banker, the U.S., through its deficit, determined the level of international liquidity. In an increasingly interdependent world, U.S. policy significantly influenced economic conditions in Europe and Japan. In addition, as long as other countries were willing to hold dollars, the U.S. could carry out massive foreign expenditures for political purposes—military activities and foreign aid—without the threat of balance-of-payments constraints.
.Dissatisfaction with the political implications of the dollar system was increased by détente between the U.S. and the Soviet Union. The Soviet military threat had been an important force in cementing the U.S.-led monetary system. The U.S. political and security umbrella helped make American economic domination palatable for Europe and Japan, which had been economically exhausted by the war. As gross domestic production grew in European countries, trade grew. When common security tensions lessened, this loosened the transatlantic dependence on defence concerns, and allowed latent economic tensions to surface.
.Reinforcing the relative decline in U.S. power and the dissatisfaction of Europe and Japan with the system was the continuing decline of the dollar—the foundation that had underpinned the post-1945 global trading system. The Vietnam War and the refusal of the administration of U.S. President Lyndon B. Johnson to pay for it and its Great Society programs through taxation resulted in an increased dollar outflow to pay for the military expenditures and rampant inflation, which led to the deterioration of the U.S. balance of trade position. In the late 1960s, the dollar was overvalued with its current trading position, while the German Mark and the yen were undervalued; and, naturally, the Germans and the Japanese had no desire to revalue and thereby make their exports more expensive, whereas the U.S. sought to maintain its international credibility by avoiding devaluation.[49] Meanwhile, the pressure on government reserves was intensified by the new international currency markets, with their vast pools of speculative capital moving around in search of quick profits.[48]
.In contrast, upon the creation of Bretton Woods, with the U.S. producing half of the world's manufactured goods and holding half its reserves, the twin burdens of international management and the Cold War were possible to meet at first. Throughout the 1950s Washington sustained a balance of payments deficit to finance loans, aid, and troops for allied regimes. But during the 1960s the costs of doing so became less tolerable. By 1970 the U.S. held under 16% of international reserves. Adjustment to these changed realities was impeded by the U.S. commitment to fixed exchange rates and by the U.S. obligation to convert dollars into gold on demand.
.By 1968, the attempt to defend the dollar at a fixed peg of $35/ounce, the policy of the Eisenhower, Kennedy and Johnson administrations, had become increasingly untenable. Gold outflows from the U.S. accelerated, and despite gaining assurances from Germany and other countries to hold gold, the unbalanced spending of the Johnson administration had transformed the dollar shortage of the 1940s and 1950s into a dollar glut by the 1960s.[50] In 1967, the IMF agreed in Rio de Janeiro to replace the tranche division set up in 1946. Special drawing rights (SDRs) were set as equal to one U.S. dollar but were not usable for transactions other than between banks and the IMF.[51] Governments were required to accept holding SDRs equal to three times their allotment, and interest would be charged, or credited, to each nation based on their SDR holding. The original interest rate was 1.5%.[52][53]
.The intent of the SDR system was to prevent countries from buying pegged gold and selling it at the higher free market price and give governments a reason to hold dollars by crediting interest, at the same time setting a clear limit to the amount of dollars that could be held.
.A negative balance of payments, growing public debt incurred by the Vietnam War and Great Society programs, and monetary inflation by the Federal Reserve caused the dollar to become increasingly overvalued.[54] The drain on U.S. gold reserves culminated with the London Gold Pool collapse in March 1968.[44] By 1970, the U.S. had seen its gold coverage deteriorate from 55% to 22%. This, in the view of neoclassical economists, represented the point where holders of the dollar had lost faith in the ability of the U.S. to cut budget and trade deficits.
.In 1971 more and more dollars were being printed in Washington, then being pumped overseas, to pay for government expenditure on the military and social programs. In the first six months of 1971, assets for $22 billion fled the U.S. In response, on 15 August 1971, Nixon issued Executive Order 11615 pursuant to the Economic Stabilization Act of 1970, unilaterally imposing 90-day wage and price controls, a 10% import surcharge, and most importantly "closed the gold window", making the dollar inconvertible to gold directly, except on the open market. Unusually, this decision was made without consulting members of the international monetary system or even his own State Department and was soon dubbed the "Nixon Shock".
.The August shock was followed by efforts under U.S. leadership to reform the international monetary system. Throughout the fall (autumn) of 1971, a series of multilateral and bilateral negotiations between the Group of Ten countries took place, seeking to redesign the exchange rate regime.
.Meeting in December 1971 at the Smithsonian Institution in Washington, D.C., the Group of Ten signed the Smithsonian Agreement. The U.S. pledged to peg the dollar at $38/ounce with 2.25% trading bands, and other countries agreed to appreciate their currencies versus the dollar. The group also planned to balance the world financial system using special drawing rights alone.
.The agreement failed to encourage discipline by the Federal Reserve or the United States government. The Federal Reserve was concerned about an increase in the domestic unemployment rate due to the devaluation of the dollar. In an attempt to undermine the efforts of the Smithsonian Agreement, the Federal Reserve lowered interest rates in pursuit of a previously established domestic policy objective of full national employment. With the Smithsonian Agreement, member countries anticipated a return flow of dollars to the U.S., but the reduced interest rates within the United States caused dollars to continue to flow out of the U.S. and into foreign central banks. The inflow of dollars into foreign banks continued the monetization of the dollar overseas, defeating the aims of the Smithsonian Agreement. As a result, the dollar price in the gold free market continued to cause pressure on its official rate; soon after a 10% devaluation was announced in February 1973, Japan and the EEC countries decided to let their currencies float. This proved to be the beginning of the collapse of the Bretton Woods System. The end of Bretton Woods was formally ratified by the Jamaica Accords in 1976. By the early 1980s, all industrialised states were using floating currencies.[55][56]
.During the 2008 financial crisis, some policymakers, such as James Chace[57] and others called for a new international monetary system that some of them also dub Bretton Woods II.[58]
.On 26 September 2008, French President Nicolas Sarkozy said, "we must rethink the financial system from scratch, as at Bretton Woods."[59]
.In March 2010, Prime Minister Papandreou of Greece wrote an op-ed in the International Herald Tribune, in which he said, "Democratic governments worldwide must establish a new global financial architecture, as bold in its own way as Bretton Woods, as bold as the creation of the European Community and European Monetary Union. And we need it fast." In interviews coinciding with his meeting with President Obama, he indicated that Obama would raise the issue of new regulations for the international financial markets at the next G20 meetings in June and November 2010.
.Over the course of the crisis, the IMF progressively relaxed its stance on "free-market" principles such as its guidance against using capital controls. In 2011, the IMF's managing director Dominique Strauss-Kahn stated that boosting employment and equity "must be placed at the heart" of the IMF's policy agenda.[60] The World Bank indicated a switch towards greater emphases on job creation.[61][62]
.Following the 2020 Economic Recession, the managing director of the IMF announced the emergence of "A New Bretton Woods Moment" which outlines the need for coordinated fiscal response on the part of central banks around the world to address the ongoing economic crisis.[63]
.The Bretton Woods System played a key role in shaping the postwar global economy, but its structure also led to significant challenges.[64] While it initially provided monetary stability and facilitated economic growth, it placed a heavy reliance on the U.S. dollar as the central reserve currency.[65] This gave the United States considerable influence over the international financial system and allowed it to run persistent trade and budget deficits with fewer immediate consequences than other nations.[66] Fixed exchange rates, a defining feature of the system, provided stability but also limited the flexibility of national governments to respond to economic shifts, sometimes leading to economic strain.[67]
.As global trade expanded, maintaining the system became increasingly difficult.[68] Rising inflation and growing U.S. trade deficits in the 1960s created tensions, particularly among European nations and Japan, which sought more control over their monetary policies.[69] The system's collapse in the early 1970s led to the widespread adoption of floating exchange rates, marking a significant shift in global finance.[70] The institutions created under Bretton Woods, particularly the International Monetary Fund (IMF) and the World Bank, have also been the subject of ongoing debate.[71] Critics argue that these institutions, while intended to promote global financial stability and development, have often imposed loan conditions that disproportionately burden developing nations.[72] The World Bank has faced scrutiny for funding projects linked to environmental and social disruptions, while the IMF has been criticized for enforcing austerity measures that some argue have deepened economic crises rather than alleviated them.[73] Critics also note that the IMF prioritizes the stability of financial institutions over individual economic freedoms, restricting the ability of local markets to self-correct and develop organically.[74][75]
.The system's collapse also contributed to broader discussions on alternative monetary arrangements.[76][77] Some economists and policymakers have advocated for a return to a commodity-backed system, while others have proposed a global reserve currency independent of any single nation, similar to Keynes’ earlier concept of the "Bancor."[78] More recently, decentralized financial systems, including Bitcoin and other cryptocurrencies, have been suggested as alternatives to government-controlled monetary systems, with proponents arguing that they offer a way to reduce reliance on centralized institutions.[79][80] While no single system has replaced Bretton Woods, its legacy continues to shape debates over global financial stability and monetary policy.[81]
.Dates are those when the rate was introduced; "*" indicates floating rate mostly taken prior to the introduction of the euro on 1 January 1999.[82]
.Note: Converted to euro on 1 January 1999 at €1 = DM 1.95583.
.Note: Converted to euro on 1 January 1999 at €1 = FRF 6.55957. Values prior to the currency reform of 1 January 1960 are shown in new francs or FRF worth 100 old francs.
.Note: Converted to euro on 1 January 1999 at €1 = 1,936.27 lire.
.Note: Converted to euro on 1 January 1999 at €1 = 166.386 pesetas.
.Note: Converted to euro on 1 January 1999 at €1 = 2.20371 gulden.
.Note: Converted to euro on 1 January 1999 at €1 = 40.3399 Belgian francs.
.Note: Converted to euro on 1 January 2001 at €1 = 340.75 drachmae.
.Note: Converted to euro on 1 January 2000 at €1 = FIM 5.94573. Prior to currency reform of 1 January 1963 values shown in new markkaa or FIM worth 100 old markkaa.
.General:
.
.The 2008 financial crisis, also known as the global financial crisis (GFC), was a major worldwide economic crisis, centered in the United States, which triggered the Great Recession of late 2007 to mid-2009, the most severe downturn since the 1929 Wall Street crash and Great Depression. The causes of the 2008 crisis included predatory lending in the form of subprime mortgages and a resulting U.S. housing bubble, excessive risk-taking by global financial institutions, and lack of regulatory oversight. The first phase of the crisis began in early 2007, as mortgage-backed securities (MBS) tied to U.S. real estate, and a vast web of derivatives linked to those MBS, collapsed in value. A liquidity crisis spread to global institutions by mid-2007 and climaxed with the bankruptcy of Lehman Brothers in September 2008, which triggered a stock market crash and international banking crisis.[1]
.During the 1990s, the U.S. Congress had passed legislation that intended to expand affordable housing through looser financing rules, and in 1999, parts of the 1933 Banking Act (Glass–Steagall Act) were repealed, enabling institutions to mix low-risk operations, such as commercial banking and insurance, with higher-risk operations such as investment banking and proprietary trading.[2] As the Federal Reserve ("Fed") lowered the federal funds rate from 2000 to 2003, institutions increasingly targeted low-income homebuyers, largely belonging to racial minorities, with high-risk loans;[3] this development went unattended by regulators.[4] As interest rates rose from 2004 to 2006, the cost of mortgages rose and the demand for housing fell; in early 2007, as more U.S. subprime mortgage holders began defaulting on their repayments, lenders went bankrupt, culminating in the bankruptcy of New Century Financial in April. As demand and prices continued to fall, the financial contagion spread to global credit markets by August 2007, and central banks began injecting liquidity. In March 2008, Bear Stearns, the fifth largest U.S. investment bank, was sold to JPMorgan Chase in a "fire sale" backed by Fed financing.
.In response to the growing crisis, governments around the world deployed massive bail-outs of financial institutions and other monetary and fiscal policies to prevent a collapse of the global financial system.[5] By July 2008, Fannie Mae and Freddie Mac, companies which together owned or guaranteed half of the U.S. housing market, verged on collapse; the Housing and Economic Recovery Act of 2008 enabled the federal government to seize them on September 7. Lehman Brothers (the fourth largest U.S. investment bank) filed for the largest bankruptcy in U.S. history on September 15, which was followed by a Fed bail-out of American International Group (the country's largest insurer) the next day, and the seizure of Washington Mutual in the largest bank failure in U.S. history on September 25. On October 3, Congress passed the Emergency Economic Stabilization Act, authorizing the Treasury Department to purchase toxic assets and bank stocks through the $700 billion Troubled Asset Relief Program (TARP). The Fed began a program of quantitative easing by buying treasury bonds and other assets, such as MBS, and the American Recovery and Reinvestment Act, signed in February 2009 by newly elected President Barack Obama, included a range of measures intended to preserve existing jobs and create new ones. These initiatives combined, coupled with actions taken in other countries, ended the worst of the Great Recession by mid-2009.
.Assessments of the crisis's impact in the U.S. vary, but suggest that some 8.7 million jobs were lost, causing unemployment to rise from 5 percent in 2007 to a high of 10 percent in October 2009. The percentage of citizens living in poverty rose from 12.5 percent in 2007 to 15.1 percent in 2010. The Dow Jones Industrial Average fell by 53 percent between October 2007 and March 2009, and some estimates suggest that one in four households lost 75 percent or more of their net worth. In 2010, the Dodd–Frank Wall Street Reform and Consumer Protection Act was passed, overhauling financial regulations.[6] It was opposed by many Republicans, and it was weakened by the Economic Growth, Regulatory Relief, and Consumer Protection Act in 2018. The Basel III capital and liquidity standards were also adopted by countries around the world.[7][8] The recession was a significant factor in the European debt crisis of the 2010s.
.The crisis sparked the Great Recession, which, at the time, was the most severe global recession since the Great Depression.[10][11][12][13][14] It was also followed by the European debt crisis, which began with a deficit in Greece in late 2009, and the 2008–2011 Icelandic financial crisis, which involved the bank failure of all three of the major banks in Iceland and, relative to the size of its economy, was the largest economic collapse suffered by any country in history.[15] It was among the five worst financial crises the world had experienced and led to a loss of more than $2 trillion from the global economy.[16][17] U.S. home mortgage debt relative to GDP increased from an average of 46% during the 1990s to 73% during 2008, reaching $10.5 (~$14.6 trillion in 2023) trillion.[18] The increase in cash out refinancings, as home values rose, fueled an increase in consumption that could no longer be sustained when home prices declined.[19][20][21] Many financial institutions owned investments whose value was based on home mortgages such as mortgage-backed securities, or credit derivatives used to insure them against failure, which declined in value significantly.[22][23][24] The International Monetary Fund estimated that large U.S. and European banks lost more than $1 trillion on toxic assets and from bad loans from January 2007 to September 2009.[25]
.Lack of investor confidence in bank solvency and declines in credit availability led to plummeting stock and commodity prices in late 2008 and early 2009.[26] The crisis rapidly spread into a global economic shock, resulting in several bank failures.[27] Economies worldwide slowed during this period since credit tightened and international trade declined.[28] Housing markets suffered and unemployment soared, resulting in evictions and foreclosures. Several businesses failed.[29][30] From its peak in the second quarter of 2007 at $61.4 trillion, household wealth in the United States fell $11 trillion, to $50.4 trillion by the end of the first quarter of 2009, resulting in a decline in consumption, then a decline in business investment.[31][32] In the fourth quarter of 2008, the quarter-over-quarter decline in real GDP in the U.S. was 8.4%.[33] The U.S. unemployment rate peaked at 11.0% in October 2009, the highest rate since 1983 and roughly twice the pre-crisis rate. The average hours per work week declined to 33, the lowest level since the government began collecting the data in 1964.[34][35]
.The economic crisis started in the U.S. but spread to the rest of the world.[29] U.S. consumption accounted for more than a third of the growth in global consumption between 2000 and 2007 and the rest of the world depended on the U.S. consumer as a source of demand.[citation needed][36][37] Toxic securities were owned by corporate and institutional investors globally. Derivatives such as credit default swaps also increased the linkage between large financial institutions. The de-leveraging of financial institutions, as assets were sold to pay back obligations that could not be refinanced in frozen credit markets, further accelerated the solvency crisis and caused a decrease in international trade. Reductions in the growth rates of developing countries were due to falls in trade, commodity prices, investment and remittances sent from migrant workers (example: Armenia[38]). States with fragile political systems feared that investors from Western states would withdraw their money because of the crisis.[39]
.As part of national fiscal policy response to the Great Recession, governments and central banks, including the Federal Reserve, the European Central Bank and the Bank of England, provided then-unprecedented trillions of dollars in bailouts and stimulus, including expansive fiscal policy and monetary policy to offset the decline in consumption and lending capacity, avoid a further collapse, encourage lending, restore faith in the integral commercial paper markets, avoid the risk of a deflationary spiral, and provide banks with enough funds to allow customers to make withdrawals.[40] In effect, the central banks went from being the "lender of last resort" to the "lender of only resort" for a significant portion of the economy. In some cases the Fed was considered the "buyer of last resort".[41][42][43][44][45] During the fourth quarter of 2008, these central banks purchased US$2.5 (~$3.47 trillion in 2023) trillion of government debt and troubled private assets from banks. This was the largest liquidity injection into the credit market, and the largest monetary policy action in world history. Following a model initiated by the 2008 United Kingdom bank rescue package,[46][47] the governments of European nations and the United States guaranteed the debt issued by their banks and raised the capital of their national banking systems, ultimately purchasing $1.5 trillion newly issued preferred stock in major banks.[32] The Federal Reserve created then-significant amounts of new currency as a method to combat the liquidity trap.[48]
.Bailouts came in the form of trillions of dollars of loans, asset purchases, guarantees, and direct spending.[49] Significant controversy accompanied the bailouts, such as in the case of the AIG bonus payments controversy, leading to the development of a variety of "decision making frameworks", to help balance competing policy interests during times of financial crisis.[50] Alistair Darling, the U.K.'s Chancellor of the Exchequer at the time of the crisis, stated in 2018 that Britain came within hours of "a breakdown of law and order" the day that Royal Bank of Scotland was bailed-out.[51] Instead of financing more domestic loans, some banks instead spent some of the stimulus money in more profitable areas such as investing in emerging markets and foreign currencies.[52]
.In July 2010, the Dodd–Frank Wall Street Reform and Consumer Protection Act was enacted in the United States to "promote the financial stability of the United States".[53] The Basel III capital and liquidity standards were adopted worldwide.[54] Since the 2008 financial crisis, consumer regulators in America have more closely supervised sellers of credit cards and home mortgages in order to deter anticompetitive practices that led to the crisis.[55]
.At least two major reports on the causes of the crisis were produced by the U.S. Congress: the Financial Crisis Inquiry Commission report, released January 2011, and a report by the United States Senate Homeland Security Permanent Subcommittee on Investigations entitled Wall Street and the Financial Crisis: Anatomy of a Financial Collapse, released April 2011.
.In total, 47 bankers served jail time as a result of the crisis, over half of which were from Iceland, where the crisis was the most severe and led to the collapse of all three major Icelandic banks.[56] In April 2012, Geir Haarde of Iceland became the only politician to be convicted as a result of the crisis.[57][58] Only one banker in the United States served jail time as a result of the crisis, Kareem Serageldin, a banker at Credit Suisse who was sentenced to 30 months in jail and returned $24.6 million in compensation for manipulating bond prices to hide $1 billion of losses.[59][56] No individuals in the United Kingdom were convicted as a result of the crisis.[60][61] Goldman Sachs paid $550 million to settle fraud charges after allegedly anticipating the crisis and selling toxic investments to its clients.[62]
.With fewer resources to risk in creative destruction, the number of patent applications was flat, compared to exponential increases in patent application in prior years.[63]
.Typical American families did not fare well, nor did the "wealthy-but-not-wealthiest" families just beneath the pyramid's top.[64][65][66] However, half of the poorest families in the United States did not have wealth declines at all during the crisis because they generally did not own financial investments whose value could fluctuate. The Federal Reserve surveyed 4,000 households from 2007 to 2009, and found that the total wealth of 63% of all Americans declined in that period and 77% of the richest families had a decrease in total wealth, while only 50% of those on the bottom of the pyramid suffered a decrease.[67][68][69]
.The following is a timeline of the major events of the financial crisis, including government responses, and the subsequent economic recovery.[70][71][72][73]
.There is a really good reason for tighter credit. Tens of millions of homeowners who had substantial equity in their homes two years ago have little or nothing today. Businesses are facing the worst downturn since the Great Depression. This matters for credit decisions. A homeowner with equity in her home is very unlikely to default on a car loan or credit card debt. They will draw on this equity rather than lose their car and/or have a default placed on their credit record. On the other hand, a homeowner who has no equity is a serious default risk. In the case of businesses, their creditworthiness depends on their future profits. Profit prospects look much worse in November 2008 than they did in November 2007 ... While many banks are obviously at the brink, consumers and businesses would be facing a much harder time getting credit right now even if the financial system were rock solid. The problem with the economy is the loss of close to $6 trillion in housing wealth and an even larger amount of stock wealth.[164].... the pace of economic contraction is slowing. Conditions in financial markets have generally improved in recent months. Household spending has shown further signs of stabilizing but remains constrained by ongoing job losses, lower housing wealth, and tight credit. Businesses are cutting back on fixed investment and staffing but appear to be making progress in bringing inventory stocks into better alignment with sales. Although economic activity is likely to remain weak for a time, the Committee continues to anticipate that policy actions to stabilize financial markets and institutions, fiscal and monetary stimulus, and market forces will contribute to a gradual resumption of sustainable economic growth in a context of price stability.[188].In the table, the names of emerging and developing economies are shown in boldface type, while the names of developed economies are in Roman (regular) type.
.The twenty largest economies contributing to global GDP (PPP) growth (2007–2017)[217]
.The expansion of central bank lending in response to the crisis was not confined to the Federal Reserve's provision of aid to individual financial institutions. The Federal Reserve has also conducted several innovative lending programs to improve liquidity and strengthen different financial institutions and markets, such as Freddie Mac and Fannie Mae. In this case, the major problem in the market is the lack of free cash reserves and flows to secure the loans. The Federal Reserve took many steps to deal with financial market liquidity worries. One of these steps was a credit line for major traders, who act as the Fed's partners in open market activities.[218] Also, loan programs were set up to make the money market mutual funds and commercial paper market more flexible. Also, the Term Asset-Backed Securities Loan Facility (TALF) was put in place thanks to a joint effort with the US Department of the Treasury. This plan made it easier for consumers and businesses to get credit by giving Americans who owned high-quality asset-backed securities more credit.
.Before the crisis, the Federal Reserve's stocks of Treasury securities were sold to pay for the increase in credit. This method was meant to keep banks from trying to give out their extra savings, which could cause the federal funds rate to drop below where it was supposed to be.[219] However, in October 2008, the Federal Reserve was granted the power to provide banks with interest payments on their surplus reserves. This created a motivation for banks to retain their reserves instead of disbursing them, thus reducing the need for the Federal Reserve to hedge its increased lending by decreases in alternative assets.[220]
.Money market funds also went through runs when people lost faith in the market. To keep it from getting worse, the Fed said it would give money to mutual fund companies. Also, Department of Treasury said that it would briefly cover the assets of the fund. Both of these things helped get the fund market back to normal, which helped the commercial paper market, which most businesses use to run. The FDIC also did several things, such as raising the insurance cap from $100,000 to $250,000, to boost customer trust.
.They engaged in quantitative easing, which added more than $4 trillion to the financial system and got banks to start lending again, both to each other and to people. Many homeowners who were trying to keep their homes from going into default got housing credits. A package of policies was passed that let borrowers refinance their loans even though the value of their homes was less than what they still owed on their mortgages.[221]
.While the causes of the bubble and subsequent crash are disputed, the precipitating factor for the Financial Crisis of 2007–2008 was the bursting of the United States housing bubble and the subsequent subprime mortgage crisis, which occurred due to a high default rate and resulting foreclosures of mortgage loans, particularly adjustable-rate mortgages. Some or all of the following factors contributed to the crisis:[222][76][77]
.The relaxing of credit lending standards by investment banks and commercial banks allowed for a significant increase in subprime lending. Subprime had not become less risky; Wall Street just accepted this higher risk.[261]
.Due to competition between mortgage lenders for revenue and market share, and when the supply of creditworthy borrowers was limited, mortgage lenders relaxed underwriting standards and originated riskier mortgages to less creditworthy borrowers. In the view of some analysts, the relatively conservative government-sponsored enterprises (GSEs) policed mortgage originators and maintained relatively high underwriting standards prior to 2003. However, as market power shifted from securitizers to originators, and as intense competition from private securitizers undermined GSE power, mortgage standards declined and risky loans proliferated. The riskiest loans were originated in 2004–2007, the years of the most intense competition between securitizers and the lowest market share for the GSEs. The GSEs eventually relaxed their standards to try to catch up with the private banks.[262][263]
.A contrarian view is that Fannie Mae and Freddie Mac led the way to relaxed underwriting standards, starting in 1995, by advocating the use of easy-to-qualify automated underwriting and appraisal systems, by designing no-down-payment products issued by lenders, by the promotion of thousands of small mortgage brokers, and by their close relationship to subprime loan aggregators such as Countrywide.[264][265]
.Depending on how "subprime" mortgages are defined, they remained below 10% of all mortgage originations until 2004, when they rose to nearly 20% and remained there through the 2005–2006 peak of the United States housing bubble.[266]
.The majority report of the Financial Crisis Inquiry Commission, written by the six Democratic appointees, the minority report, written by three of the four Republican appointees, studies by Federal Reserve economists, and the work of several independent scholars generally contend that government affordable housing policy was not the primary cause of the financial crisis. Although they concede that governmental policies had some role in causing the crisis, they contend that GSE loans performed better than loans securitized by private investment banks, and performed better than some loans originated by institutions that held loans in their own portfolios.
.In his dissent to the majority report of the Financial Crisis Inquiry Commission, conservative American Enterprise Institute fellow Peter J. Wallison[267] stated his belief that the roots of the financial crisis can be traced directly and primarily to affordable housing policies initiated by the United States Department of Housing and Urban Development (HUD) in the 1990s and to massive risky loan purchases by government-sponsored entities Fannie Mae and Freddie Mac. Based upon information in the SEC's December 2011 securities fraud case against six former executives of Fannie and Freddie, Peter Wallison and Edward Pinto estimated that, in 2008, Fannie and Freddie held 13 million substandard loans totaling over $2 trillion.[268]
.In the early and mid-2000s, the Bush administration called numerous times for investigations into the safety and soundness of the GSEs and their swelling portfolio of subprime mortgages. On September 10, 2003, the United States House Committee on Financial Services held a hearing, at the urging of the administration, to assess safety and soundness issues and to review a recent report by the Office of Federal Housing Enterprise Oversight (OFHEO) that had uncovered accounting discrepancies within the two entities.[269][270] The hearings never resulted in new legislation or formal investigation of Fannie Mae and Freddie Mac, as many of the committee members refused to accept the report and instead rebuked OFHEO for their attempt at regulation.[271] Some, such as Wallison, believe this was an early warning to the systemic risk that the growing market in subprime mortgages posed to the U.S. financial system that went unheeded.[272]
.A 2000 United States Department of the Treasury study of lending trends for 305 cities from 1993 to 1998 showed that $467 billion of mortgage lending was made by Community Reinvestment Act (CRA)-covered lenders into low and mid-level income (LMI) borrowers and neighborhoods, representing 10% of all U.S. mortgage lending during the period. The majority of these were prime loans. Sub-prime loans made by CRA-covered institutions constituted a 3% market share of LMI loans in 1998,[273] but in the run-up to the crisis, fully 25% of all subprime lending occurred at CRA-covered institutions and another 25% of subprime loans had some connection with CRA.[274] However, most sub-prime loans were not made to the LMI borrowers targeted by the CRA,[citation needed][275][276] especially in the years 2005–2006 leading up to the crisis,[citation needed][277][276][278] nor did it find any evidence that lending under the CRA rules increased delinquency rates or that the CRA indirectly influenced independent mortgage lenders to ramp up sub-prime lending.[279][verification needed]
.To other analysts the delay between CRA rule changes in 1995 and the explosion of subprime lending is not surprising, and does not exonerate the CRA. They contend that there were two, connected causes to the crisis: the relaxation of underwriting standards in 1995 and the ultra-low interest rates initiated by the Federal Reserve after the terrorist attack on September 11, 2001. Both causes had to be in place before the crisis could take place.[280] Critics also point out that publicly announced CRA loan commitments were massive, totaling $4.5 trillion in the years between 1994 and 2007.[281] They also argue that the Federal Reserve's classification of CRA loans as "prime" is based on the faulty and self-serving assumption that high-interest-rate loans (3 percentage points over average) equal "subprime" loans.[282]
.Others have pointed out that there were not enough of these loans made to cause a crisis of this magnitude. In an article in Portfolio magazine, Michael Lewis spoke with one trader who noted that "There weren't enough Americans with [bad] credit taking out [bad loans] to satisfy investors' appetite for the end product." Essentially, investment banks and hedge funds used financial innovation to enable large wagers to be made, far beyond the actual value of the underlying mortgage loans, using derivatives called credit default swaps, collateralized debt obligations and synthetic CDOs.
.By March 2011, the FDIC had paid out $9 billion (c. $12 billion in 2023[283]) to cover losses on bad loans at 165 failed financial institutions.[284][285] The Congressional Budget Office estimated, in June 2011, that the bailout to Fannie Mae and Freddie Mac exceeds $300 billion (c. $401 billion in 2023[283]) (calculated by adding the fair value deficits of the entities to the direct bailout funds at the time).[286]
.Economist Paul Krugman argued in January 2010 that the simultaneous growth of the residential and commercial real estate pricing bubbles and the global nature of the crisis undermines the case made by those who argue that Fannie Mae, Freddie Mac, CRA, or predatory lending were primary causes of the crisis. In other words, bubbles in both markets developed even though only the residential market was affected by these potential causes.[287]
.Countering Krugman, Wallison wrote: "It is not true that every bubble—even a large bubble—has the potential to cause a financial crisis when it deflates." Wallison notes that other developed countries had "large bubbles during the 1997–2007 period" but "the losses associated with mortgage delinquencies and defaults when these bubbles deflated were far lower than the losses suffered in the United States when the 1997–2007 [bubble] deflated." According to Wallison, the reason the U.S. residential housing bubble (as opposed to other types of bubbles) led to financial crisis was that it was supported by a huge number of substandard loans—generally with low or no downpayments.[288]
.Krugman's contention (that the growth of a commercial real estate bubble indicates that U.S. housing policy was not the cause of the crisis) is challenged by additional analysis. After researching the default of commercial loans during the financial crisis, Xudong An and Anthony B. Sanders reported (in December 2010): "We find limited evidence that substantial deterioration in CMBS [commercial mortgage-backed securities] loan underwriting occurred prior to the crisis."[289] Other analysts support the contention that the crisis in commercial real estate and related lending took place after the crisis in residential real estate. Business journalist Kimberly Amadeo reported: "The first signs of decline in residential real estate occurred in 2006. Three years later, commercial real estate started feeling the effects."[verification needed][290] Denice A. Gierach, a real estate attorney and CPA, wrote:
.... most of the commercial real estate loans were good loans destroyed by a really bad economy. In other words, the borrowers did not cause the loans to go bad-it was the economy.[291].Between 1998 and 2006, the price of the typical American house increased by 124%.[292] During the 1980s and 1990s, the national median home price ranged from 2.9 to 3.1 times median household income. By contrast, this ratio increased to 4.0 in 2004, and 4.6 in 2006.[293] This housing bubble resulted in many homeowners refinancing their homes at lower interest rates, or financing consumer spending by taking out second mortgages secured by the price appreciation.
.In a Peabody Award-winning program, NPR correspondents argued that a "Giant Pool of Money" (represented by $70 trillion in worldwide fixed income investments) sought higher yields than those offered by U.S. Treasury bonds early in the decade. This pool of money had roughly doubled in size from 2000 to 2007, yet the supply of relatively safe, income generating investments had not grown as fast. Investment banks on Wall Street answered this demand with products such as the mortgage-backed security and the collateralized debt obligation that were assigned safe ratings by the credit rating agencies.[294]
.In effect, Wall Street connected this pool of money to the mortgage market in the US, with enormous fees accruing to those throughout the mortgage supply chain, from the mortgage broker selling the loans to small banks that funded the brokers and the large investment banks behind them. By approximately 2003, the supply of mortgages originated at traditional lending standards had been exhausted, and continued strong demand began to drive down lending standards.[294]
.The collateralized debt obligation in particular enabled financial institutions to obtain investor funds to finance subprime and other lending, extending or increasing the housing bubble and generating large fees. This essentially places cash payments from multiple mortgages or other debt obligations into a single pool from which specific securities draw in a specific sequence of priority. Those securities first in line received investment-grade ratings from rating agencies. Securities with lower priority had lower credit ratings but theoretically a higher rate of return on the amount invested.[295]
.By September 2008, average U.S. housing prices had declined by over 20% from their mid-2006 peak.[296][297] As prices declined, borrowers with adjustable-rate mortgages could not refinance to avoid the higher payments associated with rising interest rates and began to default. During 2007, lenders began foreclosure proceedings on nearly 1.3 million properties, a 79% increase over 2006.[298] This increased to 2.3 million in 2008, an 81% increase vs. 2007.[299] By August 2008, approximately 9% of all U.S. mortgages outstanding were either delinquent or in foreclosure.[300] By September 2009, this had risen to 14.4%.[301][302]
.After the bubble burst, Australian economist John Quiggin wrote, "And, unlike the Great Depression, this crisis was entirely the product of financial markets. There was nothing like the postwar turmoil of the 1920s, the struggles over gold convertibility and reparations, or the Smoot-Hawley tariff, all of which have shared the blame for the Great Depression." Instead, Quiggin lays the blame for the 2008 near-meltdown on financial markets, on political decisions to lightly regulate them, and on rating agencies which had self-interested incentives to give good ratings.[303]
.Lower interest rates encouraged borrowing. From 2000 to 2003, the Federal Reserve lowered the federal funds rate target from 6.5% to 1.0%.[304][305] This was done to soften the effects of the collapse of the dot-com bubble and the September 11 attacks, as well as to combat a perceived risk of deflation.[306] As early as 2002, it was apparent that credit was fueling housing instead of business investment as some economists went so far as to advocate that the Fed "needs to create a housing bubble to replace the Nasdaq bubble".[307] Moreover, empirical studies using data from advanced countries show that excessive credit growth contributed greatly to the severity of the crisis.[308]
.Additional downward pressure on interest rates was created by rising U.S. current account deficit, which peaked along with the housing bubble in 2006. Federal Reserve chairman Ben Bernanke explained how trade deficits required the U.S. to borrow money from abroad, in the process bidding up bond prices and lowering interest rates.[309]
.Bernanke explained that between 1996 and 2004, the U.S. current account deficit increased by $650 billion, from 1.5% to 5.8% of GDP. Financing these deficits required the country to borrow large sums from abroad, much of it from countries running trade surpluses. These were mainly the emerging economies in Asia and oil-exporting nations. The balance of payments identity requires that a country (such as the US) running a current account deficit also have a capital account (investment) surplus of the same amount. Hence large and growing amounts of foreign funds (capital) flowed into the U.S. to finance its imports.
.All of this created demand for various types of financial assets, raising the prices of those assets while lowering interest rates. Foreign investors had these funds to lend either because they had very high personal savings rates (as high as 40% in China) or because of high oil prices. Ben Bernanke referred to this as a "saving glut".[310]
.A flood of funds (capital or liquidity) reached the U.S. financial markets. Foreign governments supplied funds by purchasing Treasury bonds and thus avoided much of the direct effect of the crisis. U.S. households, used funds borrowed from foreigners to finance consumption or to bid up the prices of housing and financial assets. Financial institutions invested foreign funds in mortgage-backed securities.[citation needed]
.The Fed then raised the Fed funds rate significantly between July 2004 and July 2006.[311] This contributed to an increase in one-year and five-year adjustable-rate mortgage (ARM) rates, making ARM interest rate resets more expensive for homeowners.[312] This may have also contributed to the deflating of the housing bubble, as asset prices generally move inversely to interest rates, and it became riskier to speculate in housing.[313][314] U.S. housing and financial assets dramatically declined in value after the housing bubble burst.[315][32]
.Subprime lending standards declined in the U.S.: in early 2000, a subprime borrower had a FICO score of 660 or less. By 2005, many lenders dropped the required FICO score to 620, making it much easier to qualify for prime loans and making subprime lending a riskier business. Proof of income and assets were de-emphasized. Loans at first required full documentation, then low documentation, then no documentation. One subprime mortgage product that gained wide acceptance was the no income, no job, no asset verification required (NINJA) mortgage. Informally, these loans were aptly referred to as "liar loans" because they encouraged borrowers to be less than honest in the loan application process.[316] Testimony given to the Financial Crisis Inquiry Commission by whistleblower Richard M. Bowen III, on events during his tenure as the Business Chief Underwriter for Correspondent Lending in the Consumer Lending Group for Citigroup, where he was responsible for over 220 professional underwriters, suggests that by 2006 and 2007, the collapse of mortgage underwriting standards was endemic. His testimony stated that by 2006, 60% of mortgages purchased by Citigroup from some 1,600 mortgage companies were "defective" (were not underwritten to policy, or did not contain all policy-required documents)—this, despite the fact that each of these 1,600 originators was contractually responsible (certified via representations and warrantees) that its mortgage originations met Citigroup standards. Moreover, during 2007, "defective mortgages (from mortgage originators contractually bound to perform underwriting to Citi's standards) increased ... to over 80% of production".[317]
.In separate testimony to the Financial Crisis Inquiry Commission, officers of Clayton Holdings, the largest residential loan due diligence and securitization surveillance company in the United States and Europe, testified that Clayton's review of over 900,000 mortgages issued from January 2006 to June 2007 revealed that scarcely 54% of the loans met their originators' underwriting standards. The analysis (conducted on behalf of 23 investment and commercial banks, including 7 "too big to fail" banks) additionally showed that 28% of the sampled loans did not meet the minimal standards of any issuer. Clayton's analysis further showed that 39% of these loans (i.e. those not meeting any issuer's minimal underwriting standards) were subsequently securitized and sold to investors.[318][319]
.Predatory lending refers to the practice of unscrupulous lenders, enticing borrowers to enter into "unsafe" or "unsound" secured loans for inappropriate purposes.[320][321][322]
.In June 2008, Countrywide Financial was sued by then California Attorney General Jerry Brown for "unfair business practices" and "false advertising", alleging that Countrywide used "deceptive tactics to push homeowners into complicated, risky, and expensive loans so that the company could sell as many loans as possible to third-party investors".[323] In May 2009, Bank of America modified 64,000 Countrywide loans as a result.[324] When housing prices decreased, homeowners in ARMs then had little incentive to pay their monthly payments, since their home equity had disappeared. This caused Countrywide's financial condition to deteriorate, ultimately resulting in a decision by the Office of Thrift Supervision to seize the lender. One Countrywide employee—who would later plead guilty to two counts of wire fraud and spent 18 months in prison—stated that, "If you had a pulse, we gave you a loan."[325]
.Former employees from Ameriquest, which was United States' leading wholesale lender, described a system in which they were pushed to falsify mortgage documents and then sell the mortgages to Wall Street banks eager to make fast profits. There is growing evidence that such mortgage frauds may be a cause of the crisis.[326]
.According to Barry Eichengreen, the roots of the financial crisis lay in the deregulation of financial markets.[327] A 2012 OECD study[328] suggest that bank regulation based on the Basel accords encourage unconventional business practices and contributed to or even reinforced the financial crisis. In other cases, laws were changed or enforcement weakened in parts of the financial system. Key examples include:
.A 2011 paper suggested that Canada's avoidance of a banking crisis in 2008 (as well as in prior eras) could be attributed to Canada possessing a single, powerful, overarching regulator, while the United States had a weak, crisis prone and fragmented banking system with multiple competing regulatory bodies.[346]
.Prior to the crisis, financial institutions became highly leveraged, increasing their appetite for risky investments and reducing their resilience in case of losses. Much of this leverage was achieved using complex financial instruments such as off-balance sheet securitization and derivatives, which made it difficult for creditors and regulators to monitor and try to reduce financial institution risk levels.[347][verification needed]
.U.S. households and financial institutions became increasingly indebted or overleveraged during the years preceding the crisis.[348] This increased their vulnerability to the collapse of the housing bubble and worsened the ensuing economic downturn.[349] Key statistics include:
.Free cash used by consumers from home equity extraction doubled from $627 billion in 2001 to $1,428 billion in 2005 as the housing bubble built, a total of nearly $5 trillion over the period, contributing to economic growth worldwide.[19][20][21] U.S. home mortgage debt relative to GDP increased from an average of 46% during the 1990s to 73% during 2008, reaching $10.5 trillion (c. $14.6 trillion in 2023[283]).[18]
.U.S. household debt as a percentage of annual disposable personal income was 127% at the end of 2007, versus 77% in 1990.[348] In 1981, U.S. private debt was 123% of GDP; by the third quarter of 2008, it was 290%.[350]
.From 2004 to 2007, the top five U.S. investment banks each significantly increased their financial leverage, which increased their vulnerability to a financial shock. Changes in capital requirements, intended to keep U.S. banks competitive with their European counterparts, allowed lower risk weightings for AAA-rated securities. The shift from first-loss tranches to AAA-rated tranches was seen by regulators as a risk reduction that compensated the higher leverage.[351] These five institutions reported over $4.1 trillion in debt for fiscal year 2007, about 30% of U.S. nominal GDP for 2007. Lehman Brothers went bankrupt and was liquidated, Bear Stearns and Merrill Lynch were sold at fire-sale prices, and Goldman Sachs and Morgan Stanley became commercial banks, subjecting themselves to more stringent regulation. With the exception of Lehman, these companies required or received government support.[352]
.Fannie Mae and Freddie Mac, two U.S. government-sponsored enterprises, owned or guaranteed nearly $5 trillion (c. $6.95 trillion in 2023[283]) trillion in mortgage obligations at the time they were placed into conservatorship by the U.S. government in September 2008.[353][354]
.These seven entities were highly leveraged and had $9 trillion in debt or guarantee obligations; yet they were not subject to the same regulation as depository banks.[337][355]
.Behavior that may be optimal for an individual, such as saving more during adverse economic conditions, can be detrimental if too many individuals pursue the same behavior, as ultimately one person's consumption is another person's income. Too many consumers attempting to save or pay down debt simultaneously is called the paradox of thrift and can cause or deepen a recession. Economist Hyman Minsky also described a "paradox of deleveraging" as financial institutions that have too much leverage (debt relative to equity) cannot all de-leverage simultaneously without significant declines in the value of their assets.[349]
.In April 2009, Federal Reserve vice-chair Janet Yellen discussed these paradoxes:
.Once this massive credit crunch hit, it didn't take long before we were in a recession. The recession, in turn, deepened the credit crunch as demand and employment fell, and credit losses of financial institutions surged. Indeed, we have been in the grips of precisely this adverse feedback loop for more than a year. A process of balance sheet deleveraging has spread to nearly every corner of the economy. Consumers are pulling back on purchases, especially on durable goods, to build their savings. Businesses are cancelling planned investments and laying off workers to preserve cash. And financial institutions are shrinking assets to bolster capital and improve their chances of weathering the current storm. Once again, Minsky understood this dynamic. He spoke of the paradox of deleveraging, in which precautions that may be smart for individuals and firms—and indeed essential to return the economy to a normal state—nevertheless magnify the distress of the economy as a whole.[349].The term financial innovation refers to the ongoing development of financial products designed to achieve particular client objectives, such as offsetting a particular risk exposure (such as the default of a borrower) or to assist with obtaining financing. Examples pertinent to this crisis included: the adjustable-rate mortgage; the bundling of subprime mortgages into mortgage-backed securities (MBS) or collateralized debt obligations (CDO) for sale to investors, a type of securitization; and a form of credit insurance called credit default swaps (CDS). The usage of these products expanded dramatically in the years leading up to the crisis. These products vary in complexity and the ease with which they can be valued on the books of financial institutions.[citation needed]
.CDO issuance grew from an estimated $20 billion in Q1 2004 to its peak of over $180 billion by Q1 2007, then declined back under $20 billion by Q1 2008. Further, the credit quality of CDO's declined from 2000 to 2007, as the level of subprime and other non-prime mortgage debt increased from 5% to 36% of CDO assets. As described in the section on subprime lending, the CDS and portfolio of CDS called synthetic CDO enabled a theoretically infinite amount to be wagered on the finite value of housing loans outstanding, provided that buyers and sellers of the derivatives could be found. For example, buying a CDS to insure a CDO ended up giving the seller the same risk as if they owned the CDO, when those CDO's became worthless.[356]
.This boom in innovative financial products went hand in hand with more complexity. It multiplied the number of actors connected to a single mortgage (including mortgage brokers, specialized originators, the securitizers and their due diligence firms, managing agents and trading desks, and finally investors, insurances and providers of repo funding). With increasing distance from the underlying asset these actors relied more and more on indirect information (including FICO scores on creditworthiness, appraisals and due diligence checks by third party organizations, and most importantly the computer models of rating agencies and risk management desks). Instead of spreading risk this provided the ground for fraudulent acts, misjudgments and finally market collapse.[357] Economists have studied the crisis as an instance of cascades in financial networks, where institutions' instability destabilized other institutions and led to knock-on effects.[358][359]
.Martin Wolf, chief economics commentator at the Financial Times, wrote in June 2009 that certain financial innovations enabled firms to circumvent regulations, such as off-balance sheet financing that affects the leverage or capital cushion reported by major banks, stating: "an enormous part of what banks did in the early part of this decade—the off-balance-sheet vehicles, the derivatives and the 'shadow banking system' itself—was to find a way round regulation."[360]
.Mortgage risks were underestimated by almost all institutions in the chain from originator to investor by underweighting the possibility of falling housing prices based on historical trends of the past 50 years. Limitations of default and prepayment models, the heart of pricing models, led to overvaluation of mortgage and asset-backed products and their derivatives by originators, securitizers, broker-dealers, rating-agencies, insurance underwriters and the vast majority of investors (with the exception of certain hedge funds).[361][362] While financial derivatives and structured products helped partition and shift risk between financial participants, it was the underestimation of falling housing prices and the resultant losses that led to aggregate risk.[362]
.For a variety of reasons, market participants did not accurately measure the risk inherent with financial innovation such as MBS and CDOs or understand its effect on the overall stability of the financial system.[249] The pricing model for CDOs clearly did not reflect the level of risk they introduced into the system. Banks estimated that $450 billion of CDO were sold between "late 2005 to the middle of 2007"; among the $102 billion of those that had been liquidated, JPMorgan estimated that the average recovery rate for "high quality" CDOs was approximately 32 cents on the dollar, while the recovery rate for mezzanine capital CDO was approximately five cents for every dollar.
.AIG insured obligations of various financial institutions through the usage of credit default swaps. The basic CDS transaction involved AIG receiving a premium in exchange for a promise to pay money to party A in the event party B defaulted. However, AIG did not have the financial strength to support its many CDS commitments as the crisis progressed and was taken over by the government in September 2008. U.S. taxpayers provided over $180 billion in government loans and investments in AIG during 2008 and early 2009, through which the money flowed to various counterparties to CDS transactions, including many large global financial institutions.[363][unreliable source?][364]
.The Financial Crisis Inquiry Commission (FCIC) made the major government study of the crisis. It concluded in January 2011:
.The Commission concludes AIG failed and was rescued by the government primarily because its enormous sales of credit default swaps were made without putting up the initial collateral, setting aside capital reserves, or hedging its exposure—a profound failure in corporate governance, particularly its risk management practices. AIG's failure was possible because of the sweeping deregulation of over-the-counter (OTC) derivatives, including credit default swaps, which effectively eliminated federal and state regulation of these products, including capital and margin requirements that would have lessened the likelihood of AIG's failure.[365][366][367].The limitations of a widely used financial model also were not properly understood.[368][369] This formula assumed that the price of CDS was correlated with and could predict the correct price of mortgage-backed securities. Because it was highly tractable, it rapidly came to be used by a huge percentage of CDO and CDS investors, issuers, and rating agencies.[369] According to one Wired article:
.Then the model fell apart. Cracks started appearing early on, when financial markets began behaving in ways that users of Li's formula hadn't expected. The cracks became full-fledged canyons in 2008—when ruptures in the financial system's foundation swallowed up trillions of dollars and put the survival of the global banking system in serious peril ... Li's Gaussian copula formula will go down in history as instrumental in causing the unfathomable losses that brought the world financial system to its knees.[369] .As financial assets became more complex and harder to value, investors were reassured by the fact that the international bond rating agencies and bank regulators accepted as valid some complex mathematical models that showed the risks were much smaller than they actually were.[370] George Soros commented that "The super-boom got out of hand when the new products became so complicated that the authorities could no longer calculate the risks and started relying on the risk management methods of the banks themselves. Similarly, the rating agencies relied on the information provided by the originators of synthetic products. It was a shocking abdication of responsibility."[371]
.A conflict of interest between investment management professional and institutional investors, combined with a global glut in investment capital, led to bad investments by asset managers in over-priced credit assets. Professional investment managers generally are compensated based on the volume of client assets under management. There is, therefore, an incentive for asset managers to expand their assets under management in order to maximize their compensation. As the glut in global investment capital caused the yields on credit assets to decline, asset managers were faced with the choice of either investing in assets where returns did not reflect true credit risk or returning funds to clients. Many asset managers continued to invest client funds in over-priced (under-yielding) investments, to the detriment of their clients, so they could maintain their assets under management. They supported this choice with a "plausible deniability" of the risks associated with subprime-based credit assets because the loss experience with early "vintages" of subprime loans was so low.[372]
.Despite the dominance of the above formula, there are documented attempts of the financial industry, occurring before the crisis, to address the formula limitations, specifically the lack of dependence dynamics and the poor representation of extreme events.[373] The volume Credit Correlation: Life After Copulas, published in 2007 by World Scientific, summarizes a 2006 conference held by Merrill Lynch in London where several practitioners attempted to propose models rectifying some of the copula limitations. See also the article by Donnelly and Embrechts[374] and the book by Brigo, Pallavicini and Torresetti, that reports relevant warnings and research on CDOs appeared in 2006.[375]
.There is strong evidence that the riskiest, worst performing mortgages were funded through the "shadow banking system" and that competition from the shadow banking system may have pressured more traditional institutions to lower their underwriting standards and originate riskier loans.
.In a June 2008 speech, President and CEO of the Federal Reserve Bank of New York Timothy Geithner—who in 2009 became United States Secretary of the Treasury—placed significant blame for the freezing of credit markets on a run on the entities in the "parallel" banking system, also called the shadow banking system. These entities became critical to the credit markets underpinning the financial system, but were not subject to the same regulatory controls. Further, these entities were vulnerable because of asset–liability mismatch, meaning that they borrowed short-term in liquid markets to purchase long-term, illiquid and risky assets. This meant that disruptions in credit markets would force them to engage in rapid deleveraging, selling their long-term assets at depressed prices. He described the significance of these entities:
.In early 2007, asset-backed commercial paper conduits, in structured investment vehicles, in auction-rate preferred securities, tender option bonds and variable rate demand notes, had a combined asset size of roughly $2.2 trillion. Assets financed overnight in tri-party repo grew to $2.5 trillion. Assets held in hedge funds grew to roughly $1.8 trillion. The combined balance sheets of the five largest investment banks totaled $4 trillion. In comparison, the total assets of the top five bank holding companies in the United States at that point were just over $6 trillion, and total assets of the entire banking system were about $10 trillion. The combined effect of these factors was a financial system vulnerable to self-reinforcing asset price and credit cycles.[376].Economist Paul Krugman, laureate of the Nobel Memorial Prize in Economic Sciences, described the run on the shadow banking system as the "core of what happened" to cause the crisis. He referred to this lack of controls as "malign neglect" and argued that regulation should have been imposed on all banking-like activity.[337] Without the ability to obtain investor funds in exchange for most types of mortgage-backed securities or asset-backed commercial paper, investment banks and other entities in the shadow banking system could not provide funds to mortgage firms and other corporations.[376][337]
.This meant that nearly one-third of the U.S. lending mechanism was frozen and continued to be frozen into June 2009.[377] According to the Brookings Institution, at that time the traditional banking system did not have the capital to close this gap: "It would take a number of years of strong profits to generate sufficient capital to support that additional lending volume." The authors also indicate that some forms of securitization were "likely to vanish forever, having been an artifact of excessively loose credit conditions". While traditional banks raised their lending standards, it was the collapse of the shadow banking system that was the primary cause of the reduction in funds available for borrowing.[29]
.In a 2008 paper, Ricardo J. Caballero, Emmanuel Farhi, and Pierre-Olivier Gourinchas argued that the financial crisis was attributable to "global asset scarcity, which led to large capital flows toward the United States and to the creation of asset bubbles that eventually burst".[378] Caballero, Farhi, and Gourinchas argued "that the sharp rise in oil prices following the subprime crisis – nearly 100 percent in just a matter of months and on the face of recessionary shocks – was the result of a speculative response to the financial crisis itself, in an attempt to rebuild asset supply. That is, the global economy was subject to one shock with multiple implications rather than to two separate shocks (financial and oil)."[378]
.Long-only commodity index funds became popular – by one estimate investment increased from $90 billion in 2006 to $200 billion at the end of 2007, while commodity prices increased 71% – which raised concern as to whether these index funds caused the commodity bubble. The empirical research has been mixed.[379]
.The cause of the global asset bubble can be partially attributable to the global savings glut. As theorized by Andrew Metrick, the demand for safe assets following the Asian Financial Crisis coupled with the lack of circulating treasuries created an unmet demand for "risk free" assets. Thus, institutional investors like sovereign wealth funds and pension funds began purchasing synthetic safe assets like Triple-A Mortgage Backed Securities.[380]
.As a consequence, the demand for so-called safe assets fueled the free flow of capital into housing in the United States. This greatly worsened the crisis as banks and other financial institutions were incentivized to issue more mortgages than before.
.In a 1998 book, John McMurtry suggested that a financial crisis is a systemic crisis of capitalism itself.[381]
.In his 1978 book, The Downfall of Capitalism and Communism, Ravi Batra suggests that growing inequality of financial capitalism produces speculative bubbles that burst and result in depression and major political changes. He also suggested that a "demand gap" related to differing wage and productivity growth explains deficit and debt dynamics important to stock market developments.[382]
.John Bellamy Foster, a political economy analyst and editor of the Monthly Review, believed that the decrease in GDP growth rates since the early 1970s is due to increasing market saturation.[383]
.Marxian economics followers Andrew Kliman, Michael Roberts, and Guglielmo Carchedi, in contradistinction to the Monthly Review school represented by Foster, pointed to capitalism's long-term tendency of the rate of profit to fall as the underlying cause of crises generally. From this point of view, the problem was the inability of capital to grow or accumulate at sufficient rates through productive investment alone. Low rates of profit in productive sectors led to speculative investment in riskier assets, where there was potential for greater return on investment. The speculative frenzy of the late 1990s and 2000s was, in this view, a consequence of a rising organic composition of capital, expressed through the fall in the rate of profit. According to Michael Roberts, the fall in the rate of profit "eventually triggered the credit crunch of 2007 when credit could no longer support profits".[384]
.In 2005 book, The Battle for the Soul of Capitalism, John C. Bogle wrote that "Corporate America went astray largely because the power of managers went virtually unchecked by our gatekeepers for far too long". Echoing the central thesis of James Burnham's 1941 seminal book, The Managerial Revolution, Bogle cites issues, including:[385]
.In his book The Big Mo, Mark Roeder, a former executive at the Swiss-based UBS Bank, suggested that large-scale momentum, or The Big Mo, "played a pivotal role" in the financial crisis. Roeder suggested that "recent technological advances, such as computer-driven trading programs, together with the increasingly interconnected nature of markets, has magnified the momentum effect. This has made the financial sector inherently unstable."[386]
.Robert Reich attributed the economic downturn to the stagnation of wages in the United States, particularly those of the hourly workers who comprise 80% of the workforce. This stagnation forced the population to borrow to meet the cost of living.[387]
.Economists Ailsa McKay and Margunn Bjørnholt argued that the financial crisis and the response to it revealed a crisis of ideas in mainstream economics and within the economics profession, and call for a reshaping of both the economy, economic theory and the economics profession.[388]
.A report by the International Labour Organization concluded that cooperative banking institutions were less likely to fail than their competitors during the crisis. The cooperative banking sector had 20% market share of the European banking sector, but accounted for only 7% of all the write-downs and losses between the third quarter of 2007 and first quarter of 2011.[389] In 2008, in the U.S., the rate of commercial bank failures was almost triple that of credit unions, and almost five times the credit union rate in 2010.[390] Credit unions increased their lending to small- and medium-sized businesses while overall lending to those businesses decreased.[391]
.Economists, particularly followers of mainstream economics, mostly failed to predict the crisis.[392] The Wharton School of the University of Pennsylvania's online business journal examined why economists failed to predict a major global financial crisis and concluded that economists used mathematical models that failed to account for the critical roles that banks and other financial institutions, as opposed to producers and consumers of goods and services, play in the economy.[393]
.Several followers of heterodox economics predicted the crisis, with varying arguments. Dirk Bezemer[394] credits 12 economists with predicting the crisis: Dean Baker (US), Wynne Godley (UK), Fred Harrison (UK), Michael Hudson (US), Eric Janszen (US), Steve Keen (Australia), Jakob Broechner Madsen & Jens Kjaer Sørensen (Denmark), Med Jones (US)[395] Kurt Richebächer (US), Nouriel Roubini (US), Peter Schiff (US), and Robert Shiller (US).
.Shiller, a founder of the Case–Shiller index that measures home prices, wrote an article a year before the collapse of Lehman Brothers in which he predicted that a slowing U.S. housing market would cause the housing bubble to burst, leading to financial collapse.[396] Peter Schiff regularly appeared on television in the years before the crisis and warned of the impending real estate collapse.[397]
.The Austrian School regarded the crisis as a vindication and classic example of a predictable credit-fueled bubble caused by laxity in monetary supply.[398]
.There were other economists that did warn of a pending crisis.[399]
.The former Governor of the Reserve Bank of India, Raghuram Rajan, had predicted the crisis in 2005 when he became chief economist at the International Monetary Fund. In 2005, at a celebration honoring Alan Greenspan, who was about to retire as chairman of the US Federal Reserve, Rajan delivered a controversial paper that was critical of the financial sector.[400] In that paper, Rajan "argued that disaster might loom".[401] Rajan argued that financial sector managers were encouraged to "take risks that generate severe adverse consequences with small probability but, in return, offer generous compensation the rest of the time. These risks are known as tail risks. But perhaps the most important concern is whether banks will be able to provide liquidity to financial markets so that if the tail risk does materialize, financial positions can be unwound and losses allocated so that the consequences to the real economy are minimized."
.Stock trader and financial risk engineer Nassim Nicholas Taleb, author of the 2007 book The Black Swan, spent years warning against the breakdown of the banking system in particular and the economy in general owing to their use of and reliance on bad risk models and reliance on forecasting, and framed the problem as part of "robustness and fragility".[402][403] He also took action against the establishment view by making a big financial bet on banking stocks and making a fortune from the crisis ("They didn't listen, so I took their money").[404] According to David Brooks from The New York Times, "Taleb not only has an explanation for what's happening, he saw it coming."[405]
.Popular articles published in the mass media have led the general public to believe that the majority of economists have failed in their obligation to predict the financial crisis. For example, an article in The New York Times noted that economist Nouriel Roubini warned of such crisis as early as September 2006, and stated that the profession of economics is bad at predicting recessions.[406] According to The Guardian, Roubini was ridiculed for predicting a collapse of the housing market and worldwide recession, while The New York Times labelled him "Dr. Doom".[407]
.In a 2012 article in the journal Japan and the World Economy, Andrew K. Rose and Mark M. Spiegel used a Multiple Indicator Multiple Cause (MIMIC) model on a cross-section of 107 countries to evaluate potential causes of the 2008 crisis. The authors examined various economic indicators, ignoring contagion effects across countries. The authors concluded: "We include over sixty potential causes of the crisis, covering such categories as: financial system policies and conditions; asset price appreciation in real estate and equity markets; international imbalances and foreign reserve adequacy; macroeconomic policies; and institutional and geographic features. Despite the fact that we use a wide number of possible causes in a flexible statistical framework, we are unable to link most of the commonly cited causes of the crisis to its incidence across countries. This negative finding in the cross-section makes us skeptical of the accuracy of 'early warning' systems of potential crises, which must also predict their timing."[408]
.The first visible institution to run into trouble in the United States was the Southern California–based IndyMac, a spin-off of Countrywide Financial. Before its failure, IndyMac Bank was the largest savings and loan association in the Los Angeles market and the seventh largest mortgage loan originator in the United States.[409] The failure of IndyMac Bank on July 11, 2008, was the fourth largest bank failure in United States history up until the crisis precipitated even larger failures,[410] and the second largest failure of a regulated thrift.[411] IndyMac Bank's parent corporation was IndyMac Bancorp until the FDIC seized IndyMac Bank.[412] IndyMac Bancorp filed for Chapter 7 bankruptcy in July 2008.[412]
.IndyMac Bank was founded as Countrywide Mortgage Investment in 1985 by David S. Loeb and Angelo Mozilo[413][414] as a means of collateralizing Countrywide Financial loans too big to be sold to Freddie Mac and Fannie Mae. In 1997, Countrywide spun off IndyMac as an independent company run by Mike Perry, who remained its CEO until the downfall of the bank in July 2008.[415]
.The primary causes of its failure were largely associated with its business strategy of originating and securitizing Alt-A loans on a large scale. This strategy resulted in rapid growth and a high concentration of risky assets. From its inception as a savings association in 2000, IndyMac grew to the seventh largest savings and loan and ninth largest originator of mortgage loans in the United States. During 2006, IndyMac originated over $90 billion (~$131 billion in 2023) of mortgages.
.IndyMac's aggressive growth strategy, use of Alt-A and other nontraditional loan products, insufficient underwriting, credit concentrations in residential real estate in the California and Florida markets—states, alongside Nevada and Arizona, where the housing bubble was most pronounced—and heavy reliance on costly funds borrowed from a Federal Home Loan Bank (FHLB) and from brokered deposits, led to its demise when the mortgage market declined in 2007.
.IndyMac often made loans without verification of the borrower's income or assets, and to borrowers with poor credit histories. Appraisals obtained by IndyMac on underlying collateral were often questionable as well. As an Alt-A lender, IndyMac's business model was to offer loan products to fit the borrower's needs, using an extensive array of risky option-adjustable-rate mortgages (option ARMs), subprime loans, 80/20 loans, and other nontraditional products. Ultimately, loans were made to many borrowers who simply could not afford to make their payments. The thrift remained profitable only as long as it was able to sell those loans in the secondary mortgage market. IndyMac resisted efforts to regulate its involvement in those loans or tighten their issuing criteria: see the comment by Ruthann Melbourne, Chief Risk Officer, to the regulating agencies.[416][417][418]
.On May 12, 2008, in the "Capital" section of its last 10-Q, IndyMac revealed that it may not be well capitalized in the future.[419]
.IndyMac reported that during April 2008, Moody's and Standard & Poor's downgraded the ratings on a significant number of Mortgage-backed security (MBS) bonds—including $160 million (~$222 million in 2023) issued by IndyMac that the bank retained in its MBS portfolio. IndyMac concluded that these downgrades would have harmed its risk-based capital ratio as of June 30, 2008. Had these lowered ratings been in effect on March 31, 2008, IndyMac concluded that the bank's capital ratio would have been 9.27% total risk-based. IndyMac warned that if its regulators found its capital position to have fallen below "well capitalized" (minimum 10% risk-based capital ratio) to "adequately capitalized" (8–10% risk-based capital ratio) the bank might no longer be able to use brokered deposits as a source of funds.
.Senator Charles Schumer (D-NY) later pointed out that brokered deposits made up more than 37% of IndyMac's total deposits, and asked the Federal Deposit Insurance Corporation (FDIC) whether it had considered ordering IndyMac to reduce its reliance on these deposits.[420] With $18.9 billion in total deposits reported on March 31,[419] Senator Schumer would have been referring to a little over $7 billion in brokered deposits. While the breakout of maturities of these deposits is not known exactly, a simple averaging would have put the threat of brokered deposits loss to IndyMac at $500 million a month, had the regulator disallowed IndyMac from acquiring new brokered deposits on June 30.
.IndyMac was taking new measures to preserve capital, such as deferring interest payments on some preferred securities. Dividends on common shares had already been suspended for the first quarter of 2008, after being cut in half the previous quarter. The company still had not secured a significant capital infusion nor found a ready buyer.[421]
.IndyMac reported that the bank's risk-based capital was only $47 million above the minimum required for this 10% mark. But it did not reveal some of that $47 million (~$65.3 million in 2023) capital it claimed it had, as of March 31, 2008, was fabricated.[422]
.When home prices declined in the latter half of 2007 and the secondary mortgage market collapsed, IndyMac was forced to hold $10.7 billion (~$15.2 billion in 2023) of loans it could not sell in the secondary market. Its reduced liquidity was further exacerbated in late June 2008 when account holders withdrew $1.55 billion (~$2.15 billion in 2023) or about 7.5% of IndyMac's deposits.[419] This bank run on the thrift followed the public release of a letter from Senator Charles Schumer to the FDIC and OTS. The letter outlined the Senator's concerns with IndyMac. While the run was a contributing factor in the timing of IndyMac's demise, the underlying cause of the failure was the unsafe and unsound way it was operated.[416]
.On June 26, 2008, Senator Charles Schumer (D-NY), a member of the Senate Banking Committee, chairman of Congress' Joint Economic Committee and the third-ranking Democrat in the Senate, released several letters he had sent to regulators, in which he was"concerned that IndyMac's financial deterioration poses significant risks to both taxpayers and borrowers." Some worried depositors began to withdraw money.[423][424]
.On July 7, 2008, IndyMac announced on the company blog that it:
.IndyMac announced the closure of both its retail lending and wholesale divisions, halted new loan submissions, and cut 3,800 jobs.[425]
.On July 11, 2008, citing liquidity concerns, the FDIC put IndyMac Bank into conservatorship. A bridge bank, IndyMac Federal Bank, FSB, was established to assume control of IndyMac Bank's assets, its secured liabilities, and its insured deposit accounts. The FDIC announced plans to open IndyMac Federal Bank, FSB on July 14, 2008. Until then, depositors would have access to their insured deposits through ATMs, their existing checks, and their existing debit cards. Telephone and Internet account access was restored when the bank reopened.[121][426][427] The FDIC guarantees the funds of all insured accounts up to US$100,000, and declared a special advance dividend to the roughly 10,000 depositors with funds in excess of the insured amount, guaranteeing 50% of any amounts in excess of $100,000.[411] Yet, even with the pending sale of Indymac to IMB Management Holdings, an estimated 10,000 uninsured depositors of Indymac are still at a loss of over $270 million.[428][429]
.With $32 billion in assets, IndyMac Bank was one of the largest bank failures in American history.[430]
.IndyMac Bancorp filed for Chapter 7 bankruptcy on July 31, 2008.[412]
.Initially the companies affected were those directly involved in home construction and mortgage lending such as Northern Rock and Countrywide Financial, as they could no longer obtain financing through the credit markets. Over 100 mortgage lenders went bankrupt during 2007 and 2008. Concerns that investment bank Bear Stearns would collapse in March 2008 resulted in its fire-sale to JP Morgan Chase. The financial institution crisis hit its peak in September and October 2008. Several major institutions either failed, were acquired under duress, or were subject to government takeover. These included Lehman Brothers, Merrill Lynch, Fannie Mae, Freddie Mac, Washington Mutual, Wachovia, Citigroup, and AIG.[32] On October 6, 2008, three weeks after Lehman Brothers filed the largest bankruptcy in U.S. history, Lehman's former CEO Richard S. Fuld Jr. found himself before Representative Henry A. Waxman, the California Democrat who chaired the House Committee on Oversight and Government Reform. Fuld said he was a victim of the collapse, blaming a "crisis of confidence" in the markets for dooming his firm.[431]
.
.The initial articles and some subsequent material were adapted from the Wikinfo article Financial crisis of 2007–2008 released under the GNU Free Documentation License Version 1.2
.Reports on causes
.Journalism and interviews
.
.Kenya,[a] officially the Republic of Kenya,[b] is a country located in East Africa. With an estimated population of more than 52.4 million as of mid-2024,[13] Kenya is the 27th-most-populous country in the world[7] and the 7th most populous in Africa. Kenya's capital and largest city is Nairobi. Its second-largest and oldest city is Mombasa, a major port city located on Mombasa Island. Other major cities within the country include Kisumu, Nakuru & Eldoret. Going clockwise, Kenya is bordered by South Sudan to the northwest (though much of that border includes the disputed Ilemi Triangle), Ethiopia to the north, Somalia to the east, the Indian Ocean to the southeast, Tanzania to the southwest, and Lake Victoria and Uganda to the west.
.Kenya's geography, climate and population vary widely. In western, rift valley counties, the landscape includes cold, snow-capped mountaintops (such as Batian, Nelion and Point Lenana on Mount Kenya) with vast surrounding forests, wildlife and fertile agricultural regions to temperate climates. In other areas, there are dry, arid and semi-arid climates as well as absolute deserts (such as Chalbi Desert and Nyiri Desert). 
.Kenya's earliest inhabitants included some of the first humans to evolve from ancestral members of the genus Homo. Ample fossil evidence for this evolutionary history has been found at Koobi Fora. Later, Kenya was inhabited by hunter-gatherers similar to the present-day Hadza people.[14][15] According to archaeological dating of associated artifacts and skeletal material, Cushitic speakers first settled in the region's lowlands between 3,200 and 1,300 BC, a phase known as the Lowland Savanna Pastoral Neolithic. Nilotic-speaking pastoralists (ancestral to Kenya's Nilotic speakers) began migrating from present-day South Sudan into Kenya around 500 BC.[16] Bantu people settled at the coast and the interior between 250 BC and 500 AD.[17]
.European contact began in 1500 AD with the Portuguese Empire, and effective colonisation of Kenya began in the 19th century during the European exploration of Africa. Modern-day Kenya emerged from a protectorate established by the British Empire in 1895 and the subsequent Kenya Colony, which began in 1920. Mombasa was the capital of the British East Africa Protectorate,[18] which included most of what is now Kenya and southwestern Somalia, from 1889 to 1907. Numerous disputes between the UK and the colony led to the Mau Mau revolution, which began in 1952, and the declaration of Kenya's independence in 1963. After independence, Kenya remained a member of the Commonwealth of Nations. The country's current constitution was adopted in 2010, replacing the previous 1963 constitution.
.Kenya is a presidential representative democratic republic, in which elected officials represent the people and the president is the head of state and government.[19] The country is a member of the United Nations, the Commonwealth, World Bank, International Monetary Fund, World Trade Organization, COMESA, International Criminal Court, as well as several other international organisations. It is also a major non-NATO ally of the United States.
.Kenya's economy is the largest in East and Central Africa, with Nairobi serving as a major regional commercial hub.[20] With a Gross National Income of $2,110, [21] the country is a lower-middle-income economy. Agriculture is the country's largest economic sector; tea and coffee are the sector's traditional cash crops, while fresh flowers are a fast-growing export. The service industry, particularly tourism, is also one of the country's major economic drivers. Kenya is a member of the East African Community trade bloc,[22][23] though some international trade organisations categorise it as part of the Greater Horn of Africa.[24] Africa is Kenya's largest export market, followed by the European Union.[25]
.The Republic of Kenya is named after Mount Kenya. The earliest recorded version of the modern name was written by German explorer Johann Ludwig Krapf in the 19th century. While travelling with a Kamba caravan led by the long-distance trader Chief Kivoi, Krapf spotted the mountain peak and asked what it was called. Kivoi told him "Kĩ-Nyaa" or "Kĩlĩma- Kĩinyaa", probably because the pattern of black rock and white snow on its peaks reminded him of the feathers of the male ostrich.[26] In archaic Kikuyu, the word 'nyaga' or more commonly 'manyaganyaga' is used to describe an extremely bright object. The Agikuyu, who inhabit the slopes of Mt. Kenya, call it Kĩrĩma Kĩrĩnyaga (literally 'the mountain with brightness') in Kikuyu, while the Embu call it "Kirinyaa". All three names have the same meaning.[27]
.Ludwig Krapf recorded the name as both Kenia and Kegnia.[28][29][30] Some have said that this was a precise notation of the African pronunciation /ˈkɛnjə/.[31] An 1882 map drawn by Joseph Thompsons, a Scottish geologist and naturalist, indicated Mt. Kenya as Mt. Kenia.[26] The mountain's name was accepted, pars pro toto, as the name of the country. It did not come into widespread official use during the early colonial period, when the country was referred to as the East African Protectorate. The official name was changed to the Colony of Kenya in 1920.
.Hominid species, such as Homo habilis (fl. 1.8 to 2.5 million years ago) and Homo erectus (fl. 1.9 million to 350,000 years ago), possibly the direct ancestors of modern Homo sapiens, had lived in Kenya in the Pleistocene epoch.[32] East Africa, including Kenya, is one of the earliest regions where modern humans (Homo sapiens) are believed to have lived. In 1984, during excavations at Lake Turkana palaeoanthropologist Richard Leakey, assisted by Kamoya Kimeu, had discovered the Turkana Boy, a 1.6-million-year-old Homo erectus fossil. Further evidence of Kenya's prehistory was found in 2018, namely the early emergence of modern behaviours, including long-distance trade networks (involving goods such as obsidian), the use of pigments, and possibly the making of projectile points, about 320,000 years ago. The authors of three 2018 studies on the site suggest that complex and modern behaviours had already begun in Africa around the time of the emergence of Homo sapiens.[33][34][35]
.The first inhabitants of present-day Kenya were hunter-gatherer groups, akin to the modern Khoisan speakers.[36] These people were later largely replaced by agropastoralist Cushitic (ancestral to Kenya's Cushitic speakers), who originated from the Horn of Africa.[37] During the early Holocene, the region's climate shifted from drier to wetter conditions. This provided an opportunity for the development of cultural traditions such as agriculture and herding in a more favourable environment.[36]
.Around 500 BC, Nilotic-speaking pastoralists (ancestral to Kenya's present-day Nilotic speakers) started migrating from what is now southern Sudan into Kenya.[16][38][39] Today, the country's Nilotic ethnic groups include the Kalenjin, Samburu, Luo, Turkana, and Maasai.[40]
.By the first millennium AD, Bantu-speaking farmers had moved into the region, initially along the Kenyan coast.[41] The Bantus had originated in West Africa along the Benue River in what is now eastern Nigeria and western Cameroon.[42] The Bantu migration brought new developments in agriculture and ironworking to the region.[42] Today, the country's Bantu groups include the Kikuyu, Luhya, Kamba, Kisii, Meru, Kuria, Aembu, Ambeere, Wadawida-Watuweta, Wapokomo, and Mijikenda, among many others.
.Notable prehistoric sites in the interior of Kenya include the (possibly archaeoastronomical) site Namoratunga on the west side of Lake Turkana and the walled settlement of Thimlich Ohinga in Migori County.
.The coastline of Kenya was home to communities of ironworkers and Bantu subsistence farmers, hunters, and fishers who supported the region's economy with agriculture, fishing, metal production, and trade with foreign countries. These communities formed the earliest city-states in the region, which were collectively known as Azania.[43] The Swahili people were of mixed African and Asian (particularly Persian) ancestry, as DNA evidence has revealed.[44]
.By the 1st century CE, many of the area's city-states, such as Mombasa, Malindi, and Zanzibar, began to establish trading relations with the Arabs. This led to increased economic growth of the Swahili states, the introduction of Islam, Arabic influences on the Swahili language, cultural diffusion, as well as the Swahili city-states becoming members of a larger trade network.[45][46] Many historians had long believed that the city-states were established by Arab or Persian traders, but archaeological evidence has led scholars to recognise the city-states as an indigenous development which, though subjected to foreign influence due to trade, retained a Bantu cultural core.[47]
.The Kilwa Sultanate was a medieval sultanate centred at Kilwa, in modern-day Tanzania. At its height, its authority stretched over the entire length of the Swahili Coast, including Kenya.[48] Beginning in the 10th century, the rulers of Kilwa would go on to build elaborate coral mosques and introduce copper coinage.[49]
.Swahili, a Bantu language with Arabic, Persian, and other Middle-Eastern and South Asian loanwords, later developed as a lingua franca for trade between the different peoples.[43] Since the turn of the 20th century, Swahili has also adopted numerous loanwords and calques from English, many of which originated during the era of British colonial rule.[50]
.The Swahili built Mombasa into a major port city and established trade links with other nearby city-states, as well as commercial centres in Persia, Arabia, and even India.[51] By the 15th century, Portuguese voyager Duarte Barbosa wrote that "Mombasa is a place of great traffic and has a good harbour in which there are always moored small craft of many kinds and also great ships, both of which are bound from Sofala and others which come from Cambay and Melinde and others which sail to the island of Zanzibar."[52]
.One major city on the Kenyan coast is Malindi. It has been an important Swahili settlement since the 14th century, and the city once rivalled Mombasa for dominance in the African Great Lakes region. Malindi has traditionally been a friendly port city for foreign powers. In 1414, the Chinese trader and explorer Zheng He, representing the Ming Dynasty, visited the East African coast on one of his last 'treasure voyages'.[53] Malindi also welcomed the Portuguese explorer Vasco da Gama in 1498.
.In the 17th century, the Swahili coast was conquered by the Omani Arabs, who expanded the slave trade to meet the demands of plantations in Oman and Zanzibar.[54] Initially, these traders came mainly from Oman, but later many came from Zanzibar (such as Tippu Tip).[55] In addition, the Portuguese started buying slaves from the Omani and Zanzibari traders in response to the interruption of the transatlantic slave trade by British abolitionists.
.During the 18th and 19th century, the Masai people moved into the central and southern Rift Valley plains of Kenya, from a region north of Lake Rudolf (now Lake Turkana). Although there weren't many of them, they managed to conquer a great amount of land in the plains, where people did not put up much resistance.[citation needed] The Nandi peoples managed to oppose the Masai, while the Taveta peoples fled to the forests on the eastern edge of Mount Kilimanjaro, although they later were forced to leave the land due to the threat of smallpox. An outbreak of either rinderpest or pleuropneumonia greatly affected the Masai's cattle, while an epidemic of smallpox affected the Masai themselves. After the death of the Masai Mbatian, the chief laibon (medicine man), the Masai split into warring factions. The Masai caused much strife in the areas they conquered; however, cooperation between such groups as the Luo people, Luhya people, and Gusii people is evidenced by shared vocabulary for modern implements and similar economic regimes.[56] Although Arab traders remained in the area, trade routes were disrupted by the hostile Masai, though there was trade in ivory between these factions.[57]
.The first foreigners to successfully get past the Masai were Johann Ludwig Krapf and Johannes Rebmann, two German missionaries who established a mission in Rabai, not too far from Mombasa. The pair were the first Europeans to sight Mount Kenya.[58]
.The German Empire established a protectorate over the Sultan of Zanzibar's coastal possessions in 1885, followed by the arrival of the Imperial British East Africa Company in 1888. Imperial rivalry was prevented by the Heligoland–Zanzibar Treaty, so Germany handed its East African coastal holdings to Britain in 1890.
.The transfer by Germany to Britain was followed by the building of the Uganda Railway passing through the country.[59]
.The building of the railway was resisted by some ethnic groups—notably the Nandi, led by Orkoiyot Koitalel Arap Samoei from 1890 to 1900—but the British eventually built it. The Nandi were the first ethnic group to be put in a native reserve to stop them from disrupting the building of the railway.[59]
.During the railway construction era, there was a significant influx of Indian workers, who provided the bulk of the skilled labour required for construction.[60] They and most of their descendants later remained in Kenya and formed the core of several distinct Indian communities, such as the Ismaili Muslim and Sikh communities. While building the railway through Tsavo, a number of the Indian railway workers and local African labourers were attacked by two lions known as the Tsavo maneaters.[61]
.At the outbreak of World War I in August 1914, the governors of British East Africa (as the protectorate was generally known) and German East Africa initially agreed on a truce in an attempt to keep the young colonies out of direct hostilities. But Lieutenant Colonel Paul von Lettow-Vorbeck, the German military commander, determined to tie down as many British resources as possible. Completely cut off from Germany, Lettow-Vorbeck conducted an effective guerrilla warfare campaign, living off the land, capturing British supplies, and remaining undefeated. He eventually surrendered in Northern Rhodesia (today Zambia) 14 days after the Armistice was signed in 1918.[60]
.To chase von Lettow, the British deployed the British Indian Army troops from India but needed large numbers of porters to overcome the formidable logistics of transporting supplies far into the interior on foot. The Carrier Corps was formed and ultimately mobilised over 400,000 Africans, contributing to their long-term politicisation.[60]
.In 1920, the East Africa Protectorate was turned into a colony and renamed Kenya after its highest mountain.[59]
.During the early part of the 20th century, the interior central highlands were settled by British and other European farmers, who became wealthy farming coffee and tea.[62] One depiction of this period of change from a colonist's perspective is found in the memoir Out of Africa by Danish author Baroness Karen von Blixen-Finecke, published in 1937. By the 1930s, approximately 30,000 white settlers lived in the area and gained a political voice because of their contribution to the market economy.[60]
.The central highlands were already home to over a million members of the Kikuyu people, most of whom had no land claims in European terms and lived as itinerant farmers. To protect their interests, the settlers banned the growing of coffee and introduced a hut tax, and the landless were granted less and less land in exchange for their labour. A massive exodus to the cities ensued as their ability to make a living from the land dwindled.[60] By the 1950s, there were 80,000 white settlers living in Kenya.[63]
.Throughout World War II, Kenya was an important source of manpower and agriculture for the United Kingdom. Kenya itself was the site of fighting between Allied forces and Italian troops in 1940–41, when Italian forces invaded. Wajir and Malindi were bombed as well.
.From October 1952 to December 1959, Kenya was in a state of emergency arising from the Mau Mau rebellion against British rule. The Mau Mau, also known as the Kenya Land and Freedom Army, were primarily Kikuyu people. During the colonial administration's crackdown, over 11,000 freedom fighters had been killed, along with 100 British troops and 2,000 Kenyan loyalist soldiers. War crimes were committed on both sides of the conflict, including the publicised Lari massacre and the Hola massacre. The governor requested and obtained British and African troops, including the King's African Rifles. The British began counter-insurgency operations. In May 1953, General Sir George Erskine took charge as commander-in-chief of the colony's armed forces, with the personal backing of Winston Churchill.[64]
.The capture of Waruhiu Itote (nom de guerre "General China") on 15 January 1954 and the subsequent interrogation led to a better understanding of the Mau Mau command structure for the British. Operation Anvil opened on 24 April 1954, after weeks of planning by the army with the approval of the War Council. The operation effectively placed Nairobi under military siege. Nairobi's occupants were screened and suspected Mau Mau supporters moved to detention camps. More than 80,000 Kikuyu were held in detention camps without trial, often subject to brutal treatment.[65] The Home Guard formed the core of the government's strategy as it was composed of loyalist Africans, not foreign forces such as the British Army and King's African Rifles.[66]
.The capture of Dedan Kimathi on 21 October 1956 in Nyeri signified the ultimate defeat of the Mau Mau and essentially ended the military offensive.[64] During this period, substantial governmental changes to land tenure occurred. The most important of these was the Swynnerton Plan, which was used to both reward loyalists and punish Mau Mau. This left roughly 1/3rd of Kikuyu bereft of any tenancy land arrangement and thus propertyless at the time of independence.[67]
.Before Kenya got its independence, Somali ethnic people in present-day Kenya in the areas of Northern Frontier Districts petitioned Her Majesty's Government not to be included in Kenya. The colonial government decided to hold Kenya's first referendum in 1962 to check the willingness of Somalis in Kenya to join Somalia.[68]
.The result of the referendum showed that 86% of Somalis in Kenya wanted to join Somalia, yet the British colonial administration rejected the result and the Somalis remained in Kenya.[69][70]
.The first direct elections for native Kenyans to the Legislative Council took place in 1957.
.Despite British hopes of handing power to "moderate" local rivals, it was the Kenya African National Union (KANU) of Jomo Kenyatta that formed a government. The Colony of Kenya and the Protectorate of Kenya each came to an end on 12 December 1963, with independence conferred on all of Kenya. The U.K. ceded sovereignty over the Colony of Kenya. The Sultan of Zanzibar agreed that simultaneous with independence for the colony, he would cease to have sovereignty over the Protectorate of Kenya so that all of Kenya would become one sovereign state.[71][72] In this way, Kenya became an independent country under the Kenya Independence Act 1963 of the United Kingdom. On 12 December 1964, Kenya became a republic under the name "Republic of Kenya".[71]
.Concurrently, the Kenyan army fought the Shifta War against ethnic Somali rebels inhabiting the Northern Frontier District who wanted to join their kin in the Somali Republic to the north.[73] A ceasefire was eventually reached with the signing of the Arusha Memorandum in October 1967, but relative insecurity prevailed through 1969.[74][75] To discourage further invasions, Kenya signed a defence pact with Ethiopia in 1969, which is still in effect.[76][77]
.On 12 December 1964, the Republic of Kenya was proclaimed, and Jomo Kenyatta became Kenya's first president.[78] Under Kenyatta, corruption became widespread throughout the government, civil service, and business community. Kenyatta and his family were tied up with this corruption as they enriched themselves through the mass purchase of property after 1963. Their acquisitions in the Central, Rift Valley, and Coast Provinces aroused great anger among landless Kenyans. His family used his presidential position to circumvent legal or administrative obstacles to acquiring property. The Kenyatta family also heavily invested in the coastal hotel business, with Kenyatta personally owning the Leonard Beach Hotel.[79]
.Kenyatta's mixed legacy was highlighted at the 10-year anniversary of Kenya's independence. A December 1973 article in The New York Times praised Kenyatta's leadership and Kenya for emerging as a model of pragmatism and conservatism. Kenya's GDP had increased at an annual rate of 6.6%, higher than the population growth rate of more than 3%.[80] But Amnesty International responded to the article by stating the cost of the stability in terms of human rights abuses. The opposition party started by Oginga Odinga—Kenya People's Union (KPU)—was banned in 1969 after the Kisumu Massacre and KPU leaders were still in detention without trial in gross violation of the U.N. Declaration of Human Rights.[81][82] The Kenya Students Union, Jehovah Witnesses and all opposition parties were outlawed.[81] Kenyatta ruled until his death on 22 August 1978.[83]
.After Kenyatta died, Daniel arap Moi became president. He retained the presidency, running unopposed in elections held in 1979, 1983 (snap elections), and 1988, all of which were held under the single-party constitution. The 1983 elections were held a year early, and were a direct result of a failed military coup on 2 August 1982.
.The 1982 coup was masterminded by a low-ranking Air Force serviceman, Senior Private Hezekiah Ochuka, and was staged mainly by enlisted men of the Air Force. It was quickly suppressed by forces commanded by Chief of General Staff Mahamoud Mohamed, a veteran Somali military official.[84] They included the General Service Unit (GSU)—a paramilitary wing of the police—and later the regular police.
.On the heels of the Garissa Massacre of 1980, Kenyan troops committed the Wagalla massacre in 1984 against thousands of civilians in Wajir County. An official probe into the atrocities was later ordered in 2011.[85][clarification needed]
.The election held in 1988 saw the advent of the mlolongo (queuing) system, where voters were supposed to line up behind their favoured candidates instead of casting a secret ballot.[86] This was seen as the climax of a very undemocratic regime and led to widespread agitation for constitutional reform. Several contentious clauses, including the one that allowed for only one political party, were changed in the following years.[87]
.In 1991, Kenya transitioned to a multiparty political system after 26 years of single-party rule. On 28 October 1992, Moi dissolved parliament, five months before the end of his term. As a result, preparations began for all elective seats in parliament as well as the president. The election was scheduled to take place on 7 December 1992, but delays led to its postponement to 29 December. Apart from KANU, the ruling party, other parties represented in the elections included FORD Kenya and FORD Asili. This election was marked by large-scale intimidation of opponents and harassment of election officials. It resulted in an economic crisis propagated by ethnic violence as the president was accused of rigging electoral results to retain power.[88][89][90] This election was a turning point for Kenya as it signified the beginning of the end of Moi's leadership and the rule of KANU. Moi retained the presidency and George Saitoti became vice president. Although it held on to power, KANU won 100 seats and lost 88 seats to the six opposition parties.[88][90]
.The 1992 elections marked the beginning of multiparty politics after more than 25 years of KANU rule.[88] Following skirmishes in the aftermath of the elections, 5,000 people were killed and another 75,000 displaced from their homes.[91] In the next five years, many political alliances were formed in preparation for the next elections. In 1994, Jaramogi Oginga Odinga died and several coalitions joined his FORD Kenya party to form a new party, United National Democratic Alliance. This party was plagued with disagreements. In 1995, Richard Leakey formed the Safina party, but it was denied registration until November 1997.[92]
.In 1996, KANU revised the constitution to allow Moi to remain president for another term. Subsequently, Moi stood for reelection and won a 5th term in 1997.[93] His win was strongly criticised by his major opponents, Kibaki and Odinga, as fraudulent.[92][94] Following this win, Moi was constitutionally barred from another presidential term. Beginning in 1998, he attempted to influence the country's succession politics to have Uhuru Kenyatta elected in the 2002 elections.[95]
.Moi's plan to be replaced by Uhuru Kenyatta failed, and Mwai Kibaki, running for the opposition coalition "National Rainbow Coalition" (NARC), was elected president. David Anderson (2003) reports the elections were judged free and fair by local and international observers, and seemed to mark a turning point in Kenya's democratic evolution.[94]
.In 2005, Kenyans rejected a plan to replace the 1963 independence constitution with a new one.[96] As a result, the elections of 2007 took place following the procedure set by the old constitution. Kibaki was reelected in highly contested elections marred by political and ethnic violence. The main opposition leader, Raila Odinga, claimed the election was rigged and that he was the rightfully elected president. In the ensuing violence, 1,500 people were killed and another 600,000 internally displaced, making it the worst post-election violence in Kenya. To stop the death and displacement of people, Kibaki and Odinga agreed to work together, with the latter taking the position of a prime minister.[97] This made Odinga the second prime minister of Kenya.
.In July 2010, Kenya partnered with other East African countries to form the new East African Common Market within the East African Community.[98] In 2011, Kenya began sending troops to Somalia to fight the terror group Al-Shabaab.[99] In mid-2011, two consecutive missed rainy seasons precipitated the worst drought in East Africa in 60 years. The northwestern Turkana region was especially affected,[100] with local schools shut down as a result.[101] The crisis was reportedly over by early 2012 because of coordinated relief efforts. Aid agencies subsequently shifted their emphasis to recovery initiatives, including digging irrigation canals and distributing plant seeds.[102]
.In August 2010, Kenyans held a referendum and passed a new constitution, which limited presidential powers and devolved the central government.[92] Following the passage of the new constitution, Kenya became a presidential representative democratic republic, whereby the President of Kenya is both head of state and head of government, and of a multi-party system. The new constitution also states that executive powers are exercised by the executive branch of government, headed by the president, who chairs a cabinet composed of people chosen from outside parliament. Legislative power is vested exclusively in Parliament. The judiciary is independent of the executive and the legislature.
.In 2013, Kenya held its first general elections under the 2010 constitution. Uhuru Kenyatta won, though the result was unsuccessfully challenged at the Supreme Court by opposition leader Raila Odinga.[103] Kenyatta was re-elected in 2017 in another disputed election. Odinga accused the Independent Electoral and Boundaries Commission of mismanaging the election and Kenyatta and his party of rigging. The Supreme Court overturned the results in a landmark ruling that established its position as an independent check on the president.[104][105] In the repeated election, Kenyatta won after Odinga refused to participate, citing irregularities.[106][107]
.In March 2018, a historic handshake between Kenyatta and Odinga signaled a period of reconciliation followed by economic growth and increased stability.[108][109] Between 2019 and 2021, the two combined efforts to promote major changes to the Kenyan constitution, labelled the "Building Bridges Initiative" (BBI). They said their efforts were to improve inclusion and overcome the country's election system that often resulted in post-election violence.[110][111] The BBI proposal called for broad expansion of the legislative and executive branches, including the creation of a prime minister with two deputies and an official leader of the opposition, reverting to selecting cabinet ministers from among the elected Members of Parliament, establishing up to 70 new constituencies, and adding up to 300 unelected members of Parliament (under an "affirmative action" plan).[110][111]
.Critics saw this as an attempt to reward political dynasties and blunt Deputy President Willian Ruto's efforts to seek the presidency and bloat the government at a high cost to the debt-laded country.[110][111] Ultimately, in May 2021, the Kenyan High Court ruled the BBI unconstitutional, because it was an effort of the government rather than a popular initiative.[110][111] The court sharply criticized Kenyatta, laying out grounds for his being sued or even impeached. The ruling was seen as a major defeat for both Kenyatta and Odinga, but a boon to Odinga's future presidential-election rival, Ruto.[110][111] In August 2021, Kenya's Court of Appeal upheld the High Court's judgement in an appeal from the BBI Secretariat .[112]
.Ruto narrowly won the 2022 presidential election, defeating Odinga to become Kenya's fifth president.[113][114] In 2024, Ruto and the Kenya Kwanza coalition faced popular protests over the Kenyan Finance Bill 2024.[115]
.At 580,367 km2 (224,081 sq mi),[11] Kenya is the world's 47th-largest country (after Madagascar). It lies between latitudes 5°N and 5°S, and longitudes 34° and 42°E. From the coast on the Indian Ocean, the low plains rise to central highlands which are bisected by the Great Rift Valley, and fertile plateaus lie on either side, around Lake Victoria and to the east.[116][117]
.The Kenyan Highlands are one of the most successful agricultural production regions in Africa.[118] The highlands are the site of the highest point in Kenya and the second highest peak on the continent: Mount Kenya, which reaches a height of 5,199 m (17,057 ft) and is the site of glaciers. Mount Kilimanjaro (5,895 m or 19,341 ft) can be seen from Kenya to the south of the Tanzanian border.
.Kenya's climate varies from tropical along the coast to temperate inland to arid in the north and northeast parts of the country. The area receives a great deal of sunshine every month. It is usually cool at night and early in the morning inland at higher elevations.
.The "long rains" season occurs from March/April to May/June. The "short rains" season occurs from October to November/December. The rainfall is sometimes heavy and often falls in the afternoons and evenings. Climate change is altering the natural pattern of the rainfall period, causing an extension of the short rains, which has begat floods,[119] and reducing the drought cycle from every ten years to annual events, producing strong droughts such as the 2008–09 Kenya Drought.[120]
.The temperature remains high throughout these months of tropical rain. The hottest period is February and March, leading into the season of the long rains, and the coldest is in July, until mid-August.[121]
.Climate change is posing an increasing threat to global socioeconomic development and environmental sustainability.[125] Developing countries with low adaptive capacity and high vulnerability to the phenomenon are disproportionately affected. Climate change in Kenya is increasingly impacting the lives of Kenya's citizens and the environment.[125] Climate change has led to more frequent extreme weather events like droughts which last longer than usual, irregular and unpredictable rainfall, flooding and increasing temperatures.
.Kenya has considerable land area devoted to wildlife habitats, including the Masai Mara, where blue wildebeest and other bovids participate in a large-scale annual migration. More than one million wildebeest and 200,000 zebras participate in the migration across the Mara River.[129]
.The "Big Five" game animals of Africa, that is the lion, leopard, buffalo, rhinoceros, and elephant, can be found in Kenya and in the Masai Mara in particular. A significant population of other wild animals, reptiles, and birds can be found in the national parks and game reserves in the country. The annual animal migration occurs between June and September, with millions of animals taking part, attracting valuable foreign tourism. Two million wildebeest migrate a distance of 2,900 kilometres (1,802 mi) from the Serengeti in neighbouring Tanzania to the Masai Mara[130] in Kenya, in a constant clockwise fashion, searching for food and water supplies. This Serengeti Migration of the wildebeest is listed among the Seven Natural Wonders of Africa.[131]
.Kenya had a 2019 Forest Landscape Integrity Index mean score of 4.2/10, ranking it 133rd globally out of 172 countries.[132]
.Kenya is a presidential representative democratic republic with a multi-party system. The president is both the head of state and head of government. Executive power is exercised by the government. Legislative power is vested in both the government and the National Assembly and the Senate. The Judiciary is independent of the executive and the legislature. There has been growing concern, especially during former president Daniel arap Moi's tenure, that the executive was increasingly meddling with the affairs of the judiciary.[133]
.Kenya has high levels of corruption according to Transparency International's Corruption Perceptions Index (CPI), a metric which attempts to gauge the prevalence of public-sector corruption in various countries. In 2019, the nation placed 137th out of 180 countries in the index, with a score of 28 out of 100.[134] But there are several rather significant developments with regard to curbing corruption from the Kenyan government, for instance the establishment of a new and independent Ethics and Anti-Corruption Commission (EACC).[135]
.Following general elections held in 1997, the Constitution of Kenya Review Act, designed to pave the way for more comprehensive amendments to the Kenyan constitution, was passed by the national parliament.[136]
.In December 2002, Kenya held democratic and open elections, which were judged free and fair by most international observers.[137] The 2002 elections marked an important turning point in Kenya's democratic evolution in that power was transferred peacefully from the Kenya African National Union (KANU), which had ruled the country since independence, to the National Rainbow Coalition (NARC), a coalition of political parties.
.Under the presidency of Mwai Kibaki, the new ruling coalition promised to focus its efforts on generating economic growth, combating corruption, improving education, and rewriting its constitution. A few of these promises have been met. There is free primary education.[138] In 2007, the government issued a statement declaring that from 2008, secondary education would be heavily subsidised, with the government footing all tuition fees.[139]
.Under the new constitution and with President Kibaki prohibited by term limits from running for a third term, Deputy Prime Minister Uhuru Kenyatta ran for office. He won with 50.51% of the vote in March 2013.
.In December 2014, President Kenyatta signed a Security Laws Amendment Bill, which supporters of the law suggested was necessary to guard against armed groups. Opposition politicians, human rights groups, and nine Western countries criticised the security bill, arguing that it infringed on democratic freedoms. The governments of the United States, the United Kingdom, Germany, and France also collectively issued a press statement cautioning about the law's potential impact. Through the Jubilee Coalition, the Bill was later passed on 19 December in the National Assembly under acrimonious circumstances.[140]
.Kenya has close ties with its fellow Swahili-speaking neighbours in the African Great Lakes region. Relations with Uganda and Tanzania are generally strong, as the three nations work toward economic and social integration through common membership in the East African Community.
.Relations with Somalia have historically been tense, although there has been some military co-ordination against Islamist insurgents. Kenya has good relations with the United Kingdom.[141] Kenya is one of the most pro-American nations in Africa, and the wider world.[142]
.With International Criminal Court trial dates scheduled in 2013 for both President Kenyatta and Deputy President William Ruto related to the 2007 election aftermath, US president Barack Obama, who is half-Kenyan, chose not to visit the country during his mid-2013 African trip.[143] Later in the summer, Kenyatta visited China at the invitation of President Xi Jinping after a stop in Russia and not having visited the United States as president.[144] In July 2015, Obama visited Kenya, the first American president to visit the country while in office.[145]
.The British Army Training Unit Kenya (BATUK) is used for the training of British infantry battalions in the arid and rugged terrain of the Great Rift Valley.[146][147]
.The Kenya Defence Forces are the armed forces of Kenya. The Kenya Army, Kenya Navy, and Kenya Air Force compose the National Defence Forces. The current Kenya Defence Forces were established, and its composition laid out, in Article 241 of the 2010 Constitution of Kenya; the KDF is governed by the Kenya Defence Forces Act of 2012.[148] The President of Kenya is the commander-in-chief of all the armed forces.
.The armed forces are regularly deployed in peacekeeping missions around the world. Further, in the aftermath of the national elections of December 2007 and the violence that subsequently engulfed the country, a commission of inquiry, the Waki Commission, commended its readiness and adjudged it to "have performed its duty well."[149] Nevertheless, there have been serious allegations of human rights violations, most recently while conducting counter-insurgency operations in the Mt Elgon area[150] and also in the district of Mandera central.[151]
.Kenya's armed forces, like many government institutions in the country, have been tainted by corruption allegations. Because the operations of the armed forces have been traditionally cloaked by the ubiquitous blanket of "state security", the corruption has been hidden from public view, and thus less subject to public scrutiny and notoriety. This has changed recently. In what are by Kenyan standards unprecedented revelations, in 2010, credible claims of corruption were made with regard to recruitment[152] and procurement of armoured personnel carriers.[153] Further, the wisdom and prudence of certain decisions of procurement have been publicly questioned.[154]
.Kenya is divided into 47 semi-autonomous counties that are headed by governors. These 47 counties form the first-order divisions of Kenya.
.The smallest administrative units in Kenya are called locations. Locations often coincide with electoral wards. Locations are usually named after their central villages/towns. Many larger towns consist of several locations. Each location has a chief, appointed by the state.
.Constituencies are an electoral subdivision, with each county comprising a whole number of constituencies. An interim boundaries commission was formed in 2010 to review the constituencies and in its report, it recommended the creation of an additional 80 constituencies. Previous to the 2013 elections, there were 210 constituencies in Kenya.[155]
.Homosexual acts are illegal in Kenya and typically punishable by up to 14 years in prison.[156] According to a 2020 survey by the Pew Research Center, 83% of Kenyans believe that homosexuality should not be accepted by society.[157] While addressing a joint press conference together with President Barack Obama in 2015, President Kenyatta declined to assure Kenya's commitment to gay rights, saying that "the issue of gay rights is really a non-issue... But there are some things that we must admit we don't share. Our culture, our societies don't accept."[158]
.In November 2008, WikiLeaks brought wide international attention to The Cry of Blood report, which documents the extrajudicial killing of gangsters by the Kenyan police. In the report, the Kenya National Commission on Human Rights (KNCHR) reported these in their key finding "e)", stating that the forced disappearances and extrajudicial killings appeared to be official policy sanctioned by the political leadership and the police.[159][160]
.Kenya's macroeconomic outlook has steadily posted robust growth over the past few decades mostly from road, rail, air and water transport infrastructure projects as well as massive investments in Information and Communication Technology. The Kenyan economy is the largest in East Africa. After independence, Kenya promoted rapid economic growth through public investment, encouraged smallholder agricultural production and provided incentives for private industrial investment. Kenya is East Africa's regional transportation and financial hub. Kenya's financial sector is vibrant, well developed and diversified boasting the highest financial inclusion in the region and globally.[161]
.Foreign investments in Kenya remain relatively weak considering the size of its economy and its level of development. As of 2022, Kenya's total FDI stock stood at US$10.4 billion, accounting for a mere 9.5% of the country's GDP.[162]
.Kenya has a Human Development Index (HDI) of 0.628 (medium), ranked 143 out of 193 in the world as of 2023[update].  As of 2022, 25.4% of Kenyans lived on less than $2.15 a day.[163] Based on the most recent data from 2022, 25.4% of the population is affected by multidimensional poverty and an additional 26.4% vulnerable to it.[164][163] In 2017, Kenya ranked 92nd in the World Bank ease of doing business rising from 113rd in 2016 (of 190 countries).[165] The important agricultural sector employs around 30% of the workforce.[166]  Kenya is usually classified as a frontier market or occasionally an emerging market, and is one of the Developing countries.
.The economy has seen much expansion, seen by strong performance in tourism, higher education and telecommunications and decent post-drought results in agriculture, especially the vital tea sector.[167] Kenya's economy grew by more than 7% in 2007, and its foreign debt was greatly reduced.[167] This changed immediately after the disputed presidential election of December 2007, following the chaos which engulfed the country.
.Telecommunications and financial activity over the last decade now comprise 62% of GDP. 22% of GDP still comes from the unreliable agricultural sector which employs 75% of the labour force (a characteristic of under-developed economies that have not attained food security). A small portion of the population relies on food aid.[168] Industry and manufacturing is the smallest sector, accounting for 16% of GDP.  The services, industry and manufacturing sectors only employ 25% of the labour force but contribute 75% of GDP.[167]
Kenya also exports textiles worth over $400 million under AGOA.
.Privatisation of state corporations like the defunct Kenya Post and Telecommunications Company, which resulted in East Africa's most profitable company—Safaricom, has led to their revival because of massive private investment.
.As of May 2011[update], economic prospects are positive with 4–5% GDP growth expected, largely because of expansions in tourism, telecommunications, transport, construction, and a recovery in agriculture. The World Bank estimated growth of 4.3% in 2012.[169]
.In March 1996 the presidents of Kenya, Tanzania, and Uganda re-established the East African Community (EAC). The EAC's objectives include harmonising tariffs and customs regimes, free movement of people and improving regional infrastructures. In March 2004, the three East African countries signed a Customs Union Agreement.
.Kenya has a more developed financial services sector than its neighbours. The Nairobi Securities Exchange (NSE) is ranked 4th in Africa in terms of market capitalisation. The Kenyan banking system is supervised by the Central Bank of Kenya (CBK). As of late July 2004, the system consisted of 43 commercial banks (down from 48 in 2001) and several non-bank financial institutions including mortgage companies, four savings and loan associations and several core foreign-exchange bureaus.[167]
.The inaugural Kenya Innovation Week (KIW) was started in 2021, from December 6th to 10th, 2021, at the Kenya School of Government in Lower Kabete, Nairobi.[170][171]
.Tourism contributes around 6% to Kenya’s economy.[172]Tourism in Kenya is the third-largest source of foreign exchange revenue following diaspora remittances and agriculture.[173]  The Kenya Tourism Board is responsible for maintaining information pertaining to tourism in Kenya.[174][175]
The main tourist attractions are photo safaris through the 60 national parks and game reserves. Other attractions include the wildebeest migration at the Masaai Mara, which is considered to be the 7th wonder of the world; historical mosques, and colonial-era forts at Mombasa, Malindi, and Lamu; renowned scenery such as the white-capped Mount Kenya and the Great Rift Valley; tea plantations at Kericho; coffee plantations at Thika; a splendid view of Mount Kilimanjaro across the border into Tanzania;  and the beaches along the Swahili Coast, in the Indian Ocean. Tourists, the largest number being from Germany and the United Kingdom, are attracted mainly to the coastal beaches and the game reserves, notably, the expansive East and Tsavo West National Park, 20,808 square kilometres (8,034 sq mi) to the southeast.[citation needed]
.Agriculture is the second largest contributor to Kenya's gross domestic product (GDP) after the service sector. In 2005, agriculture, including forestry and fishing, accounted for 24% of GDP, as well as for 18% of wage employment and 50% of revenue from exports. The principal cash crops are tea, horticultural produce, and coffee. Horticultural produce and tea are the main growth sectors and the two most valuable of all of Kenya's exports. The production of major food staples such as corn is subject to sharp weather-related fluctuations. Production downturns periodically necessitate food aid—for example in 2004, due to one of Kenya's intermittent droughts.[176]
.A consortium led by the International Crops Research Institute for the Semi-Arid Tropics (ICRISAT) has had some success in helping farmers grow new pigeon pea varieties instead of maize, in particularly dry areas. Pigeon peas are very drought-resistant, so can be grown in areas with less than 650 millimetres (26 in) annual rainfall. Successive projects encouraged the commercialisation of legumes by stimulating the growth of local seed production and agro-dealer networks for distribution and marketing. This work, which included linking producers to wholesalers, helped to increase local producer prices by 20–25% in Nairobi and Mombasa. The commercialisation of the pigeon pea is now enabling some farmers to buy assets ranging from mobile phones to productive land and livestock, and is opening pathways for them to move out of poverty.[177]
.Tea, coffee, sisal, pyrethrum, corn, and wheat are grown in the fertile highlands, one of the most successful agricultural production regions in Africa.[118] Livestock predominates in the semi-arid savanna to the north and east. Coconuts, pineapples, cashew nuts, cotton, sugarcane, sisal, and corn are grown in the lower-lying areas. Kenya has not attained the level of investment and efficiency in agriculture that can guarantee food security, and coupled with resulting poverty (53% of the population lives below the poverty line), a significant portion of the population regularly starves and is heavily dependent on food aid.[168] Poor roads, an inadequate railway network, under-used water transport, and expensive air transport have isolated mostly arid and semi-arid areas, and farmers in other regions often leave food to rot in the fields because they cannot access markets. This was last seen in August and September 2011, prompting the Kenyans for Kenya initiative by the Red Cross.[178]
.Kenya's irrigation sector is categorised into three organizational types: smallholder schemes, centrally-managed public schemes, and private/commercial irrigation schemes.
.The smallholder schemes are owned, developed, and managed by individuals or groups of farmers operating as water users or self-help groups. Irrigation is carried out on individual or on group farms averaging 0.1–0.4 ha. There are about 3,000 smallholder irrigation schemes covering a total area of 47,000 ha.
The country has seven large, centrally managed irrigation schemes, namely Mwea, Bura, Hola, Perkera, West Kano, Bunyala, and Ahero, covering a total area of 18,200 ha and averaging 2,600 ha per scheme. These schemes are managed by the National Irrigation Board and account for 18% of irrigated land area in Kenya.
Large-scale private commercial farms cover 45,000 hectares, accounting for 40% of irrigated land. They utilise high technology and produce high-value crops for the export market, especially flowers and vegetables.[179]
.Kenya is the world's 3rd largest exporter of cut flowers.[180] Roughly half of Kenya's 127 flower farms are concentrated around Lake Naivasha, 90 kilometres northwest of Nairobi.[180] To speed their export, Nairobi airport has a terminal dedicated to the transport of flowers and vegetables.[180]
.Although Kenya is a low middle-income country, manufacturing accounts for 14% of the GDP, with industrial activity concentrated around the three largest urban centres of Nairobi, Mombasa, and Kisumu, and is dominated by food-processing industries such as grain milling, beer production, sugarcane crushing, and the fabrication of consumer goods, e.g., vehicles from kits.
.Kenya also has a cement production industry.[181] Kenya has an oil refinery that processes imported crude petroleum into petroleum products, mainly for the domestic market. In addition, a substantial and expanding informal sector commonly referred to as jua kali engages in small-scale manufacturing of household goods, auto parts, and farm implements.[182][183]
.Kenya's inclusion among the beneficiaries of the US Government's African Growth and Opportunity Act (AGOA) has given a boost to manufacturing in recent years. Since AGOA took effect in 2000, Kenya's clothing sales to the United States increased from US$44 million to US$270 million (2006).[184] Other initiatives to strengthen manufacturing have been the new government's favourable tax measures, including the removal of duty on capital equipment and other raw materials.[185]
.In 2023, Kenya is in the process of constructing five industrial parks that will operate tax-free, with an anticipated completion date set for 2030. There are intentions to develop an additional 20 industrial parks in the future.[186]
.Two trans-African automobile routes pass through Kenya: the Cairo-Cape Town Highway and the Lagos-Mombasa Highway, so the country has an extensive road network of paved and unpaved roads. Kenya's railway system links the nation's ports and major cities, connecting it with neighbouring Uganda. There are 15 airports which have paved runways.
.The largest share of Kenya's electricity supply comes from geothermal energy,[187] followed by hydroelectric stations at dams along the upper Tana River, as well as the Turkwel Gorge Dam in the west. A petroleum-fired plant on the coast, geothermal facilities at Olkaria (near Nairobi), and electricity imported from Uganda make up the rest of the supply. A 2,000 MW powerline from Ethiopia was completed in 2022.
.Kenya's installed capacity increased from 1,142 megawatts between 2001 and 2003 to 2,341 in 2016.[188] The state-owned Kenya Electricity Generating Company (KenGen), established in 1997 under the name of Kenya Power Company, handles the generation of electricity, while Kenya Power handles the electricity transmission and distribution system in the country. Shortfalls of electricity occur periodically, when drought reduces water flow. To become energy sufficient, Kenya has installed wind power and solar power (over 300 MW each), and aims to build a nuclear power plant by 2027.[189][190]
.Kenya has proven deposits of oil in Turkana. Tullow Oil estimates the country's oil reserves to be around one billion barrels.[191] Exploration is still continuing to determine whether there are more reserves. Kenya currently imports all crude petroleum requirements. It has no strategic reserves and relies solely on oil marketers' 21-day oil reserves required under industry regulations. Petroleum accounts for 20% to 25% of the national import bill.[192]
.Published comments on Kenya's Capital FM website by Liu Guangyuan, China's ambassador to Kenya, at the time of President Kenyatta's 2013 trip to Beijing, said, "Chinese investment in Kenya ... reached $474 million, representing Kenya's largest source of foreign direct investment, and ... bilateral trade ... reached $2.84 billion" in 2012. Kenyatta was "[a]ccompanied by 60 Kenyan business people [and hoped to] ... gain support from China for a planned $2.5 billion railway from the southern Kenyan port of Mombasa to neighbouring Uganda, as well as a nearly $1.8 billion dam", according to a statement from the president's office, also at the time of the trip.[144]
.Base Titanium, a subsidiary of Base resources of Australia, shipped its first major consignment of minerals to China. About 25,000 tonnes of ilmenite was flagged off the Kenyan coastal town of Kilifi. The first shipment was expected to earn Kenya about KSh.15–20 billion/= in earnings.[193] In 2014, the Chinese contracted railway project from Nairobi to Mombasa was suspended due to a dispute over compensation for land acquisition.[194]
.In 2007, the Kenyan government unveiled Vision 2030, an economic development programme it hopes will put the country in the same league as the Asian Economic Tigers by 2030. In 2013, it launched a National Climate Change Action Plan, having acknowledged that omitting climate as a key development issue in Vision 2030 was an oversight failure. The 200-page Action Plan, developed with support from the Climate & Development Knowledge Network, sets out the Government of Kenya's vision for a 'low-carbon climate resilient development pathway'. At the launch in March 2013, the Secretary of the Ministry of Planning, National Development, and Vision 2030 emphasized that climate would be a central issue in the renewed Medium-Term Plan that would be launched in the coming months. This would create a direct and robust delivery framework for the Action Plan and ensure climate change is treated as an economy-wide issue.[195] Furthermore, Kenya submitted an updated, more ambitious NDC on 24 December 2020, with a commitment to abate greenhouse gases by 32 percent by 2030 relative to the business-as-usual scenario and in line with its sustainable development agenda and national circumstances.[196]
.There exists an informal economy that is never counted as part of the official GDP figures.
.Kenya has proven oil deposits in Turkana County. President Mwai Kibaki announced on 26 March 2012 that Tullow Oil, an Anglo-Irish oil exploration firm, had struck oil, but its commercial viability and subsequent production would take about three years to confirm.[197]
.Early in 2006, Chinese president Hu Jintao signed an oil exploration contract with Kenya, part of a series of deals designed to keep Africa's natural resources flowing to China's rapidly expanding economy.
.The deal allowed for China's state-controlled offshore oil and gas company, CNOOC, to prospect for oil in Kenya, which is just beginning to drill its first exploratory wells on the borders of Sudan and the disputed area of North Eastern Province, on the border with Somalia and in coastal waters. There are formal estimates of the possible reserves of oil discovered.[198]
.In 2017, Kenya banned single-use plastic bags. According to the national environmental authority, 80% of the public has adhered to this ban. Subsequently, in 2020, the prohibition of single-use plastics was extended to protected areas, including parks and forests.[199]
.A law passed in July 2023 mandates companies to actively reduce the pollution and environmental impact caused by the products they introduce into the Kenyan market, either individually or through collective schemes. Unlike previous practices, businesses are now obligated to participate in waste collection and recycling initiatives, such the Petco initiative established by the government in 2018.[199]
.Kenya has a labour force of around 24 million and a total labour force participation rate of 74%, the unemployment rate in 2022 was estimated at 5.6%[200]
.According to the World Bank's 2019 Human Capital Index (HCI), which measured human capital of the next generation, Kenya ranked first in sub-Saharan Africa with an HCI score of 0.52. The index combined several key indicators, which are school enrolment, child survival, quality of learning, healthy growth and adult survival into a single index ranging between 0–1.[201]
.More than 20 institutions offer business loans on a large scale, specific agriculture loans, education loans, and loans for other purposes. Additionally, there are:
.Out of approximately 40 million Kenyans, about 14 million are unable to receive financial service through formal loan application services, and an additional 12 million have no access to financial service institutions at all. Further, one million Kenyans are reliant on informal groups for receiving financial aid.[202]
.To mitigate this problem, the mobile banking service M-Pesa was launched in 2007 by Vodafone and Safaricom, in collaboration from the Financial Deepening Challenge Fund competition established by the UK government's Department for International Development. M-Pesa allows users to deposit, withdraw, transfer money, pay for goods and services (Lipa na M-Pesa), access credit and savings, all with a mobile device,[203] has provided access to digital transactions to millions of Kenyans in poverty situation.[204]
.The census carried out by the Kenya National Bureau of Statistics in 2019 reported Kenya's population as 47,564,296.[7] The country has a young population, with 73% of residents under 30 because of rapid population growth,[207][208] from 2.9 million to 40 million inhabitants over the last century.[209]
.Nairobi is home to Kibera, one of the world's largest slums. The shantytown is believed to house between 170,000[210] and one million people.[211] The UNHCR base in Dadaab in the north houses around 500,000.[212]
.Kenya has a diverse population that includes many of Africa's major ethnoracial and linguistic groups. Although there is no official list of Kenyan ethnic groups, the number of ethnic categories and sub-categories recorded in the country's census has changed significantly over time, expanding from 42 in 1969 to more than 120 in 2019.[213] Most residents are Bantus (60%) or Nilotes (30%).[214] Cushitic groups also form a small ethnic minority, as do Arabs, Indians, and Europeans.[214][215]
.According to the Kenya National Bureau of Statistics (KNBS), in 2019, the largest native ethnic groups were the Kikuyu (8,148,668), Luhya (6,823,842), Kalenjin (6,358,113), Luo (5,066,966), Kamba (4,663,910), Somali (2,780,502), Kisii (2,703,235), Mijikenda (2,488,691), Meru (1,975,869), Maasai (1,189,522), and Turkana (1,016,174). The North Eastern Province of Kenya, formerly known as NFD, is predominantly inhabited by the indigenous ethnic Somalis. Foreign-rooted populations include Arabs, Asians, and Europeans.[2]
.Kenya's ethnic groups typically speak their mother tongues within their own communities. The two official languages, English and Swahili, are used in varying degrees of fluency for communication with other populations. English is widely spoken in commerce, schooling, and government.[216] Peri-urban and rural dwellers are less multilingual, with many in rural areas speaking only their native languages.[217]
.British English is primarily used in Kenya. Additionally, a distinct local dialect, Kenyan English, is used by some communities and individuals in the country, and contains features unique to it that were derived from local Bantu languages such as Kiswahili and Kikuyu.[218] It has been developing since colonisation and also contains certain elements of American English. Sheng is a Kiswahili-based cant spoken in some urban areas. Primarily a mixture of Swahili and English, it is an example of linguistic code-switching.[219]
.69 languages are spoken in Kenya. Most belong to two broad language families: Niger-Congo (Bantu branch) and Nilo-Saharan (Nilotic branch), spoken by the country's Bantu and Nilotic populations respectively. The Cushitic and Arab ethnic minorities speak languages belonging to the separate Afroasiatic family, with the Indian and European residents speaking languages from the Indo-European family.[220]
.Most Kenyans are Christian (85.5%), with 33.4% Protestant, 20.6% Roman Catholic and 20.4% Evangelical. Roman Catholicism is the largest single Christian denomination in Kenya with about 10 million members.[2] The Presbyterian Church of East Africa has 3 million followers in Kenya and surrounding countries.[222] There are smaller conservative Reformed churches, the Africa Evangelical Presbyterian Church,[223] the Independent Presbyterian Church in Kenya, and the Reformed Church of East Africa. Orthodox Christianity has 621,200 adherents.[224] Kenya has by far the highest number of Quakers of any country in the world, with around 146,300.[225] The only Jewish synagogue in the country is in Nairobi.
.Islam is the second largest religion, comprising 11% of the population.[3] 60% of Kenyan Muslims live in the Coastal Region, comprising 50% of the total population there, while the upper part of Kenya's Eastern Region is home to 10% of the country's Muslims, where they are the majority religious group.[226] Indigenous beliefs are practised by 0.7% of the population, although many self-identifying Christians and Muslims maintain some traditional beliefs and customs. Nonreligious Kenyans are 1.6% of the population.[2]
.Some Hindus also live in Kenya. The numbers are estimated to be around 60,287, or 0.13% of the population.[2]
.Health care is one of the low-priority sectors in Kenya and was allocated 4.8% of the national budget in 2019/2020 or just 4.59% of GDP compared to high-priority sectors such as education which was allocated more than 25%. This is below the 4.98% average in Sub-Saharan Africa and 9.83% spent globally.
.According to the National and County Health Budget Analysis FY 2020/21, the breakdown of county health expenditure was 58% on Policy Planning and Administrative Support Services, 28% on Curative and Rehabilitative Health Services, 8% on Preventive and Promotive Health Services and 7% on Other Programmes.
.Health care is largely funded by private individuals and their families or employers through direct payments to health care providers, to the National Health Insurance Fund or to medical insurance companies. Additional funding comes from local, international and some government social safety net schemes. Public hospitals are fee-for-service establishments that generate large amounts of county and national government revenues making them highly political and corrupt enterprises.[228]
.Private health facilities are diverse, highly dynamic, and difficult to classify, unlike public health facilities, which are easily grouped in classes that consist of community-based (level I) services, run by community health workers; dispensaries (level II facilities) run by nurses; health centres (level III facilities), run by clinical officers; sub-county hospitals (level IV facilities), which may be run by a clinical officer or a medical officer; county hospitals (level V facilities), which may be run by a medical officer or a medical practitioner; and national referral hospitals (level VI facilities), which are run by fully qualified medical practitioners.
.Nurses are by far the largest group of front-line health care providers in all sectors, followed by clinical officers, medical officers, and medical practitioners. These are absorbed and deployed into government service in accordance with the Scheme of Service for Nursing Personnel (2014), the Revised Scheme of Service for Clinical Personnel (2020) and the Revised Scheme of Service for Medical Officers and Dental Officers (2016).
.Traditional healers (herbalists, witch doctors, and faith healers) are readily available, trusted, and widely consulted as practitioners of first or last choice by both rural and urban dwellers.
.Despite major achievements in the health sector, Kenya still faces many challenges. The estimated life expectancy dropped in 2009 to approximately 55 years — five years below the 1990 level.[229] The infant mortality rate was high at approximately 44 deaths per 1,000 children in 2012.[230] The WHO estimated in 2011 that only 42% of births were attended by a skilled health professional.[231]
.Diseases of poverty directly correlate with a country's economic performance and wealth distribution: In 2015/16, 35.6% of Kenyans lived below the poverty line.[232] Preventable diseases like malaria, HIV/AIDS, pneumonia, diarrhoea, and malnutrition are the biggest burden, major child-killers, and responsible for much morbidity; weak policies, corruption, inadequate health workers, weak management, and poor leadership in the public health sector are largely to blame. According to 2009 estimates, HIV/AIDS prevalence is about 6.3% of the adult population.[233] However, the 2011 UNAIDS Report suggests that the HIV epidemic may be improving in Kenya, as HIV prevalence is declining among young people (ages 15–24) and pregnant women.[234] Kenya had an estimated 15 million cases of malaria in 2006.[235] Tuberculosis is a major public health problem. The per capita incidence of TB in Kenya more than quadrupled between 1990 and 2015.[236]
.The 2024 Global Hunger Index gave Kenya a 25.0 score, indicating that the severity of hunger is "serious".[237]
.The total fertility rate in Kenya was estimated to be 4.49 children per woman in 2012.[238] According to a 2008–09 survey by the Kenyan government, the total fertility rate was 4.6% and the contraception usage rate among married women was 46%.[239] Maternal mortality is high, partly because of female genital mutilation,[167] with about 27% of women having undergone it.[240]
This practice is however on the decline as the country becomes more modernised, and in 2011 it was banned in Kenya.[241]
Women were economically empowered before colonialisation.
By colonial land alienation, women lost access and control of land.[242] They became more economically dependent on men.[242] A colonial order of gender emerged where males dominated females.[242]
Median age at first marriage increases with increasing education.[243]
Rape, defilement, and battering are not always seen as serious crimes.[244]
Reports of sexual assault are not always taken seriously.[244]
.Article 260 of the Kenyan Constitution of 2010 defines youth as those between the ages of 18 and 34.[245] According to the 2019 Population and Census results, 75 percent of the 47.6 million population is under the age of 35, making Kenya a country of the youth.[246] Youth unemployment and underemployment in Kenya has become a problem.[247] According to the Kenya National Bureau of Statistics (KNBS), approximately 1.7 million people lost their jobs as a result of the COVID-19 pandemic, which eliminated some informal jobs and caused the economy to slow.[247] The Kenyan government has made progress in addressing the high youth unemployment by implementing various affirmative action programs and projects which include: the National Youth Service, The National Youth Enterprise Development Fund,[248] The Women Enterprise Fund,[249] Kazi Mtaani, Ajira Digital, Kikao Mtaani,[250] Uwezo fund,[251] Future Bora[252] and Studio mashinani[253] that empower youth, offer job opportunities and to raise one's standard of living.
.Children attend nursery school, or kindergarten in the private sector until they are five years old. This lasts one to three years (KG1, KG2 and KG3) and is financed privately because there has been no government policy on pre-schooling until recently.[254]
.Basic formal education starts at age six and lasts 12 years, consisting of eight years in primary school and four in high school or secondary. Primary school is free in public schools and those attending can join a vocational youth/village polytechnic, or make their own arrangements for an apprenticeship program and learn a trade such as tailoring, carpentry, motor vehicle repair, brick-laying and masonry for about two years.[255]
.Those who complete high school can join a polytechnic or other technical college and study for three years, or proceed directly to university and study for four years. Graduates from the polytechnics and colleges can then join the workforce and later obtain a specialised higher diploma qualification after a further one to two years of training, or join the university—usually in the second or third year of their respective course. The higher diploma is accepted by many employers in place of a bachelor's degree and direct or accelerated admission to post-graduate studies is possible in some universities.
.Public universities in Kenya are highly commercialised institutions and only a small fraction of qualified high school graduates are admitted on limited government-sponsorship into programs of their choice. Most are admitted into the social sciences, which are cheap to run, or as self-sponsored students paying the full cost of their studies. Most qualified students who miss out opt for middle-level diploma programs in public or private universities, colleges, and polytechnics.
.In 2018, 18.5 percent of the Kenyan adult population was illiterate, which was the highest rate of literacy in East Africa.[256][257] There are very wide regional disparities: for example, Nairobi had the highest level of literacy at 87.1 per cent, compared to North Eastern Province, the lowest, at 8.0 per cent. Preschool, which targets children from age three to five, is an integral component of the education system and is a key requirement for admission to Standard One (First Grade). At the end of primary education, pupils sit the Kenya Certificate of Primary Education (KCPE), which determines those who proceed to secondary school or vocational training. The result of this examination is needed for placement at secondary school.[255]
.Primary school is for students aged 6/7-13/14 years. For those who proceed to the secondary level, there is a national examination at the end of Form Four – the Kenya Certificate of Secondary Education (KCSE), which determines those proceeding to the universities, other professional training, or employment. Students sit examinations in eight subjects of their choosing. However, English, Kiswahili, and mathematics are compulsory subjects.
.The Kenya Universities and Colleges Central Placement Service (KUCCPS), formerly the Joint Admissions Board (JAB), is responsible for selecting students joining the public universities. Other than the public schools, there are many private schools, mainly in urban areas. Similarly, there are a number of international schools catering to various overseas educational systems.
.Kenya was ranked 96th in the Global Innovation Index in 2024.[258]
.The culture of Kenya comprises multiple traditions. Kenya has no single prominent culture. It instead consists of the various cultures of the country's different communities.
.Notable populations include the Swahili on the coast, several other Bantu communities in the central and western regions, and Nilotic communities in the northwest. The Maasai culture is well known to tourism, despite constituting a relatively small part of Kenya's population. They are renowned for their elaborate upper-body adornment and jewellery.
.Kenya has an extensive music, television, and theatre scene.
.Ngũgĩ wa Thiong'o is one of Kenya's best-known writers. His novel Weep Not, Child depicts life in Kenya during the British occupation. The story details the effects of the Mau Mau on the lives of Kenyans. Its combination of themes—colonialism, education, and love—helped make it one of the best-known African novels.
.M.G. Vassanji's 2003 novel The In-Between World of Vikram Lall won the Giller Prize in 2003. It is the fictional memoir of a Kenyan of Indian heritage and his family as they adjust to the changing political climates in colonial and post-colonial Kenya.
.Since 2003, the literary journal Kwani? has been publishing Kenyan contemporary literature. Kenya has also nurtured emerging versatile authors such as Paul Kipchumba (Kipwendui, Kibiwott) who demonstrate a pan-African outlook.[259]
.Kenya has a diverse assortment of popular music forms, in addition to multiple types of folk music based on the variety of over 40 regional languages.[260]
.Drums are the most dominant instrument in popular Kenyan music. Drum beats are very complex and include both native rhythms and imported ones, especially the Congolese cavacha rhythm. Popular Kenyan music usually involves the interplay of multiple parts, and more recently, showy guitar solos as well. There are also a number of local hip-hop artists, including Jua Cali; Afro-pop bands such as Sauti Sol; and musicians who play local genres like Benga, such as Akothee.
.Lyrics are most often in Kiswahili or English. There is also some emerging aspect of Lingala borrowed from Congolese musicians. Lyrics are also written in local languages. Urban radio generally only plays English music, though there also exist a number of vernacular radio stations.
.Zilizopendwa is a genre of local urban music that was recorded in the 1960s, 70s, and 80s by musicians such as Daudi Kabaka, Fadhili William, and Sukuma Bin Ongaro, and is particularly enjoyed by older people—having been popularised by the Kenya Broadcasting Corporation's Kiswahili service (formerly called Voice of Kenya or VOK).
.The Isukuti is a vigorous dance performed by the Luhya sub-tribes to the beat of a traditional drum called the Isukuti during many occasions such as the birth of a child, marriage, or funeral. Other traditional dances include the Ohangla among the Luo, Nzele among the Mijikenda, Mugithi among the Kikuyu, and Taarab among the Swahili.
.Kenya has a growing Christian gospel music scene. Prominent local gospel musicians include the Kenyan Boys Choir.
.Benga music has been popular since the late 1960s, especially in the area around Lake Victoria. The word benga is occasionally used to refer to any kind of pop music. Bass, guitar, and percussion are the usual instruments.
.Kenya is active in several sports, among them cricket, rallying, football, rugby, field hockey, and boxing. The country is known chiefly for its dominance in middle-distance and long-distance athletics, having consistently produced Olympic and Commonwealth Games champions in various distance events, especially in 800 m, 1,500 m, 3,000 m steeplechase, 5,000 m, 10,000 m, and the marathon. Kenyan athletes (particularly Kalenjin), continue to dominate the world of distance running, although competition from Morocco and Ethiopia has reduced this supremacy. Some of Kenya's best-known athletes include the four-time women's Boston Marathon winner and two-time world champion Catherine Ndereba, 800m world record holder David Rudisha, former marathon world record-holder Paul Tergat, and 5000m Olympic gold medalist John Ngugi. Kenya's most decorated athlete is three-time Olympic gold medalist and eleven-time world marathon major champion, Eliud Kipchoge.
.Kenya won several medals during the Beijing Olympics: six gold, four silver, and four bronze, making it Africa's most successful nation in the 2008 Olympics. New athletes gained attention, such as Pamela Jelimo, the women's 800m gold medalist who went on to win the IAAF Golden League jackpot, and Samuel Wanjiru, who won the men's marathon. Retired Olympic and Commonwealth Games champion Kipchoge Keino helped usher in Kenya's ongoing distance dynasty in the 1970s and was followed by Commonwealth Champion Henry Rono's spectacular string of world record performances. Lately, there has been controversy in Kenyan athletics circles, with the defection of a number of Kenyan athletes to represent other countries, chiefly Bahrain and Qatar.[261] The Kenyan Ministry of Sports has tried to stop the defections, but they have continued anyway, with Bernard Lagat being the latest, choosing to represent the United States.[261] Most of these defections occur because of economic or financial factors.[262] Decisions by the Kenyan government to tax athletes' earnings may also be a motivating factor.[263] Some elite Kenyan runners who cannot qualify for their country's strong national team find it easier to qualify by running for other countries.[264]
.Kenya has been a dominant force in women's volleyball within Africa, with both the clubs and the national team winning various continental championships in the past decade.[265][266] The women's team has competed at the Olympics and World Championships, though without any notable success. Cricket is another popular sport, also ranking as the most successful team sport. Kenya has competed in the Cricket World Cup since 1996. They upset some of the world's best teams and reached the semi-finals of the 2003 tournament. They won the inaugural World Cricket League Division 1 hosted in Nairobi and participated in the World T20. They also participated in the ICC Cricket World Cup 2011. Their current captain is Rakep Patel.[267]
.Rugby is increasing in popularity, especially with the annual Safari Sevens tournament. The Kenya Sevens team ranked 9th in the IRB Sevens World Series for the 2006 season. In 2016, the team beat Fiji at the Singapore Sevens finals, making Kenya the second African nation after South Africa to win a World Series championship.[268][269][270] Kenya was once also a regional powerhouse in football. However, its dominance has been eroded by wrangles within the now defunct Kenya Football Federation,[271] leading to a suspension by FIFA which was lifted in March 2007.[citation needed]
.In the motor rallying arena, Kenya is home to the world-famous Safari Rally, commonly acknowledged as one of the toughest rallies in the world.[272] First held in 1953, it was a part of the World Rally Championship for many years until its exclusion after the 2002 event owing to financial difficulties. Some of the best rally drivers in the world have taken part in and won the rally, such as Björn Waldegård, Hannu Mikkola, Tommi Mäkinen, Shekhar Mehta, Carlos Sainz, and Colin McRae. The Safari Rally returned to the world championship in 2021, after the 2003–2019 events ran as part of the African Rally Championship.[citation needed]
.Nairobi has hosted several major continental sports events, including the FIBA Africa Championship 1993, where Kenya's national basketball team finished in the top four, its best performance to date.[273]
.Kenya also has its own ice hockey team, the Kenya Ice Lions.[274] The team's home ground is the Solar Ice Rink at the Panari Sky Centre in Nairobi,[275][276] which is the first and largest ice rink in all of Africa.[277]
.Kenya men's national field hockey team was considered one of the good teams in the world during 1960s and 1970s.
.Kenya took 6th position in 1964 Summer Olympics hockey tournament and 4th in 1971 Men's FIH Hockey World Cup.
.Kenyans generally have three meals in a day—breakfast (kiamsha kinywa), lunch (chakula cha mchana), and supper (chakula cha jioni or simply chajio). In between, they have the 10-o'clock tea (chai ya saa nne) and 4 p.m. tea (chai ya saa kumi). Breakfast is usually tea or porridge with bread, chapati, mahamri, boiled sweet potatoes, or yams. Githeri is a common lunchtime dish in many households, while Ugali with vegetables, sour milk (mursik), meat, fish, or any other stew is generally eaten by much of the population for lunch or supper. Regional variations and dishes also exist.
.In western Kenya, among the Luo, fish is a common dish; among the Kalenjin, who dominate much of the Rift Valley Region, mursik—sour milk—is a common drink.
.In cities such as Nairobi, there are fast-food restaurants, including Steers, KFC,[278] and Subway.[279] There are also many fish-and-chips shops.[280]
.Cheese is becoming more popular in Kenya, with consumption increasing particularly among the middle class.[281][282]
.1°N 38°E﻿ / ﻿1°N 38°E﻿ / 1; 38
.Natural language processing (NLP) is a subfield of computer science and especially artificial intelligence. It is primarily concerned with providing computers with the ability to process data encoded in natural language and is thus closely related to information retrieval, knowledge representation and computational linguistics, a subfield of linguistics.
.Major tasks in natural language processing are speech recognition, text classification, natural language understanding, and natural language generation.
.Natural language processing has its roots in the 1950s.[1] Already in 1950, Alan Turing published an article titled "Computing Machinery and Intelligence" which proposed what is now called the Turing test as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language.
.The premise of symbolic NLP is well-summarized by John Searle's Chinese room experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.
.Up until the 1980s, most natural language processing systems were based on complex sets of hand-written rules.  Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing.  This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.[8]
.Symbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular:[18][19] such as by writing grammars or devising heuristic rules for stemming.
.Machine learning approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach: 
.Rule-based systems are commonly used:
.In the late 1980s and mid-1990s, the statistical approach ended a period of AI winter, which was caused by the inefficiencies of the rule-based approaches.[20][21]
.The earliest decision trees, producing systems of hard if–then rules, were still very similar to the old rule-based approaches.
Only the introduction of hidden Markov models, applied to part-of-speech tagging, announced the end of the old rule-based approach.
.A major drawback of statistical methods is that they require elaborate feature engineering. Since 2015,[22] the statistical approach has been replaced by the neural networks approach, using semantic networks[23] and word embeddings to capture semantic properties of words.  
.Intermediate tasks (e.g., part-of-speech tagging and dependency parsing) are not needed anymore. 
.Neural machine translation, based on then-newly invented sequence-to-sequence transformations, made obsolete the intermediate steps, such as word alignment, previously necessary for statistical machine translation.
.The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.
.Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below.
.Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed:[46]
.Most higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above).
.Cognition refers to "the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses."[47] Cognitive science is the interdisciplinary, scientific study of the mind and its processes.[48] Cognitive linguistics is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics.[49] Especially during the age of symbolic NLP, the area of computational linguistics maintained strong ties with cognitive studies.
.As an example, George Lakoff offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics,[50] with two defining aspects:
.Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar,[53] functional grammar,[54] construction grammar,[55] computational psycholinguistics and cognitive neuroscience (e.g., ACT-R), however, with limited uptake in mainstream NLP (as measured by presence on major conferences[56] of the ACL). More recently, ideas of cognitive NLP have been revived as an approach to achieve explainability, e.g., under the notion of "cognitive AI".[57] Likewise, ideas of cognitive NLP are inherent to neural models multimodal NLP (although rarely made explicit)[58] and developments in artificial intelligence, specifically tools and technologies using large language model approaches[59] and new directions in artificial general intelligence based on the free energy principle[60] by British neuroscientist and theoretician at University College London Karl J. Friston.
.
.Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals.[1]
.High-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., ChatGPT and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: "A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore."[2][3]
.Various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include learning, reasoning, knowledge representation, planning, natural language processing, perception, and support for robotics.[a] To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics.[b] AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields.[4] Some companies, such as OpenAI, Google DeepMind and Meta, aim to create artificial general intelligence (AGI)—AI that can complete virtually any cognitive task at least as well as a human.[5]
.Artificial intelligence was founded as an academic discipline in 1956,[6] and the field went through multiple cycles of optimism throughout its history,[7][8] followed by periods of disappointment and loss of funding, known as AI winters.[9][10] Funding and interest vastly increased after 2012 when graphics processing units started being used to accelerate neural networks, and deep learning outperformed previous AI techniques.[11] This growth accelerated further after 2017 with the transformer architecture.[12] In the 2020s, the period of rapid progress marked by advanced generative AI became known as the AI boom. Generative AI and its ability to create and modify content exposed several unintended consequences and harms in the present and raised ethical concerns about AI's long-term effects and potential existential risks, prompting discussions about regulatory policies to ensure the safety and benefits of the technology.
.The general problem of simulating (or creating) intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research.[a]
.Early researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions.[13] By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics.[14]
.Many of these algorithms are insufficient for solving large reasoning problems because they experience a "combinatorial explosion": They become exponentially slower as the problems grow.[15] Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments.[16] Accurate and efficient reasoning is an unsolved problem.
.Knowledge representation and knowledge engineering[17] allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval,[18] scene interpretation,[19] clinical decision support,[20] knowledge discovery (mining "interesting" and actionable inferences from large databases),[21] and other areas.[22]
.A knowledge base is a body of knowledge represented in a form that can be used by a program. An ontology is the set of objects, relations, concepts, and properties used by a particular domain of knowledge.[23] Knowledge bases need to represent things such as objects, properties, categories, and relations between objects;[24] situations, events, states, and time;[25] causes and effects;[26] knowledge about knowledge (what we know about what other people know);[27] default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing);[28] and many other aspects and domains of knowledge.
.Among the most difficult problems in knowledge representation are the breadth of commonsense knowledge (the set of atomic facts that the average person knows is enormous);[29] and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as "facts" or "statements" that they could express verbally).[16] There is also the difficulty of knowledge acquisition, the problem of obtaining knowledge for AI applications.[c]
.An "agent" is anything that perceives and takes actions in the world. A rational agent has goals or preferences and takes actions to make them happen.[d][32] In automated planning, the agent has a specific goal.[33] In automated decision-making, the agent has preferences—there are some situations it would prefer to be in, and some situations it is trying to avoid. The decision-making agent assigns a number to each situation (called the "utility") that measures how much the agent prefers it. For each possible action, it can calculate the "expected utility": the utility of all possible outcomes of the action, weighted by the probability that the outcome will occur. It can then choose the action with the maximum expected utility.[34]
.In classical planning, the agent knows exactly what the effect of any action will be.[35] In most real-world problems, however, the agent may not be certain about the situation they are in (it is "unknown" or "unobservable") and it may not know for certain what will happen after each possible action (it is not "deterministic"). It must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked.[36]
.In some problems, the agent's preferences may be uncertain, especially if there are other agents or humans involved. These can be learned (e.g., with inverse reinforcement learning), or the agent can seek information to improve its preferences.[37] Information value theory can be used to weigh the value of exploratory or experimental actions.[38] The space of possible future actions and situations is typically intractably large, so the agents must take actions and evaluate situations while being uncertain of what the outcome will be.
.A Markov decision process has a transition model that describes the probability that a particular action will change the state in a particular way and a reward function that supplies the utility of each state and the cost of each action. A policy associates a decision with each possible state. The policy could be calculated (e.g., by iteration), be heuristic, or it can be learned.[39]
.Game theory describes the rational behavior of multiple interacting agents and is used in AI programs that make decisions that involve other agents.[40]
.Machine learning is the study of programs that can improve their performance on a given task automatically.[41] It has been a part of AI from the beginning.[e]
.There are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance.[44] Supervised learning requires labeling the training data with the expected answers, and comes in two main varieties: classification (where the program must learn to predict what category the input belongs in) and regression (where the program must deduce a numeric function based on numeric input).[45]
.In reinforcement learning, the agent is rewarded for good responses and punished for bad ones. The agent learns to choose responses that are classified as "good".[46] Transfer learning is when the knowledge gained from one problem is applied to a new problem.[47] Deep learning is a type of machine learning that runs inputs through biologically inspired artificial neural networks for all of these types of learning.[48]
.Computational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization.[49]
.Natural language processing (NLP)[50] allows programs to read, write and communicate in human languages such as English. Specific problems include speech recognition, speech synthesis, machine translation, information extraction, information retrieval and question answering.[51]
.Early work, based on Noam Chomsky's generative grammar and semantic networks, had difficulty with word-sense disambiguation[f] unless restricted to small domains called "micro-worlds" (due to the common sense knowledge problem[29]). Margaret Masterman believed that it was meaning and not grammar that was the key to understanding languages, and that thesauri and not dictionaries should be the basis of computational language structure.
.Modern deep learning techniques for NLP include word embedding (representing words, typically as vectors encoding their meaning),[52] transformers (a deep learning architecture using an attention mechanism),[53] and others.[54] In 2019, generative pre-trained transformer (or "GPT") language models began to generate coherent text,[55][56] and by 2023, these models were able to get human-level scores on the bar exam, SAT test, GRE test, and many other real-world applications.[57]
.Machine perception is the ability to use input from sensors (such as cameras, microphones, wireless signals, active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Computer vision is the ability to analyze visual input.[58]
.The field includes speech recognition,[59] image classification,[60] facial recognition, object recognition,[61]object tracking,[62] and robotic perception.[63]
.Affective computing is a field that comprises systems that recognize, interpret, process, or simulate human feeling, emotion, and mood.[65] For example, some virtual assistants are programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human–computer interaction.
.However, this tends to give naïve users an unrealistic conception of the intelligence of existing computer agents.[66] Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis, wherein AI classifies the effects displayed by a videotaped subject.[67]
.A machine with artificial general intelligence should be able to solve a wide variety of problems with breadth and versatility similar to human intelligence.[68]
.AI research uses a wide variety of techniques to accomplish the goals above.[b]
.AI can solve many problems by intelligently searching through many possible solutions.[69] There are two very different kinds of search used in AI: state space search and local search.
.State space search searches through a tree of possible states to try to find a goal state.[70] For example, planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis.[71]
.Simple exhaustive searches[72] are rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes.[15] "Heuristics" or "rules of thumb" can help prioritize choices that are more likely to reach a goal.[73]
.Adversarial search is used for game-playing programs, such as chess or Go. It searches through a tree of possible moves and countermoves, looking for a winning position.[74]
.Local search uses mathematical optimization to find a solution to a problem. It begins with some form of guess and refines it incrementally.[75]
.Gradient descent is a type of local search that optimizes a set of numerical parameters by incrementally adjusting them to minimize a loss function. Variants of gradient descent are commonly used to train neural networks,[76] through the backpropagation algorithm.
.Another type of local search is evolutionary computation, which aims to iteratively improve a set of candidate solutions by "mutating" and "recombining" them, selecting only the fittest to survive each generation.[77]
.Distributed search processes can coordinate via swarm intelligence algorithms. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking) and ant colony optimization (inspired by ant trails).[78]
.Formal logic is used for reasoning and knowledge representation.[79]
Formal logic comes in two main forms: propositional logic (which operates on statements that are true or false and uses logical connectives such as "and", "or", "not" and "implies")[80] and predicate logic (which also operates on objects, predicates and relations and uses quantifiers such as "Every X is a Y" and "There are some Xs that are Ys").[81]
.Deductive reasoning in logic is the process of proving a new statement (conclusion) from other statements that are given and assumed to be true (the premises).[82] Proofs can be structured as proof trees, in which nodes are labelled by sentences, and children nodes are connected to parent nodes by inference rules.
.Given a problem and a set of premises, problem-solving reduces to searching for a proof tree whose root node is labelled by a solution of the problem and whose leaf nodes are labelled by premises or axioms. In the case of Horn clauses, problem-solving search can be performed by reasoning forwards from the premises or backwards from the problem.[83] In the more general case of the clausal form of first-order logic, resolution is a single, axiom-free rule of inference, in which a problem is solved by proving a contradiction from premises that include the negation of the problem to be solved.[84]
.Inference in both Horn clause logic and first-order logic is undecidable, and therefore intractable. However, backward reasoning with Horn clauses, which underpins computation in the logic programming language Prolog, is Turing complete. Moreover, its efficiency is competitive with computation in other symbolic programming languages.[85]
.Fuzzy logic assigns a "degree of truth" between 0 and 1. It can therefore handle propositions that are vague and partially true.[86]
.Non-monotonic logics, including logic programming with negation as failure, are designed to handle default reasoning.[28] Other specialized versions of logic have been developed to describe many complex domains.
.Many problems in AI (including reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of tools to solve these problems using methods from probability theory and economics.[87] Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis,[88] and information value theory.[89] These tools include models such as Markov decision processes,[90] dynamic decision networks,[91] game theory and mechanism design.[92]
.Bayesian networks[93] are a tool that can be used for reasoning (using the Bayesian inference algorithm),[g][95] learning (using the expectation–maximization algorithm),[h][97] planning (using decision networks)[98] and perception (using dynamic Bayesian networks).[91]
.Probabilistic algorithms can also be used for filtering, prediction, smoothing, and finding explanations for streams of data, thus helping perception systems analyze processes that occur over time (e.g., hidden Markov models or Kalman filters).[91]
.The simplest AI applications can be divided into two types: classifiers (e.g., "if shiny then diamond"), on one hand, and controllers (e.g., "if diamond then pick up"), on the other hand. Classifiers[99] are functions that use pattern matching to determine the closest match. They can be fine-tuned based on chosen examples using supervised learning. Each pattern (also called an "observation") is labeled with a certain predefined class. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.[45]
.There are many kinds of classifiers in use.[100] The decision tree is the simplest and most widely used symbolic machine learning algorithm.[101] K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s, and Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s.[102]
The naive Bayes classifier is reportedly the "most widely used learner"[103] at Google, due in part to its scalability.[104]
Neural networks are also used as classifiers.[105]
.An artificial neural network is based on a collection of nodes also known as artificial neurons, which loosely model the neurons in a biological brain. It is trained to recognise patterns; once trained, it can recognise those patterns in fresh data. There is an input, at least one hidden layer of nodes and an output. Each node applies a function and once the weight crosses its specified threshold, the data is transmitted to the next layer. A network is typically called a deep neural network if it has at least 2 hidden layers.[105]
.Learning algorithms for neural networks use local search to choose the weights that will get the right output for each input during training. The most common training technique is the backpropagation algorithm.[106] Neural networks learn to model complex relationships between inputs and outputs and find patterns in data. In theory, a neural network can learn any function.[107]
.In feedforward neural networks the signal passes in only one direction.[108] Recurrent neural networks feed the output signal back into the input, which allows short-term memories of previous input events. Long short term memory is the most successful network architecture for recurrent networks.[109] Perceptrons[110] use only a single layer of neurons; deep learning[111] uses multiple layers. Convolutional neural networks strengthen the connection between neurons that are "close" to each other—this is especially important in image processing, where a local set of neurons must identify an "edge" before the network can identify an object.[112]
.Deep learning[111] uses several layers of neurons between the network's inputs and outputs. The multiple layers can progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits, letters, or faces.[113]
.Deep learning has profoundly improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, natural language processing, image classification,[114] and others. The reason that deep learning performs so well in so many applications is not known as of 2021.[115] The sudden success of deep learning in 2012–2015 did not occur because of some new discovery or theoretical breakthrough (deep neural networks and backpropagation had been described by many people, as far back as the 1950s)[i] but because of two factors: the incredible increase in computer power (including the hundred-fold increase in speed by switching to GPUs) and the availability of vast amounts of training data, especially the giant curated datasets used for benchmark testing, such as ImageNet.[j]
.Generative pre-trained transformers (GPT) are large language models (LLMs) that generate text based on the semantic relationships between words in sentences. Text-based GPT models are pre-trained on a large corpus of text that can be from the Internet. The pretraining consists of predicting the next token (a token being usually a word, subword, or punctuation). Throughout this pretraining, GPT models accumulate knowledge about the world and can then generate human-like text by repeatedly predicting the next token. Typically, a subsequent training phase makes the model more truthful, useful, and harmless, usually with a technique called reinforcement learning from human feedback (RLHF). Current GPT models are prone to generating falsehoods called "hallucinations". These can be reduced with RLHF and quality data, but the problem has been getting worse for reasoning systems.[123] Such systems are used in chatbots, which allow people to ask a question or request a task in simple text.[124][125]
.Current models and services include Gemini (formerly Bard), ChatGPT, Grok, Claude, Copilot, and LLaMA.[126] Multimodal GPT models can process different types of data (modalities) such as images, videos, sound, and text.[127]
.In the late 2010s, graphics processing units (GPUs) that were increasingly designed with AI-specific enhancements and used with specialized TensorFlow software had replaced previously used central processing unit (CPUs) as the dominant means for large-scale (commercial and academic) machine learning models' training.[128] Specialized programming languages such as Prolog were used in early AI research,[129] but general-purpose programming languages like Python have become predominant.[130]
.The transistor density in integrated circuits has been observed to roughly double every 18 months—a trend known as Moore's law, named after the Intel co-founder Gordon Moore, who first identified it. Improvements in GPUs have been even faster,[131] a trend sometimes called Huang's law,[132] named after Nvidia co-founder and CEO Jensen Huang.
.AI and machine learning technology is used in most of the essential applications of the 2020s, including: search engines (such as Google Search), targeting online advertisements, recommendation systems (offered by Netflix, YouTube or Amazon), driving internet traffic, targeted advertising (AdSense, Facebook), virtual assistants (such as Siri or Alexa), autonomous vehicles (including drones, ADAS and self-driving cars), automatic language translation (Microsoft Translator, Google Translate), facial recognition (Apple's FaceID or Microsoft's DeepFace and Google's FaceNet) and image labeling (used by Facebook, Apple's Photos and TikTok). The deployment of AI may be overseen by a Chief automation officer (CAO).
.The application of AI in medicine and medical research has the potential to increase patient care and quality of life.[133] Through the lens of the Hippocratic Oath, medical professionals are ethically compelled to use AI, if applications can more accurately diagnose and treat patients.[134][135]
.For medical research, AI is an important tool for processing and integrating big data. This is particularly important for organoid and tissue engineering development which use microscopy imaging as a key technique in fabrication.[136] It has been suggested that AI can overcome discrepancies in funding allocated to different fields of research.[136][137] New AI tools can deepen the understanding of biomedically relevant pathways. For example, AlphaFold 2 (2021) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein.[138] In 2023, it was reported that AI-guided drug discovery helped find a class of antibiotics capable of killing two different types of drug-resistant bacteria.[139] In 2024, researchers used machine learning to accelerate the search for Parkinson's disease drug treatments. Their aim was to identify compounds that block the clumping, or aggregation, of alpha-synuclein (the protein that characterises Parkinson's disease). They were able to speed up the initial screening process ten-fold and reduce the cost by a thousand-fold.[140][141]
.Game playing programs have been used since the 1950s to demonstrate and test AI's most advanced techniques.[142] Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997.[143] In 2011, in a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin.[144] In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps. Then, in 2017, it defeated Ke Jie, who was the best Go player in the world.[145] Other programs handle imperfect-information games, such as the poker-playing program Pluribus.[146] DeepMind developed increasingly generalistic reinforcement learning models, such as with MuZero, which could be trained to play chess, Go, or Atari games.[147] In 2019, DeepMind's AlphaStar achieved grandmaster level in StarCraft II, a particularly challenging real-time strategy game that involves incomplete knowledge of what happens on the map.[148] In 2021, an AI agent competed in a PlayStation Gran Turismo competition, winning against four of the world's best Gran Turismo drivers using deep reinforcement learning.[149] In 2024, Google DeepMind introduced SIMA, a type of AI capable of autonomously playing nine previously unseen open-world video games by observing screen output, as well as executing short, specific tasks in response to natural language instructions.[150]
.Large language models, such as GPT-4, Gemini, Claude, LLaMa or Mistral, are increasingly used in mathematics. These probabilistic models are versatile, but can also produce wrong answers in the form of hallucinations. They sometimes need a large database of mathematical problems to learn from, but also methods such as supervised fine-tuning[151] or trained classifiers with human-annotated data to improve answers for new problems and learn from corrections.[152] A February 2024 study showed that the performance of some language models for reasoning capabilities in solving math problems not included in their training data was low, even for problems with only minor deviations from trained data.[153] One technique to improve their performance involves training the models to produce correct reasoning steps, rather than just the correct result.[154] The Alibaba Group developed a version of its Qwen models called Qwen2-Math, that achieved state-of-the-art performance on several mathematical benchmarks, including 84% accuracy on the MATH dataset of competition mathematics problems.[155] In January 2025, Microsoft proposed the technique rStar-Math that leverages Monte Carlo tree search and step-by-step reasoning, enabling a relatively small language model like Qwen-7B to solve 53% of the AIME 2024 and 90% of the MATH benchmark problems.[156]
.Alternatively, dedicated models for mathematical problem solving with higher precision for the outcome including proof of theorems have been developed such as AlphaTensor, AlphaGeometry and AlphaProof all from Google DeepMind,[157] Llemma from EleutherAI[158] or Julius.[159]
.When natural language is used to describe mathematical problems, converters can transform such prompts into a formal language such as Lean to define mathematical tasks.
.Some models have been developed to solve challenging problems and reach good results in benchmark tests, others to serve as educational tools in mathematics.[160]
.Topological deep learning integrates various topological approaches.
.Finance is one of the fastest growing sectors where applied AI tools are being deployed: from retail online banking to investment advice and insurance, where automated "robot advisers" have been in use for some years.[161]
.According to Nicolas Firzli, director of the World Pensions & Investments Forum, it may be too early to see the emergence of highly innovative AI-informed financial products and services. He argues that "the deployment of AI tools will simply further automatise things: destroying tens of thousands of jobs in banking, financial planning, and pension advice in the process, but I'm not sure it will unleash a new wave of [e.g., sophisticated] pension innovation."[162]
.Various countries are deploying AI military applications.[163] The main applications enhance command and control, communications, sensors, integration and interoperability.[164] Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles.[163] AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Joint Fires between networked combat vehicles, both human operated and autonomous.[164]
.AI has been used in military operations in Iraq, Syria, Israel and Ukraine.[163][165][166][167]
.Generative artificial intelligence (Generative AI, GenAI,[168] or GAI) is a subfield of artificial intelligence that uses generative models to produce text, images, videos, or other forms of data.[169][170][171] These models learn the underlying patterns and structures of their training data and use them to produce new data[172][173] based on the input, which often comes in the form of natural language prompts.[174][175]
.Generative AI tools have become more common since an "AI boom" in the 2020s. This boom was made possible by improvements in transformer-based deep neural networks, particularly large language models (LLMs). Major tools include chatbots such as ChatGPT, Copilot, Gemini, Grok, and DeepSeek; text-to-image artificial intelligence image generation systems such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video AI generators such as Sora.[176][177][178][179] Technology companies developing generative AI include OpenAI, Anthropic, Meta AI, Microsoft, Google, DeepSeek, and Baidu.[174][180][181]
.Artificial intelligent (AI) agents are software entities designed to perceive their environment, make decisions, and take actions autonomously to achieve specific goals. These agents can interact with users, their environment, or other agents. AI agents are used in various applications, including virtual assistants, chatbots, autonomous vehicles, game-playing systems, and industrial robotics. AI agents operate within the constraints of their programming, available computational resources, and hardware limitations. This means they are restricted to performing tasks within their defined scope and have finite memory and processing capabilities. In real-world applications, AI agents often face time constraints for decision-making and action execution. Many AI agents incorporate learning algorithms, enabling them to improve their performance over time through experience or training. Using machine learning, AI agents can adapt to new situations and optimise their behaviour for their designated tasks.[185][186][187]
.Applications of AI in this domain include AI-enabled menstruation and fertility trackers that analyze user data to offer prediction,[188] AI-integrated sex toys (e.g., teledildonics),[189] AI-generated sexual education content,[190] and AI agents that simulate sexual and romantic partners (e.g., Replika).[191]  AI is also used for the production of non-consensual deepfake pornography, raising significant ethical and legal concerns.[192]
.AI technologies have also been used to attempt to identify online gender-based violence and online sexual grooming of minors.[193][194]
.There are also thousands of successful AI applications used to solve specific problems for specific industries or institutions. In a 2017 survey, one in five companies reported having incorporated "AI" in some offerings or processes.[195] A few examples are energy storage, medical diagnosis, military logistics, applications that predict the result of judicial decisions, foreign policy, or supply chain management.
.AI applications for evacuation and disaster management are growing. AI has been used to investigate if and how people evacuated in large scale and small scale evacuations using historical data from GPS, videos or social media. Further, AI can provide real time information on the real time evacuation conditions.[196][197][198]
.In agriculture, AI has helped farmers identify areas that need irrigation, fertilization, pesticide treatments or increasing yield. Agronomists use AI to conduct research and development. AI has been used to predict the ripening time for crops such as tomatoes, monitor soil moisture, operate agricultural robots, conduct predictive analytics, classify livestock pig call emotions, automate greenhouses, detect diseases and pests, and save water.
.Artificial intelligence is used in astronomy to analyze increasing amounts of available data and applications, mainly for "classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights." For example, it is used for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy. Additionally, it could be used for activities in space, such as space exploration, including the analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance, and more autonomous operation.
.During the 2024 Indian elections, US$50 million was spent on authorized AI-generated content, notably by creating deepfakes of allied (including sometimes deceased) politicians to better engage with voters, and by translating speeches to various local languages.[199]
.AI has potential benefits and potential risks.[200] AI may be able to advance science and find solutions for serious problems: Demis Hassabis of DeepMind hopes to "solve intelligence, and then use that to solve everything else".[201] However, as the use of AI has become widespread, several unintended consequences and risks have been identified.[202] In-production systems can sometimes not factor ethics and bias into their AI training processes, especially when the AI algorithms are inherently unexplainable in deep learning.[203]
.Machine learning algorithms require large amounts of data. The techniques used to acquire this data have raised concerns about privacy, surveillance and copyright.
.AI-powered devices and services, such as virtual assistants and IoT products, continuously collect personal information, raising concerns about intrusive data gathering and unauthorized access by third parties. The loss of privacy is further exacerbated by AI's ability to process and combine vast amounts of data, potentially leading to a surveillance society where individual activities are constantly monitored and analyzed without adequate safeguards or transparency.
.Sensitive user data collected may include online activity records, geolocation data, video, or audio.[204] For example, in order to build speech recognition algorithms, Amazon has recorded millions of private conversations and allowed temporary workers to listen to and transcribe some of them.[205] Opinions about this widespread surveillance range from those who see it as a necessary evil to those for whom it is clearly unethical and a violation of the right to privacy.[206]
.AI developers argue that this is the only way to deliver valuable applications and have developed several techniques that attempt to preserve privacy while still obtaining the data, such as data aggregation, de-identification and differential privacy.[207] Since 2016, some privacy experts, such as Cynthia Dwork, have begun to view privacy in terms of fairness. Brian Christian wrote that experts have pivoted "from the question of 'what they know' to the question of 'what they're doing with it'."[208]
.Generative AI is often trained on unlicensed copyrighted works, including in domains such as images or computer code; the output is then used under the rationale of "fair use". Experts disagree about how well and under what circumstances this rationale will hold up in courts of law; relevant factors may include "the purpose and character of the use of the copyrighted work" and "the effect upon the potential market for the copyrighted work".[209][210] Website owners who do not wish to have their content scraped can indicate it in a "robots.txt" file.[211] In 2023, leading authors (including John Grisham and Jonathan Franzen) sued AI companies for using their work to train generative AI.[212][213] Another discussed approach is to envision a separate sui generis system of protection for creations generated by AI to ensure fair attribution and compensation for human authors.[214]
.The commercial AI scene is dominated by Big Tech companies such as Alphabet Inc., Amazon, Apple Inc., Meta Platforms, and Microsoft.[215][216][217] Some of these players already own the vast majority of existing cloud infrastructure and computing power from data centers, allowing them to entrench further in the marketplace.[218][219]
.In January 2024, the International Energy Agency (IEA) released Electricity 2024, Analysis and Forecast to 2026, forecasting electric power use.[220] This is the first IEA report to make projections for data centers and power consumption for artificial intelligence and cryptocurrency. The report states that power demand for these uses might double by 2026, with additional electric power usage equal to electricity used by the whole Japanese nation.[221]
.Prodigious power consumption by AI is responsible for the growth of fossil fuels use, and might delay closings of obsolete, carbon-emitting coal energy facilities. There is a feverish rise in the construction of data centers throughout the US, making large technology firms (e.g., Microsoft, Meta, Google, Amazon) into voracious consumers of electric power. Projected electric consumption is so immense that there is concern that it will be fulfilled no matter the source. A ChatGPT search involves the use of 10 times the electrical energy as a Google search. The large firms are in haste to find power sources – from nuclear energy to geothermal to fusion. The tech firms argue that – in the long view – AI will be eventually kinder to the environment, but they need the energy now. AI makes the power grid more efficient and "intelligent", will assist in the growth of nuclear power, and track overall carbon emissions, according to technology firms.[222]
.A 2024 Goldman Sachs Research Paper, AI Data Centers and the Coming US Power Demand Surge, found "US power demand (is) likely to experience growth not seen in a generation...." and forecasts that, by 2030, US data centers will consume 8% of US power, as opposed to 3% in 2022, presaging growth for the electrical power generation industry by a variety of means.[223] Data centers' need for more and more electrical power is such that they might max out the electrical grid. The Big Tech companies counter that AI can be used to maximize the utilization of the grid by all.[224]
.In 2024, the Wall Street Journal reported that big AI companies have begun negotiations with the US nuclear power providers to provide electricity to the data centers. In March 2024 Amazon purchased a Pennsylvania nuclear-powered data center for $650 Million (US).[225] Nvidia CEO Jen-Hsun Huang said nuclear power is a good option for the data centers.[226]
.In September 2024, Microsoft announced an agreement with Constellation Energy to re-open the Three Mile Island nuclear power plant to provide Microsoft with 100% of all electric power produced by the plant for 20 years. Reopening the plant, which suffered a partial nuclear meltdown of its Unit 2 reactor in 1979, will require Constellation to get through strict regulatory processes which will include extensive safety scrutiny from the US Nuclear Regulatory Commission. If approved (this will be the first ever US re-commissioning of a nuclear plant), over 835 megawatts of power – enough for 800,000 homes – of energy will be produced. The cost for re-opening and upgrading is estimated at $1.6 billion (US) and is dependent on tax breaks for nuclear power contained in the 2022 US Inflation Reduction Act.[227] The US government and the state of Michigan are investing almost $2 billion (US) to reopen the Palisades Nuclear reactor on Lake Michigan. Closed since 2022, the plant is planned to be reopened in October 2025. The Three Mile Island facility will be renamed the Crane Clean Energy Center after Chris Crane, a nuclear proponent and former CEO of Exelon who was responsible for Exelon spinoff of Constellation.[228]
.After the last approval in September 2023, Taiwan suspended the approval of data centers north of Taoyuan with a capacity of more than 5 MW in 2024, due to power supply shortages.[229] Taiwan aims to phase out nuclear power by 2025.[229] On the other hand, Singapore imposed a ban on the opening of data centers in 2019 due to electric power, but in 2022, lifted this ban.[229]
.Although most nuclear plants in Japan have been shut down after the 2011 Fukushima nuclear accident, according to an October 2024 Bloomberg article in Japanese, cloud gaming services company Ubitus, in which Nvidia has a stake, is looking for land in Japan near nuclear power plant for a new data center for generative AI.[230] Ubitus CEO Wesley Kuo said nuclear power plants are the most efficient, cheap and stable power for AI.[230]
.On 1 November 2024, the Federal Energy Regulatory Commission (FERC) rejected an application submitted by Talen Energy for approval to supply some electricity from the nuclear power station Susquehanna to Amazon's data center.[231] 
According to the Commission Chairman Willie L. Phillips, it is a burden on the electricity grid as well as a significant cost shifting concern to households and other business sectors.[231]
.In 2025 a report prepared by the International Energy Agency estimated the greenhouse gas emissions from the energy consumption of AI at 180 million tons. By 2035, these emissions could rise to 300-500 million tonnes depending on what measures will be taken. This is below 1.5% of the energy sector emissions. The emissions reduction potential of AI was estimated at 5% of the energy sector emissions, but rebound effects (for example if people will pass from public transport to autonomous cars) can reduce it.[232]
.YouTube, Facebook and others use recommender systems to guide users to more content. These AI programs were given the goal of maximizing user engagement (that is, the only goal was to keep people watching). The AI learned that users tended to choose misinformation, conspiracy theories, and extreme partisan content, and, to keep them watching, the AI recommended more of it. Users also tended to watch more content on the same subject, so the AI led people into filter bubbles where they received multiple versions of the same misinformation.[233] This convinced many users that the misinformation was true, and ultimately undermined trust in institutions, the media and the government.[234] The AI program had correctly learned to maximize its goal, but the result was harmful to society. After the U.S. election in 2016, major technology companies took some steps to mitigate the problem.[235]
.In 2022, generative AI began to create images, audio, video and text that are indistinguishable from real photographs, recordings, films, or human writing.[236] It is possible for bad actors to use this technology to create massive amounts of misinformation or propaganda.[237] One such potential malicious use is deepfakes for computational propaganda.[238] AI pioneer Geoffrey Hinton expressed concern about AI enabling "authoritarian leaders to manipulate their electorates" on a large scale, among other risks.[239]
.AI researchers at Microsoft, OpenAI, universities and other organisations have suggested using "personhood credentials" as a way to overcome online deception enabled by AI models.[240]
.Machine learning applications will be biased[k] if they learn from biased data.[242] The developers may not be aware that the bias exists.[243] Bias can be introduced by the way training data is selected and by the way a model is deployed.[244][242] If a biased algorithm is used to make decisions that can seriously harm people (as it can in medicine, finance, recruitment, housing or policing) then the algorithm may cause discrimination.[245] The field of fairness studies how to prevent harms from algorithmic biases.
.On June 28, 2015, Google Photos's new image labeling feature mistakenly identified Jacky Alcine and a friend as "gorillas" because they were black. The system was trained on a dataset that contained very few images of black people,[246] a problem called "sample size disparity".[247] Google "fixed" this problem by preventing the system from labelling anything as a "gorilla". Eight years later, in 2023, Google Photos still could not identify a gorilla, and neither could similar products from Apple, Facebook, Microsoft and Amazon.[248]
.COMPAS is a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist. In 2016, Julia Angwin at ProPublica discovered that COMPAS exhibited racial bias, despite the fact that the program was not told the races of the defendants. Although the error rate for both whites and blacks was calibrated equal at exactly 61%, the errors for each race were different—the system consistently overestimated the chance that a black person would re-offend and would underestimate the chance that a white person would not re-offend.[249] In 2017, several researchers[l] showed that it was mathematically impossible for COMPAS to accommodate all possible measures of fairness when the base rates of re-offense were different for whites and blacks in the data.[251]
.A program can make biased decisions even if the data does not explicitly mention a problematic feature (such as "race" or "gender"). The feature will correlate with other features (like "address", "shopping history" or "first name"), and the program will make the same decisions based on these features as it would on "race" or "gender".[252] Moritz Hardt said "the most robust fact in this research area is that fairness through blindness doesn't work."[253]
.Criticism of COMPAS highlighted that machine learning models are designed to make "predictions" that are only valid if we assume that the future will resemble the past. If they are trained on data that includes the results of racist decisions in the past, machine learning models must predict that racist decisions will be made in the future. If an application then uses these predictions as recommendations, some of these "recommendations" will likely be racist.[254] Thus, machine learning is not well suited to help make decisions in areas where there is hope that the future will be better than the past. It is descriptive rather than prescriptive.[m]
.Bias and unfairness may go undetected because the developers are overwhelmingly white and male: among AI engineers, about 4% are black and 20% are women.[247]
.There are various conflicting definitions and mathematical models of fairness. These notions depend on ethical assumptions, and are influenced by beliefs about society. One broad category is distributive fairness, which focuses on the outcomes, often identifying groups and seeking to compensate for statistical disparities. Representational fairness tries to ensure that AI systems do not reinforce negative stereotypes or render certain groups invisible. Procedural fairness focuses on the decision process rather than the outcome. The most relevant notions of fairness may depend on the context, notably the type of AI application and the stakeholders. The subjectivity in the notions of bias and fairness makes it difficult for companies to operationalize them. Having access to sensitive attributes such as race or gender is also considered by many AI ethicists to be necessary in order to compensate for biases, but it may conflict with anti-discrimination laws.[241]
.At its 2022 Conference on Fairness, Accountability, and Transparency (ACM FAccT 2022), the Association for Computing Machinery, in Seoul, South Korea, presented and published findings that recommend that until AI and robotics systems are demonstrated to be free of bias mistakes, they are unsafe, and the use of self-learning neural networks trained on vast, unregulated sources of flawed internet data should be curtailed.[dubious – discuss][256]
.Many AI systems are so complex that their designers cannot explain how they reach their decisions.[257] Particularly with deep neural networks, in which there are a large amount of non-linear relationships between inputs and outputs. But some popular explainability techniques exist.[258]
.It is impossible to be certain that a program is operating correctly if no one knows how exactly it works. There have been many cases where a machine learning program passed rigorous tests, but nevertheless learned something different than what the programmers intended. For example, a system that could identify skin diseases better than medical professionals was found to actually have a strong tendency to classify images with a ruler as "cancerous", because pictures of malignancies typically include a ruler to show the scale.[259] Another machine learning system designed to help effectively allocate medical resources was found to classify patients with asthma as being at "low risk" of dying from pneumonia. Having asthma is actually a severe risk factor, but since the patients having asthma would usually get much more medical care, they were relatively unlikely to die according to the training data. The correlation between asthma and low risk of dying from pneumonia was real, but misleading.[260]
.People who have been harmed by an algorithm's decision have a right to an explanation.[261] Doctors, for example, are expected to clearly and completely explain to their colleagues the reasoning behind any decision they make. Early drafts of the European Union's General Data Protection Regulation in 2016 included an explicit statement that this right exists.[n] Industry experts noted that this is an unsolved problem with no solution in sight. Regulators argued that nevertheless the harm is real: if the problem has no solution, the tools should not be used.[262]
.DARPA established the XAI ("Explainable Artificial Intelligence") program in 2014 to try to solve these problems.[263]
.Several approaches aim to address the transparency problem. SHAP enables to visualise the contribution of each feature to the output.[264] LIME can locally approximate a model's outputs with a simpler, interpretable model.[265] Multitask learning provides a large number of outputs in addition to the target classification. These other outputs can help developers deduce what the network has learned.[266] Deconvolution, DeepDream and other generative methods can allow developers to see what different layers of a deep network for computer vision have learned, and produce output that can suggest what the network is learning.[267] For generative pre-trained transformers, Anthropic developed a technique based on dictionary learning that associates patterns of neuron activations with human-understandable concepts.[268]
.Artificial intelligence provides a number of tools that are useful to bad actors, such as authoritarian governments, terrorists, criminals or rogue states.
.A lethal autonomous weapon is a machine that locates, selects and engages human targets without human supervision.[o] Widely available AI tools can be used by bad actors to develop inexpensive autonomous weapons and, if produced at scale, they are potentially weapons of mass destruction.[270] Even when used in conventional warfare, they currently cannot reliably choose targets and could potentially kill an innocent person.[270] In 2014, 30 nations (including China) supported a ban on autonomous weapons under the United Nations' Convention on Certain Conventional Weapons, however the United States and others disagreed.[271] By 2015, over fifty countries were reported to be researching battlefield robots.[272]
.AI tools make it easier for authoritarian governments to efficiently control their citizens in several ways. Face and voice recognition allow widespread surveillance. Machine learning, operating this data, can classify potential enemies of the state and prevent them from hiding. Recommendation systems can precisely target propaganda and misinformation for maximum effect. Deepfakes and generative AI aid in producing misinformation. Advanced AI can make authoritarian centralized decision making more competitive than liberal and decentralized systems such as markets. It lowers the cost and difficulty of digital warfare and advanced spyware.[273] All these technologies have been available since 2020 or earlier—AI facial recognition systems are already being used for mass surveillance in China.[274][275]
.There many other ways that AI is expected to help bad actors, some of which can not be foreseen. For example, machine-learning AI is able to design tens of thousands of toxic molecules in a matter of hours.[276]
.Economists have frequently highlighted the risks of redundancies from AI, and speculated about unemployment if there is no adequate social policy for full employment.[277]
.In the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that "we're in uncharted territory" with AI.[278] A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment, but they generally agree that it could be a net benefit if productivity gains are redistributed.[279] Risk estimates vary; for example, in the 2010s, Michael Osborne and Carl Benedikt Frey estimated 47% of U.S. jobs are at "high risk" of potential automation, while an OECD report classified only 9% of U.S. jobs as "high risk".[p][281] The methodology of speculating about future employment levels has been criticised as lacking evidential foundation, and for implying that technology, rather than social policy, creates unemployment, as opposed to redundancies.[277] In April 2023, it was reported that 70% of the jobs for Chinese video game illustrators had been eliminated by generative artificial intelligence.[282][283]
.Unlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist stated in 2015 that "the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution" is "worth taking seriously".[284] Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy.[285]
.From the early days of the development of artificial intelligence, there have been arguments, for example, those put forward by Joseph Weizenbaum, about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculation and qualitative, value-based judgement.[286]
.It has been argued AI will become so powerful that humanity may irreversibly lose control of it. This could, as physicist Stephen Hawking stated, "spell the end of the human race".[287] This scenario has been common in science fiction, when a computer or robot suddenly develops a human-like "self-awareness" (or "sentience" or "consciousness") and becomes a malevolent character.[q] These sci-fi scenarios are misleading in several ways.
.First, AI does not require human-like sentience to be an existential risk. Modern AI programs are given specific goals and use learning and intelligence to achieve them. Philosopher Nick Bostrom argued that if one gives almost any goal to a sufficiently powerful AI, it may choose to destroy humanity to achieve it (he used the example of a paperclip factory manager).[289] Stuart Russell gives the example of household robot that tries to find a way to kill its owner to prevent it from being unplugged, reasoning that "you can't fetch the coffee if you're dead."[290] In order to be safe for humanity, a superintelligence would have to be genuinely aligned with humanity's morality and values so that it is "fundamentally on our side".[291]
.Second, Yuval Noah Harari argues that AI does not require a robot body or physical control to pose an existential risk. The essential parts of civilization are not physical. Things like ideologies, law, government, money and the economy are built on language; they exist because there are stories that billions of people believe. The current prevalence of misinformation suggests that an AI could use language to convince people to believe anything, even to take actions that are destructive.[292]
.The opinions amongst experts and industry insiders are mixed, with sizable fractions both concerned and unconcerned by risk from eventual superintelligent AI.[293] Personalities such as Stephen Hawking, Bill Gates, and Elon Musk,[294] as well as AI pioneers such as Yoshua Bengio, Stuart Russell, Demis Hassabis, and Sam Altman, have expressed concerns about existential risk from AI.
.In May 2023, Geoffrey Hinton announced his resignation from Google in order to be able to "freely speak out about the risks of AI" without "considering how this impacts Google".[295] He notably mentioned risks of an AI takeover,[296] and stressed that in order to avoid the worst outcomes, establishing safety guidelines will require cooperation among those competing in use of AI.[297]
.In 2023, many leading AI experts endorsed the joint statement that "Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war".[298]
.Some other researchers were more optimistic. AI pioneer Jürgen Schmidhuber did not sign the joint statement, emphasising that in 95% of all cases, AI research is about making "human lives longer and healthier and easier."[299] While the tools that are now being used to improve lives can also be used by bad actors, "they can also be used against the bad actors."[300][301] Andrew Ng also argued that "it's a mistake to fall for the doomsday hype on AI—and that regulators who do will only benefit vested interests."[302] Yann LeCun "scoffs at his peers' dystopian scenarios of supercharged misinformation and even, eventually, human extinction."[303] In the early 2010s, experts argued that the risks are too distant in the future to warrant research or that humans will be valuable from the perspective of a superintelligent machine.[304] However, after 2016, the study of current and future risks and possible solutions became a serious area of research.[305]
.Friendly AI are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans. Eliezer Yudkowsky, who coined the term, argues that developing friendly AI should be a higher research priority: it may require a large investment and it must be completed before AI becomes an existential risk.[306]
.Machines with intelligence have the potential to use their intelligence to make ethical decisions. The field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas.[307]
The field of machine ethics is also called computational morality,[307]
and was founded at an AAAI symposium in 2005.[308]
.Other approaches include Wendell Wallach's "artificial moral agents"[309] and Stuart J. Russell's three principles for developing provably beneficial machines.[310]
.Active organizations in the AI open-source community include Hugging Face,[311] Google,[312] EleutherAI and Meta.[313] Various AI models, such as Llama 2, Mistral or Stable Diffusion, have been made open-weight,[314][315] meaning that their architecture and trained parameters (the "weights") are publicly available. Open-weight models can be freely fine-tuned, which allows companies to specialize them with their own data and for their own use-case.[316] Open-weight models are useful for research and innovation but can also be misused. Since they can be fine-tuned, any built-in security measure, such as objecting to harmful requests, can be trained away until it becomes ineffective. Some researchers warn that future AI models may develop dangerous capabilities (such as the potential to drastically facilitate bioterrorism) and that once released on the Internet, they cannot be deleted everywhere if needed. They recommend pre-release audits and cost-benefit analyses.[317]
.Artificial Intelligence projects can be guided by ethical considerations during the design, development, and implementation of an AI system. An AI framework such as the Care and Act Framework, developed by the Alan Turing Institute and based on the SUM values, outlines four main ethical dimensions, defined as follows:[318][319]
.Other developments in ethical frameworks include those decided upon during the Asilomar Conference, the Montreal Declaration for Responsible AI, and the IEEE's Ethics of Autonomous Systems initiative, among others;[320] however, these principles are not without criticism, especially regards to the people chosen to contribute to these frameworks.[321]
.Promotion of the wellbeing of the people and communities that these technologies affect requires consideration of the social and ethical implications at all stages of AI system design, development and implementation, and collaboration between job roles such as data scientists, product managers, data engineers, domain experts, and delivery managers.[322]
.The UK AI Safety Institute released in 2024 a testing toolset called 'Inspect' for AI safety evaluations available under a MIT open-source licence which is freely available on GitHub and can be improved with third-party packages. It can be used to evaluate AI models in a range of areas including core knowledge, ability to reason, and autonomous capabilities.[323]
.The regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating AI; it is therefore related to the broader regulation of algorithms.[324] The regulatory and policy landscape for AI is an emerging issue in jurisdictions globally.[325] According to AI Index at Stanford, the annual number of AI-related laws passed in the 127 survey countries jumped from one passed in 2016 to 37 passed in 2022 alone.[326][327] Between 2016 and 2020, more than 30 countries adopted dedicated strategies for AI.[328] Most EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, U.S., and Vietnam. Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia.[328] The Global Partnership on Artificial Intelligence was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology.[328] Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher published a joint statement in November 2021 calling for a government commission to regulate AI.[329] In 2023, OpenAI leaders published recommendations for the governance of superintelligence, which they believe may happen in less than 10 years.[330] In 2023, the United Nations also launched an advisory body to provide recommendations on AI governance; the body comprises technology company executives, governments officials and academics.[331] In 2024, the Council of Europe created the first international legally binding treaty on AI, called the "Framework Convention on Artificial Intelligence and Human Rights, Democracy and the Rule of Law". It was adopted by the European Union, the United States, the United Kingdom, and other signatories.[332]
.In a 2022 Ipsos survey, attitudes towards AI varied greatly by country; 78% of Chinese citizens, but only 35% of Americans, agreed that "products and services using AI have more benefits than drawbacks".[326] A 2023 Reuters/Ipsos poll found that 61% of Americans agree, and 22% disagree, that AI poses risks to humanity.[333] In a 2023 Fox News poll, 35% of Americans thought it "very important", and an additional 41% thought it "somewhat important", for the federal government to regulate AI, versus 13% responding "not very important" and 8% responding "not at all important".[334][335]
.In November 2023, the first global AI Safety Summit was held in Bletchley Park in the UK to discuss the near and far term risks of AI and the possibility of mandatory and voluntary regulatory frameworks.[336] 28 countries including the United States, China, and the European Union issued a declaration at the start of the summit, calling for international co-operation to manage the challenges and risks of artificial intelligence.[337][338] In May 2024 at the AI Seoul Summit, 16 global AI tech companies agreed to safety commitments on the development of AI.[339][340]
.The study of mechanical or "formal" reasoning began with philosophers and mathematicians in antiquity. The study of logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as "0" and "1", could simulate any conceivable form of mathematical reasoning.[342][343] This, along with concurrent discoveries in cybernetics, information theory and neurobiology, led researchers to consider the possibility of building an "electronic brain".[r] They developed several areas of research that would become part of AI,[345] such as McCullouch and Pitts design for "artificial neurons" in 1943,[116] and Turing's influential 1950 paper 'Computing Machinery and Intelligence', which introduced the Turing test and showed that "machine intelligence" was plausible.[346][343]
.The field of AI research was founded at a workshop at Dartmouth College in 1956.[s][6] The attendees became the leaders of AI research in the 1960s.[t] They and their students produced programs that the press described as "astonishing":[u] computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English.[v][7] Artificial intelligence laboratories were set up at a number of British and U.S. universities in the latter 1950s and early 1960s.[343]
.Researchers in the 1960s and the 1970s were convinced that their methods would eventually succeed in creating a machine with general intelligence and considered this the goal of their field.[350] In 1965 Herbert Simon predicted, "machines will be capable, within twenty years, of doing any work a man can do".[351] In 1967 Marvin Minsky agreed, writing that "within a generation ... the problem of creating 'artificial intelligence' will substantially be solved".[352] They had, however, underestimated the difficulty of the problem.[w] In 1974, both the U.S. and British governments cut off exploratory research in response to the criticism of Sir James Lighthill[354] and ongoing pressure from the U.S. Congress to fund more productive projects.[355] Minsky's and Papert's book Perceptrons was understood as proving that artificial neural networks would never be useful for solving real-world tasks, thus discrediting the approach altogether.[356] The "AI winter", a period when obtaining funding for AI projects was difficult, followed.[9]
.In the early 1980s, AI research was revived by the commercial success of expert systems,[357] a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S. and British governments to restore funding for academic research.[8] However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.[10]
.Up to this point, most of AI's funding had gone to projects that used high-level symbols to represent mental objects like plans, goals, beliefs, and known facts. In the 1980s, some researchers began to doubt that this approach would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition,[358] and began to look into "sub-symbolic" approaches.[359] Rodney Brooks rejected "representation" in general and focussed directly on engineering machines that move and survive.[x] Judea Pearl, Lofti Zadeh, and others developed methods that handled incomplete and uncertain information by making reasonable guesses rather than precise logic.[87][364] But the most important development was the revival of "connectionism", including neural network research, by Geoffrey Hinton and others.[365] In 1990, Yann LeCun successfully showed that convolutional neural networks can recognize handwritten digits, the first of many successful applications of neural networks.[366]
.AI gradually restored its reputation in the late 1990s and early 21st century by exploiting formal mathematical methods and by finding specific solutions to specific problems. This "narrow" and "formal" focus allowed researchers to produce verifiable results and collaborate with other fields (such as statistics, economics and mathematics).[367] By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as "artificial intelligence" (a tendency known as the AI effect).[368]
However, several academic researchers became concerned that AI was no longer pursuing its original goal of creating versatile, fully intelligent machines. Beginning around 2002, they founded the subfield of artificial general intelligence (or "AGI"), which had several well-funded institutions by the 2010s.[68]
.Deep learning began to dominate industry benchmarks in 2012 and was adopted throughout the field.[11]
For many specific tasks, other methods were abandoned.[y]
Deep learning's success was based on both hardware improvements (faster computers,[370] graphics processing units, cloud computing[371]) and access to large amounts of data[372] (including curated datasets,[371] such as ImageNet). Deep learning's success led to an enormous increase in interest and funding in AI.[z] The amount of machine learning research (measured by total publications) increased by 50% in the years 2015–2019.[328]
.In 2016, issues of fairness and the misuse of technology were catapulted into center stage at machine learning conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. The alignment problem became a serious field of academic study.[305]
.In the late 2010s and early 2020s, AGI companies began to deliver programs that created enormous interest. In 2015, AlphaGo, developed by DeepMind, beat the world champion Go player. The program taught only the game's rules and developed a strategy by itself. GPT-3 is a large language model that was released in 2020 by OpenAI and is capable of generating high-quality human-like text.[373] ChatGPT, launched on November 30, 2022, became the fastest-growing consumer software application in history, gaining over 100 million users in two months.[374] It marked what is widely regarded as AI's breakout year, bringing it into the public consciousness.[375] These programs, and others, inspired an aggressive AI boom, where large companies began investing billions of dollars in AI research. According to AI Impacts, about $50 billion annually was invested in "AI" around 2022 in the U.S. alone and about 20% of the new U.S. Computer Science PhD graduates have specialized in "AI".[376] About 800,000 "AI"-related U.S. job openings existed in 2022.[377] According to PitchBook research, 22% of newly funded startups in 2024 claimed to be AI companies.[378]
.Philosophical debates have historically sought to determine the nature of intelligence and how to make intelligent machines.[379] Another major focus has been whether machines can be conscious, and the associated ethical implications.[380] Many other topics in philosophy are relevant to AI, such as epistemology and free will.[381] Rapid advancements have intensified public discussions on the philosophy and ethics of AI.[380]
.Alan Turing wrote in 1950 "I propose to consider the question 'can machines think'?"[382] He advised changing the question from whether a machine "thinks", to "whether or not it is possible for machinery to show intelligent behaviour".[382] He devised the Turing test, which measures the ability of a machine to simulate human conversation.[346] Since we can only observe the behavior of the machine, it does not matter if it is "actually" thinking or literally has a "mind". Turing notes that we can not determine these things about other people but "it is usual to have a polite convention that everyone thinks."[383]
.Russell and Norvig agree with Turing that intelligence must be defined in terms of external behavior, not internal structure.[1] However, they are critical that the test requires the machine to imitate humans. "Aeronautical engineering texts", they wrote, "do not define the goal of their field as making 'machines that fly so exactly like pigeons that they can fool other pigeons.'"[385] AI founder John McCarthy agreed, writing that "Artificial intelligence is not, by definition, simulation of human intelligence".[386]
.McCarthy defines intelligence as "the computational part of the ability to achieve goals in the world".[387] Another AI founder, Marvin Minsky, similarly describes it as "the ability to solve hard problems".[388] The leading AI textbook defines it as the study of agents that perceive their environment and take actions that maximize their chances of achieving defined goals.[1] These definitions view intelligence in terms of well-defined problems with well-defined solutions, where both the difficulty of the problem and the performance of the program are direct measures of the "intelligence" of the machine—and no other philosophical discussion is required, or may not even be possible.
.Another definition has been adopted by Google,[389] a major practitioner in the field of AI. This definition stipulates the ability of systems to synthesize information as the manifestation of intelligence, similar to the way it is defined in biological intelligence.
.Some authors have suggested in practice, that the definition of AI is vague and difficult to define, with contention as to whether classical algorithms should be categorised as AI,[390] with many companies during the early 2020s AI boom using the term as a marketing buzzword, often even if they did "not actually use AI in a material way".[391]
.No established unifying theory or paradigm has guided AI research for most of its history.[aa] The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term "artificial intelligence" to mean "machine learning with neural networks"). This approach is mostly sub-symbolic, soft and narrow. Critics argue that these questions may have to be revisited by future generations of AI researchers.
.Symbolic AI (or "GOFAI")[393] simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at "intelligent" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the physical symbol systems hypothesis: "A physical symbol system has the necessary and sufficient means of general intelligent action."[394]
.However, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning. Moravec's paradox is the discovery that high-level "intelligent" tasks were easy for AI, but low level "instinctive" tasks were extremely difficult.[395] Philosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a "feel" for the situation, rather than explicit symbolic knowledge.[396] Although his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree with him.[ab][16]
.The issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. Critics such as Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence,[398][399] in part because sub-symbolic AI is a move away from explainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision. The emerging field of neuro-symbolic artificial intelligence attempts to bridge the two approaches.
."Neats" hope that intelligent behavior is described using simple, elegant principles (such as logic, optimization, or neural networks). "Scruffies" expect that it necessarily requires solving a large number of unrelated problems. Neats defend their programs with theoretical rigor, scruffies rely mainly on incremental testing to see if they work. This issue was actively discussed in the 1970s and 1980s,[400] but eventually was seen as irrelevant. Modern AI has elements of both.
.Finding a provably correct or optimal solution is intractable for many important problems.[15] Soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 1980s and most successful AI programs in the 21st century are examples of soft computing with neural networks.
.AI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence directly or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field's long-term goals.[401][402] General intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focusing on specific problems with specific solutions. The sub-field of artificial general intelligence studies this area exclusively.
.There is no settled consensus in philosophy of mind on whether a machine can have a mind, consciousness and mental states in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field: to build machines that can solve problems using intelligence. Russell and Norvig add that "[t]he additional project of making a machine conscious in exactly the way humans are is not one that we are equipped to take on."[403] However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction.
.David Chalmers identified two problems in understanding the mind, which he named the "hard" and "easy" problems of consciousness.[404] The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett's consciousness illusionism says this is an illusion). While human information processing is easy to explain, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person to know what red looks like.[405]
.Computationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind–body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.[406]
.Philosopher John Searle characterized this position as "strong AI": "The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds."[ac] Searle challenges this claim with his Chinese room argument, which attempts to show that even a computer capable of perfectly simulating human behavior would not have a mind.[410]
.It is difficult or impossible to reliably evaluate whether an advanced AI is sentient (has the ability to feel), and if so, to what degree.[411] But if there is a significant chance that a given machine can feel and suffer, then it may be entitled to certain rights or welfare protection measures, similarly to animals.[412][413] Sapience (a set of capacities related to high intelligence, such as discernment or self-awareness) may provide another moral basis for AI rights.[412] Robot rights are also sometimes proposed as a practical way to integrate autonomous agents into society.[414]
.In 2017, the European Union considered granting "electronic personhood" to some of the most capable AI systems. Similarly to the legal status of companies, it would have conferred rights but also responsibilities.[415] Critics argued in 2018 that granting rights to AI systems would downplay the importance of human rights, and that legislation should focus on user needs rather than speculative futuristic scenarios. They also noted that robots lacked the autonomy to take part to society on their own.[416][417]
.Progress in AI increased interest in the topic. Proponents of AI welfare and rights often argue that AI sentience, if it emerges, would be particularly easy to deny. They warn that this may be a moral blind spot analogous to slavery or factory farming, which could lead to large-scale suffering if sentient AI is created and carelessly exploited.[413][412]
.A superintelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind.[402] If research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to what I. J. Good called an "intelligence explosion" and Vernor Vinge called a "singularity".[418]
.However, technologies cannot improve exponentially indefinitely, and typically follow an S-shaped curve, slowing when they reach the physical limits of what the technology can do.[419]
.Robot designer Hans Moravec, cyberneticist Kevin Warwick and inventor Ray Kurzweil have predicted that humans and machines may merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, has roots in the writings of Aldous Huxley and Robert Ettinger.[420]
.Edward Fredkin argues that "artificial intelligence is the next step in evolution", an idea first proposed by Samuel Butler's "Darwin among the Machines" as far back as 1863, and expanded upon by George Dyson in his 1998 book Darwin Among the Machines: The Evolution of Global Intelligence.[421]
.Thought-capable artificial beings have appeared as storytelling devices since antiquity,[422] and have been a persistent theme in science fiction.[423]
.A common trope in these works began with Mary Shelley's Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.[424]
.Isaac Asimov introduced the Three Laws of Robotics in many stories, most notably with the "Multivac" super-intelligent computer. Asimov's laws are often brought up during lay discussions of machine ethics;[425] while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.[426]
.Several works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel Čapek's R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.[427]
.The two most widely used textbooks in 2023 (see the Open Syllabus):
.The four most widely used AI textbooks in 2008:
.Other textbooks:
.
.Finance refers to monetary resources and to the study and discipline of money, currency, assets and liabilities.[a] As a subject of study, it is related to but distinct from economics, which is the study of the production, distribution, and consumption of goods and services.[b] Based on the scope of financial activities in financial systems, the discipline can be divided into personal, corporate, and public finance.
.In these financial systems, assets are bought, sold, or traded as financial instruments, such as currencies, loans, bonds, shares, stocks, options, futures, etc. Assets can also be banked, invested, and insured to maximize value and minimize loss. In practice, risks are always present in any financial action and entities.
.Due to its wide scope, a broad range of subfields exists within finance. Asset-, money-, risk- and investment management aim to maximize value and minimize volatility. Financial analysis assesses the viability, stability, and profitability of an action or entity. Some fields are multidisciplinary, such as mathematical finance, financial law, financial economics, financial engineering and financial technology. These fields are the foundation of business and accounting. In some cases, theories in finance can be tested using the scientific method, covered by experimental finance.
.The early history of finance parallels the early history of money, which is prehistoric. Ancient and medieval civilizations incorporated basic functions of finance, such as banking, trading and accounting, into their economies. In the late 19th century, the global financial system was formed.
.In the middle of the 20th century, finance emerged as a distinct academic discipline,[c] separate from economics.[1] The earliest doctoral programs in finance were established in the 1960s and 1970s.[2] Today, finance is also widely studied through career-focused undergraduate and master's level programs.[3][4]
.As outlined, the financial system consists of the flows of capital that take place between individuals and households (personal finance), governments (public finance), and businesses (corporate finance). "Finance" thus studies the process of channeling money from savers and investors to entities that need it.[d] Savers and investors have money available which could earn interest or dividends if put to productive use. Individuals, companies and governments must obtain money from some external source, such as loans or credit, when they lack sufficient funds to run their operations.
.In general, an entity whose income exceeds its expenditure can lend or invest the excess, intending to earn a fair return. Correspondingly, an entity where income is less than expenditure can raise capital usually in one of two ways: (i) by borrowing in the form of a loan (private individuals), or by selling government or corporate bonds; (ii) by a corporation selling equity, also called stock or shares (which may take various forms: preferred stock or common stock). The owners of both bonds and stock may be institutional investors—financial institutions such as investment banks and pension funds—or private individuals, called private investors or retail investors. (See Financial market participants.)
.The lending is often indirect, through a financial intermediary such as a bank, or via the purchase of notes or bonds (corporate bonds, government bonds, or mutual bonds) in the bond market. The lender receives interest, the borrower pays a higher interest than the lender receives, and the financial intermediary earns the difference for arranging the loan.[6][7][8]
A bank aggregates the activities of many borrowers and lenders. Banks accept deposits from individuals and businesses, paying interest on these funds. The bank then lends these deposits to borrowers. Banks facilitate transactions between borrowers and lenders of various sizes, enabling efficient financial coordination.
.Investing typically entails the purchase of stock, either individual securities or via a mutual fund, for example. Stocks are usually sold by corporations to investors so as to raise required capital in the form of "equity financing", as distinct from the debt financing described above. The financial intermediaries here are the investment banks (which find the initial investors and facilitate the listing of the securities, typically shares and bonds), the securities exchanges (which allow their trade thereafter), and the various investment service providers (including mutual funds, pension funds, wealth managers, and stock brokers, typically servicing retail investors).
.Inter-institutional trade and investment, and fund-management at this scale, is referred to as "wholesale finance".
Institutions here extend the products offered, with related trading, to include bespoke options, swaps, and structured products, as well as specialized financing; this "financial engineering" is inherently mathematical, and these institutions are then the major employers of quantitative analysts (or "quants", see below). In these institutions, risk management, regulatory capital, and compliance play major roles.
.As outlined, finance comprises, broadly, the three areas of personal finance, corporate finance, and public finance. These, in turn, overlap and employ various activities and sub-disciplines—chiefly investments, risk management, and quantitative finance.
.Personal finance refers to the practice of budgeting to ensure enough funds are available to meet basic needs, while ensuring there is only a reasonable level of risk to lose said capital. Personal finance may involve paying for education, financing durable goods such as real estate and cars, buying insurance, investing, and saving for retirement.[9] Personal finance may also involve paying for a loan or other debt obligations. The main areas of personal finance are considered to be income, spending, saving, investing, and protection. The following steps, as outlined by the Financial Planning Standards Board,[10] suggest that an individual will understand a potentially secure personal finance plan after:
.Corporate finance deals with the actions that managers take to increase the value of the firm to the shareholders, the sources of funding and the capital structure of corporations, and the tools and analysis used to allocate financial resources. While corporate finance is in principle different from managerial finance, which studies the financial management of all firms rather than corporations alone, the concepts are applicable to the financial problems of all firms,[12] and this area is then often referred to as "business finance".
.Typically, "corporate finance" relates to the long term objective of maximizing the value of the entity's assets, its stock, and its return to shareholders, while also balancing risk and profitability. This entails[13] three primary areas:
.The latter creates the link with investment banking and securities trading, as above, in that the capital raised will generically comprise debt, i.e. corporate bonds, and equity, often listed shares. Re risk management within corporates, see below.
.Financial managers—i.e. as distinct from corporate financiers—focus more on the short term elements of profitability, cash flow, and "working capital management" (inventory, credit and debtors), which is concerned about the daily funding operations, and the goal is to maintain liquidity, minimize risk and maximize efficiency ensuring that the firm can safely and profitably carry out its financial and operational objectives; i.e. that it: (1) can service both maturing short-term debt repayments, and scheduled long-term debt payments, and (2) has sufficient cash flow for ongoing and upcoming operational expenses. (See Financial management and FP&A.) 
.Public finance describes finance as related to sovereign states, sub-national entities, and related public entities or agencies. It generally encompasses a long-term strategic perspective regarding investment decisions that affect public entities.[15] These long-term strategic periods typically encompass five or more years.[16] Public finance is primarily concerned with:[17]
.Central banks, such as the Federal Reserve System banks in the United States and the Bank of England in the United Kingdom, are strong players in public finance. They act as lenders of last resort as well as strong influences on monetary and credit conditions in the economy.[18]
.Development finance, which is related, concerns investment in economic development projects provided by a (quasi) governmental institution on a non-commercial basis; these projects would otherwise not be able to get financing. A public–private partnership is primarily used for infrastructure projects: a private sector corporate provides the financing up-front, and then draws profits from taxpayers or users.
Climate finance, and the related Environmental finance, address the financial strategies, resources and instruments used in climate change mitigation.
.Investment management[12] is the professional asset management of various securities—typically shares and bonds, but also other assets, such as real estate, commodities and alternative investments—in order to meet specified investment goals for the benefit of investors.
.As above, investors may be institutions, such as insurance companies, pension funds, corporations, charities, educational establishments, or private investors, either directly via investment contracts or, more commonly, via collective investment schemes like mutual funds, exchange-traded funds, or real estate investment trusts.
.At the heart of investment management[12] is asset allocation—diversifying the exposure among these asset classes, and among individual securities within each asset class—as appropriate to the client's investment policy, in turn, a function of risk profile, investment goals, and investment horizon (see Investor profile). Here:
.Overlaid is the portfolio manager's investment style—broadly, active vs passive, value vs growth, and small cap vs. large cap—and investment strategy.
.In a well-diversified portfolio, achieved investment performance will, in general, largely be a function of the asset mix selected, while the individual securities are less impactful. The specific approach or philosophy will also be significant, depending on the extent to which it is complementary with the market cycle.
.Additional to this diversification, the fundamental risk mitigant employed, investment managers will apply various hedging techniques as appropriate,[12] these may relate to the portfolio as a whole or to individual stocks. Bond portfolios are often (instead) managed via cash flow matching or immunization, while for derivative portfolios and positions, traders use "the Greeks" to measure and then offset sensitivities. In parallel, managers – active and passive – will monitor tracking error, thereby minimizing and preempting any underperformance vs their "benchmark".
.A quantitative fund is managed using computer-based mathematical techniques (increasingly, machine learning) instead of human judgment. The actual trading is typically automated via sophisticated algorithms.
.Risk management, in general, is the study of how to control risks and balance the possibility of gains; it is the process of measuring risk and then developing and implementing strategies to manage that risk.
Financial risk management[20][21] is the practice of protecting corporate value against financial risks, often by "hedging" exposure to these using financial instruments. The focus is particularly on credit and market risk, and in banks, through regulatory capital, includes operational risk.
.Financial risk management is related to corporate finance[12] in two ways. Firstly, firm exposure to market risk is a direct result of previous capital investments and funding decisions; while credit risk arises from the business's credit policy and is often addressed through credit insurance and provisioning.  Secondly, both disciplines share the goal of enhancing or at least preserving, the firm's economic value, and in this context[22] overlaps also enterprise risk management, typically the domain of strategic management.  Here, businesses devote much time and effort to forecasting, analytics and performance monitoring. (See ALM and treasury management.)
.For banks and other wholesale institutions,[23] risk management focuses on managing, and as necessary hedging, the various positions held by the institution—both trading positions and long term exposures—and on calculating and monitoring the resultant economic capital, and regulatory capital under Basel III. The calculations here are mathematically sophisticated, and within the domain of quantitative finance as below. Credit risk is inherent in the business of banking, but additionally, these institutions are exposed to counterparty credit risk. Banks typically employ Middle office "Risk Groups", whereas front office risk teams provide risk "services" (or "solutions") to customers.
.Insurers[24] manage their own risks with a focus on solvency and the ability to pay claims: Life Insurers are concerned more with longevity risk and interest rate risk; Short-Term Insurers (Property, Health,Casualty) emphasize catastrophe- and claims volatility risks. For expected claims reserves are set aside periodically, while to absorb unexpected losses, a minimum level of capital is maintained.
.Quantitative finance—also referred to as "mathematical finance"—includes those finance activities where a sophisticated mathematical model is required,[25] and thus overlaps several of the above.
.As a specialized practice area, quantitative finance comprises primarily three sub-disciplines; the underlying theory and techniques are discussed in the next section:
.DCF valuation formula widely applied in business and finance, since articulated in 1938. Here, to get the value of the firm, its forecasted free cash flows are discounted to the present using the weighted average cost of capital for the discount factor.
For share valuation investors use the related dividend discount model.
.Financial theory is studied and developed within the disciplines of management, (financial) economics, accountancy and applied mathematics.
In the abstract,[12][26] finance is concerned with the investment and deployment of assets and liabilities over "space and time"; i.e., it is about performing valuation and asset allocation today, based on the risk and uncertainty of future outcomes while appropriately incorporating the time value of money.
Determining the present value of these future values, "discounting", must be at the risk-appropriate discount rate, in turn, a major focus of finance-theory.[27]As financial theory has roots in many disciplines, including mathematics, statistics, economics, physics, and psychology, it can be considered a mix of an art and science,[1] and there are ongoing related efforts to organize a list of unsolved problems in finance.
.Managerial finance[29]
is the branch of finance that deals with the financial aspects of the management of a company, and the financial dimension of managerial decision-making more broadly. It provides the theoretical underpin for the practice described above, concerning itself with the managerial application of the various finance techniques. Academics working in this area are typically based in business school finance departments, in accounting, or in management science.
.The tools addressed and developed relate in the main to managerial accounting and corporate finance:
the former allow management to better understand, and hence act on, financial information relating to profitability and performance; the latter, as above, are about optimizing the overall financial structure, including its impact on working capital. Key aspects of managerial finance thus include:
.The discussion, however, extends to business strategy more broadly, emphasizing alignment with the company's overall strategic objectives; and similarly incorporates the managerial perspectives of planning, directing, and controlling.
.Financial economics[31] is the branch of economics that studies the interrelation of financial variables, such as prices, interest rates and shares, as opposed to real economic variables, i.e. goods and services. It thus centers on pricing, decision making, and risk management in the financial markets,[31][26] and produces many of the commonly employed financial models. (Financial econometrics is the branch of financial economics that uses econometric techniques to parameterize the relationships suggested.)
.The discipline has two main areas of focus:[26] asset pricing and corporate finance; the first being the perspective of providers of capital, i.e. investors, and the second of users of capital; respectively:
.Financial mathematics[33] is the field of applied mathematics concerned with financial markets;
Louis Bachelier's doctoral thesis, defended in 1900, is considered to be the first scholarly work in this area. The field is largely focused on the modeling of derivatives—with much emphasis on interest rate- and credit risk modeling—while other important areas include insurance mathematics and quantitative portfolio management. Relatedly, the techniques developed are applied to pricing and hedging a wide range of asset-backed, government, and corporate-securities.
.As above, in terms of practice, the field is referred to as quantitative finance and / or mathematical finance, and comprises primarily the three areas discussed.
The main mathematical tools and techniques are, correspondingly:
.Mathematically, these separate into two analytic branches: derivatives pricing uses risk-neutral probability (or arbitrage-pricing probability), denoted by "Q"; while risk and portfolio management generally use physical (or actual or actuarial) probability, denoted by "P". These are interrelated through the above "Fundamental theorem of asset pricing".
.The subject has a close relationship with financial economics, which, as outlined, is concerned with much of the underlying theory that is involved in financial mathematics:  generally, financial mathematics will derive and extend the mathematical models suggested. Computational finance is the branch of (applied) computer science that deals with problems of practical interest in finance, and especially[33] emphasizes the numerical methods applied here.
.Experimental finance[36] aims to establish different market settings and environments to experimentally observe and provide a lens through which science can analyze agents' behavior and the resulting characteristics of trading flows, information diffusion, and aggregation, price setting mechanisms, and returns processes. Researchers in experimental finance can study to what extent existing financial economics theory makes valid predictions and therefore prove them, as well as attempt to discover new principles on which such theory can be extended and be applied to future financial decisions. Research may proceed by conducting trading simulations or by establishing and studying the behavior of people in artificial, competitive, market-like settings.
.Behavioral finance studies how the psychology of investors or managers affects financial decisions and markets[37]
and is relevant when making a decision that can impact either negatively or positively on one of their areas. With more in-depth research into behavioral finance, it is possible to bridge what actually happens in financial markets with analysis based on financial theory.[38] Behavioral finance has grown over the last few decades to become an integral aspect of finance. Nowadays there is a need for more theory and testing of the effects of feelings on financial decisions. Especially, because now the time has come to move beyond behavioral finance to social finance, which studies the structure of social interactions, how financial ideas spread, and how social processes affect financial decisions and outcomes.[39][40]
.Behavioral finance includes such topics as:
.A strand of behavioral finance has been dubbed quantitative behavioral finance, which uses mathematical and statistical methodology to understand behavioral biases in conjunction with valuation.
.Quantum finance involves applying quantum mechanical approaches to financial theory, providing novel methods and perspectives in the field.[41] Quantum finance is an interdisciplinary field, in which theories and methods developed by quantum physicists and economists are applied to solve financial problems. It represents a branch known as econophysics. Although quantum computational methods have been around for quite some time and use the basic principles of physics to better understand the ways to implement and manage cash flows, it is mathematics that is actually important in this new scenario[42] Finance theory is heavily based on financial instrument pricing such as stock option pricing. Many of the problems facing the finance community have no known analytical solution. As a result, numerical methods and computer simulations for solving these problems have proliferated. This research area is known as computational finance. Many computational finance problems have a high degree of computational complexity and are slow to converge to a solution on classical computers. In particular, when it comes to option pricing, there is additional complexity resulting from the need to respond to quickly changing markets. For example, in order to take advantage of inaccurately priced stock options, the computation must complete before the next change in the almost continuously changing stock market. As a result, the finance community is always looking for ways to overcome the resulting performance issues that arise when pricing options. This has led to research that applies alternative computing techniques to finance. Most commonly used quantum financial models are quantum continuous model, quantum binomial model, multi-step quantum binomial model etc.
.The origin of finance can be traced to the beginning of state formation and trade during the Bronze Age. The earliest historical evidence of finance is dated to around 3000 BCE. Banking originated in West Asia, where temples and palaces were used as safe places for the storage of valuables. Initially, the only valuable that could be deposited was grain, but cattle and precious materials were eventually included. During the same period, the Sumerian city of Uruk in Mesopotamia supported trade by lending as well as the use of interest. In Sumerian, "interest" was mas, which translates to "calf". In Greece and Egypt, the words used for interest, tokos and ms respectively, meant "to give birth". In these cultures, interest indicated a valuable increase, and seemed to consider it from the lender's point of view.[43] The Code of Hammurabi (1792–1750 BCE) included laws governing banking operations. The Babylonians were accustomed to charging interest at the rate of 20 percent per year. By 1200 BCE, cowrie shells were used as a form of money in China.
.The use of coins as a means of representing money began in the years between 700 and 500 BCE.[44] Herodotus mentions the use of crude coins in Lydia around 687 BCE and, by 640 BCE, the Lydians had started to use coin money more widely and opened permanent retail shops.[45] Shortly after, cities in Classical Greece, such as Aegina, Athens, and Corinth, started minting their own coins between 595 and 570 BCE. During the Roman Republic, interest was outlawed by the Lex Genucia reforms in 342 BCE, though the provision went largely unenforced. Under Julius Caesar, a ceiling on interest rates of 12% was set, and much later under Justinian it was lowered even further to between 4% and 8%.[46]
.The first stock exchange was opened in Antwerp in 1531.[47] Since then, popular exchanges such as the London Stock Exchange (founded in 1773) and the New York Stock Exchange (founded in 1793) were created.[48][49]
.
.
.A cryptocurrency (colloquially crypto) is a digital currency designed to work through a computer network that is not reliant on any central authority, such as a government or bank, to uphold or maintain it.[2]
.Individual coin ownership records are stored in a digital ledger or blockchain, which is a computerized database that uses a consensus mechanism to secure transaction records, control the creation of additional coins, and verify the transfer of coin ownership.[3][4][5] The two most common consensus mechanisms are proof of work and proof of stake.[6] Despite the name, which has come to describe many of the fungible blockchain tokens that have been created, cryptocurrencies are not considered to be currencies in the traditional sense, and varying legal treatments have been applied to them in various jurisdictions, including classification as commodities, securities, and currencies. Cryptocurrencies are generally viewed as a distinct asset class in practice.[7][8][9]
.The first cryptocurrency was bitcoin, which was first released as open-source software in 2009. As of June 2023, there were more than 25,000 other cryptocurrencies in the marketplace, of which more than 40 had a market capitalization exceeding $1 billion.[10] As of April 2025, the cryptocurrency market capitalization was already estimated at $2.76 trillion.[11]
.In 1983, American cryptographer David Chaum conceived of a type of cryptographic electronic money called ecash.[12][13] Later, in 1995, he implemented it through Digicash,[14] an early form of cryptographic electronic payments. Digicash required user software in order to withdraw notes from a bank and designate specific encrypted keys before they could be sent to a recipient. This allowed the digital currency to be untraceable by a third party.
.In 1996, the National Security Agency published a paper entitled How to Make a Mint: The Cryptography of Anonymous Electronic Cash, describing a cryptocurrency system. The paper was first published in an MIT mailing list (October 1996) and later (April 1997) in The American Law Review.[15]
.In 1998, Wei Dai described "b-money," an anonymous, distributed electronic cash system.[16] Shortly thereafter, Nick Szabo described bit gold.[17] Like bitcoin and other cryptocurrencies that would follow it, bit gold (not to be confused with the later gold-based exchange BitGold) was described as an electronic currency system that required users to complete a proof of work function with solutions being cryptographically put together and published.
.In January 2009, bitcoin was created by pseudonymous developer Satoshi Nakamoto. It used SHA-256, a cryptographic hash function, in its proof-of-work scheme.[18][19] In April 2011, Namecoin was created as an attempt at forming a decentralized DNS. In October 2011, Litecoin was released, which used scrypt as its hash function instead of SHA-256. Peercoin, created in August 2012, used a hybrid of proof-of-work and proof-of-stake.[20]
.Cryptocurrency has undergone several periods of growth and retraction, including several bubbles and market crashes, such as in 2011, 2013–2014/15, 2017–2018, and 2021–2023.[21][22]
.On 6 August 2014, the UK announced its Treasury had commissioned a study of cryptocurrencies and what role, if any, they could play in the UK economy. The study was also to report on whether regulation should be considered.[23] Its final report was published in 2018,[24] and it issued a consultation on cryptoassets and stablecoins in January 2021.[25]
.In June 2021, El Salvador became the first country to accept bitcoin as legal tender, after the Legislative Assembly had voted 62–22 to pass a bill submitted by President Nayib Bukele classifying the cryptocurrency as such.[26]
.In August 2021, Cuba followed with Resolution 215 to recognize and regulate cryptocurrencies such as bitcoin.[27]
.In September 2021, the government of China, the single largest market for cryptocurrency, declared all cryptocurrency transactions illegal. This completed a crackdown on cryptocurrency that had previously banned the operation of intermediaries and miners within China.[28]
.On 15 September 2022, the world's second largest cryptocurrency at that time, Ethereum, transitioned its consensus mechanism from proof-of-work (PoW) to proof-of-stake (PoS) in an upgrade process known as "the Merge".  According to the Ethereum Founder, the upgrade would cut both Ethereum's energy use and carbon-dioxide emissions by 99.9%.[29]
.On 11 November 2022, FTX Trading Ltd., a cryptocurrency exchange, which also operated a crypto hedge fund, and had been valued at $18 billion,[30] filed for bankruptcy.[31] The financial impact of the collapse extended beyond the immediate FTX customer base, as reported,[32] while, at a Reuters conference, financial industry executives said that "regulators must step in to protect crypto investors."[33] Technology analyst Avivah Litan commented on the cryptocurrency ecosystem that "everything...needs to improve dramatically in terms of user experience, controls, safety, customer service."[34]
.According to Jan Lansky, a cryptocurrency is a system that meets six conditions:[35]
.In March 2018, the word cryptocurrency was added to the Merriam-Webster Dictionary.[36]
.After the early innovation of bitcoin in 2008 and the early network effect gained by bitcoin, tokens, cryptocurrencies, and other digital assets that were not bitcoin became collectively known during the 2010s as alternative cryptocurrencies,[37][38][39] or "altcoins".[40]
Sometimes the term "alt coins" was used,[41][42] or disparagingly, "shitcoins".[43] Paul Vigna of The Wall Street Journal described altcoins in 2020 as "alternative versions of Bitcoin"[44] given its role as the model protocol for cryptocurrency designers. A Polytechnic University of Catalonia thesis in 2021 used a broader description, including not only alternative versions of bitcoin but every cryptocurrency other than bitcoin. As of early 2020, there were more than 5,000 cryptocurrencies.
. Altcoins often have underlying differences when compared to bitcoin. For example, Litecoin aims to process a block every 2.5 minutes, rather than bitcoin's 10 minutes which allows Litecoin to confirm transactions faster than bitcoin.[20] Another example is Ethereum, which has smart contract functionality that allows decentralized applications to be run on its blockchain.[45] Ethereum was the most used blockchain in 2020, according to Bloomberg News.[46] In 2016, it had the largest "following" of any altcoin, according to the New York Times.[47]
.Significant market price rallies across multiple altcoin markets are often referred to as an "altseason".[48][49]
.Stablecoins are cryptocurrencies designed to maintain a stable level of purchasing power.[50] Notably, these designs are not foolproof, as a number of stablecoins have crashed or lost their peg. For example, on 11 May 2022, Terra's stablecoin UST fell from $1 to 26 cents.[51][52] The subsequent failure of Terraform Labs resulted in the loss of nearly $40B invested in the Terra and Luna coins.[53] In September 2022, South Korean prosecutors requested the issuance of an Interpol Red Notice against the company's founder, Do Kwon.[54] In Hong Kong, the expected regulatory framework for stablecoins in 2023/24 is being shaped and includes a few considerations.[55]
.Memecoins are a category of cryptocurrencies that originated from Internet memes or jokes. The most notable example is Dogecoin, a memecoin featuring the Shiba Inu dog from the Doge meme.[56] Memecoins are known for extreme volatility; for example, the record-high value for a Dogecoin was 73 cents, but that had plunged to 13 cents by mid-2024.[56] Scams are prolific among memecoins.[56]
.Physical cryptocurrency coins have been made as promotional items and some have become collectibles.[57] Some of these have a private key embedded in them to access crypto worth a few dollars. There have also been attempts to issue bitcoin "bank notes".[58]
.The term "physical bitcoin" is used in the finance industry when investment funds that hold crypto purchased from crypto exchanges put their crypto holdings in a specialised bank called a "custodian".[59]
.These physical representations of cryptocurrency do not hold any value by themselves; these are only utilized for collectable purposes. For example, the first incarnation of the bitcoin Casascius, coins made of silver, brass or aluminum sometimes with gold plating, or Titan Bitcoin, which in silver or gold versions are sought after by numismatists.[60]
.Cryptocurrency is produced by an entire cryptocurrency system collectively, at a rate that is defined when the system is created and that is publicly stated. In centralized banking and economic systems such as the US Federal Reserve System, corporate boards or governments control the supply of currency.[citation needed] In the case of cryptocurrency, companies or governments cannot produce new units and have not so far provided backing for other firms, banks, or corporate entities that hold asset value measured in it. The underlying technical system upon which cryptocurrencies are based was created by Satoshi Nakamoto.[61]
.Within a proof-of-work system such as bitcoin, the safety, integrity, and balance of ledgers are maintained by a community of mutually distrustful parties referred to as miners. Miners use their computers to help validate and timestamp transactions, adding them to the ledger in accordance with a particular timestamping scheme.[18] In a proof-of-stake blockchain, transactions are validated by holders of the associated cryptocurrency, sometimes grouped together in stake pools.
.Most cryptocurrencies are designed to gradually decrease the production of that currency, placing a cap on the total amount of that currency that will ever be in circulation.[62] Compared with ordinary currencies held by financial institutions or kept as cash on hand, cryptocurrencies can be more difficult for seizure by law enforcement.[3]
.The validity of each cryptocurrency's coins is provided by a blockchain. A blockchain is a continuously growing list of records, called blocks, which are linked and secured using cryptography.[61][63] Each block typically contains a hash pointer as a link to a previous block,[63] a timestamp, and transaction data.[64] By design, blockchains are inherently resistant to modification of the data. A blockchain is "an open, distributed ledger that can record transactions between two parties efficiently and in a verifiable and permanent way".[65] For use as a distributed ledger, a blockchain is typically managed by a peer-to-peer network collectively adhering to a protocol for validating new blocks. Once recorded, the data in any given block cannot be altered retroactively without the alteration of all subsequent blocks, which requires collusion of the network majority.
.Blockchains are secure by design and are an example of a distributed computing system with high Byzantine fault tolerance. Decentralized consensus has therefore been achieved with a blockchain.[66]
.A node is a computer that connects to a cryptocurrency network. The node supports the cryptocurrency's network through either relaying transactions, validation, or hosting a copy of the blockchain. In terms of relaying transactions, each network computer (node) has a copy of the blockchain of the cryptocurrency it supports. When a transaction is made, the node creating the transaction broadcasts details of the transaction using encryption to other nodes throughout the node network so that the transaction (and every other transaction) is known.
.Node owners are either volunteers, those hosted by the organization or body responsible for developing the cryptocurrency blockchain network technology, or those who are enticed to host a node to receive rewards from hosting the node network.[67]
.Cryptocurrencies use various timestamping schemes to "prove" the validity of transactions added to the blockchain ledger without the need for a trusted third party.
.The first timestamping scheme invented was the proof-of-work scheme. The most widely used proof-of-work schemes are based on SHA-256 and scrypt.[20]
.Some other hashing algorithms that are used for proof-of-work include CryptoNote, Blake, SHA-3, and X11.
.Another method is called the proof-of-stake scheme. Proof-of-stake is a method of securing a cryptocurrency network and achieving distributed consensus through requesting users to show ownership of a certain amount of currency. It is different from proof-of-work systems that run difficult hashing algorithms to validate electronic transactions. The scheme is largely dependent on the coin, and there is currently no standard form of it. Some cryptocurrencies use a combined proof-of-work and proof-of-stake scheme.[20]
.On a blockchain, mining is the validation of transactions. For this effort, successful miners obtain new cryptocurrency as a reward. The reward decreases transaction fees by creating a complementary incentive to contribute to the processing power of the network. The rate of generating hashes, which validate any transaction, has been increased by the use of specialized hardware such as FPGAs and ASICs running complex hashing algorithms like SHA-256 and scrypt.[68] This arms race for cheaper-yet-efficient machines has existed since bitcoin was introduced in 2009.[68] Mining is measured by hash rate, typically in TH/s.[69] A 2023 IMF working paper found that crypto mining could generate 450 million tons of CO2 emissions by 2027, accounting for 0.7 percent of global emissions, or 1.2 percent of the world total[70]
.With more people entering the world of virtual currency, generating hashes for validation has become more complex over time, forcing miners to invest increasingly large sums of money to improve computing performance. Consequently, the reward for finding a hash has diminished and often does not justify the investment in equipment and cooling facilities (to mitigate the heat the equipment produces) and the electricity required to run them.[71] Popular regions for mining include those with inexpensive electricity, a cold climate, and jurisdictions with clear and conducive regulations. By July 2019, bitcoin's electricity consumption was estimated to be approximately 7 gigawatts, around 0.2% of the global total, or equivalent to the energy consumed nationally by Switzerland.[72]
.Some miners pool resources, sharing their processing power over a network to split the reward equally, according to the amount of work they contributed to the probability of finding a block. A "share" is awarded to members of the mining pool who present a valid partial proof-of-work.
.As of February 2018[update], the Chinese government has halted trading of virtual currency, banned initial coin offerings, and shut down mining. Many Chinese miners have since relocated to Canada[73] and Texas.[74] One company is operating data centers for mining operations at Canadian oil and gas field sites due to low gas prices.[75] In June 2018, Hydro Quebec proposed to the provincial government to allocate 500 megawatts of power to crypto companies for mining.[76] According to a February 2018 report from Fortune, Iceland has become a haven for cryptocurrency miners in part because of its cheap electricity.[77]
.In March 2018, the city of Plattsburgh, New York put an 18-month moratorium on all cryptocurrency mining in an effort to preserve natural resources and the "character and direction" of the city.[78] In 2021, Kazakhstan became the second-biggest crypto-currency mining country, producing 18.1% of the global exahash rate. The country built a compound containing 50,000 computers near Ekibastuz.[79]
.An increase in cryptocurrency mining increased the demand for graphics cards (GPU) in 2017.[80] The computing power of GPUs makes them well-suited to generating hashes. Popular favorites of cryptocurrency miners, such as Nvidia's GTX 1060 and GTX 1070 graphics cards, as well as AMD's RX 570 and RX 580 GPUs, doubled or tripled in price – or were out of stock.[81] A GTX 1070 Ti, which was released at a price of $450, sold for as much as $1,100. Another popular card, the GTX 1060 (6 GB model), was released at an MSRP of $250 and sold for almost $500. RX 570 and RX 580 cards from AMD were out of stock for almost a year. Miners regularly buy up the entire stock of new GPUs as soon as they are available.[82]
.Nvidia has asked retailers to do what they can when it comes to selling GPUs to gamers instead of miners. Boris Böhles, PR manager for Nvidia in the German region, said: "Gamers come first for Nvidia."[83]
.Numerous companies developed dedicated crypto-mining accelerator chips, capable of price-performance far higher than that of CPU or GPU mining. At one point, Intel marketed its own brand of crypto accelerator chip, named Blockscale.[84]
.A cryptocurrency wallet is a means of storing the public and private "keys" (address) or seed, which can be used to receive or spend the cryptocurrency.[85] With the private key, it is possible to write in the public ledger, effectively spending the associated cryptocurrency. With the public key, it is possible for others to send currency to the wallet.
.There exist multiple methods of storing keys or seed in a wallet. These methods range from using paper wallets (which are public, private, or seed keys written on paper), to using hardware wallets (which are hardware to store your wallet information), to a digital wallet (which is a computer with software hosting your wallet information), to hosting your wallet using an exchange where cryptocurrency is traded, or by storing your wallet information on a digital medium such as plaintext.[86]
.Bitcoin is pseudonymous, rather than anonymous; the cryptocurrency in a wallet is not tied to a person but rather to one or more specific keys (or "addresses").[87] Thereby, bitcoin owners are not immediately identifiable, but all transactions are publicly available in the blockchain.[88] Still, cryptocurrency exchanges are often required by law to collect the personal information of their users.[89]
.Some cryptocurrencies, such as Monero, Zerocoin, Zerocash, and CryptoNote, implement additional measures to increase privacy, such as by using zero-knowledge proofs.[90][91]
.A recent 2020 study presented different attacks on privacy in cryptocurrencies. The attacks demonstrated how the anonymity techniques are not sufficient safeguards. In order to improve privacy, researchers suggested several different ideas, including new cryptographic schemes and mechanisms for hiding the IP address of the source.[92]
.Cryptocurrencies are used primarily outside banking and governmental institutions and are exchanged over the Internet.
.Proof-of-work cryptocurrencies, such as bitcoin, offer block rewards incentives for miners. There has been an implicit belief that whether miners are paid by block rewards or transaction fees does not affect the security of the blockchain, but a study suggests that this may not be the case under certain circumstances.[93]
.The rewards paid to miners increase the supply of the cryptocurrency. By making sure that verifying transactions is a costly business, the integrity of the network can be preserved as long as benevolent nodes control a majority of computing power. The verification algorithm requires a lot of processing power, and thus electricity, in order to make verification costly enough to accurately validate the public blockchain. Not only do miners have to factor in the costs associated with expensive equipment necessary to stand a chance of solving a hash problem, they must further consider the significant amount of electrical power in search of the solution. Generally, the block rewards outweigh electricity and equipment costs, but this may not always be the case.[94]
.The current value, not the long-term value, of the cryptocurrency supports the reward scheme to incentivize miners to engage in costly mining activities.[95] In 2018, bitcoin's design caused a 1.4% welfare loss compared to an efficient cash system, while a cash system with 2% money growth has a minor 0.003% welfare cost. The main source for this inefficiency is the large mining cost, which is estimated to be US$360 million per year. This translates into users being willing to accept a cash system with an inflation rate of 230% before being better off using bitcoin as a means of payment. However, the efficiency of the bitcoin system can be significantly improved by optimizing the rate of coin creation and minimizing transaction fees. Another potential improvement is to eliminate inefficient mining activities by changing the consensus protocol altogether.[96]
.Transaction fees (sometimes also referred to as miner fees or gas fees) for cryptocurrency depend mainly on the supply of network capacity at the time, versus the demand from the currency holder for a faster transaction.[97] The ability for the holder to be allowed to set the fee manually often depends on the wallet software used, and central exchanges for cryptocurrency (CEX) usually do not allow the customer to set a custom transaction fee for the transaction.[citation needed] Their wallet software, such as Coinbase Wallet, however, might support adjusting the fee.[98]
.Select cryptocurrency exchanges have offered to let the user choose between different presets of transaction fee values during the currency conversion. One of those exchanges, namely LiteBit, previously headquartered in the Netherlands, was forced to cease all operations on August 13th, 2023, "due to market changes and regulatory pressure".[99]
.The "recommended fee" suggested by the network will often depend on the time of day (due to depending on network load).
.For Ethereum, transaction fees differ by computational complexity, bandwidth use, and storage needs, while bitcoin transaction fees differ by transaction size and whether the transaction uses SegWit. In February 2023, the median transaction fee for Ether corresponded to $2.2845,[100] while for bitcoin it corresponded to $0.659.[101]
.Some cryptocurrencies have no transaction fees, the most well-known example being Nano (XNO), and instead rely on client-side proof-of-work as the transaction prioritization and anti-spam mechanism.[102][103][104]
.Cryptocurrency exchanges allow customers to trade cryptocurrencies[105] for other assets, such as conventional fiat money, or to trade between different digital currencies.
.Crypto marketplaces do not guarantee that an investor is completing a purchase or trade at the optimal price. As a result, as of 2020, it was possible to arbitrage to find the difference in price across several markets.[106]
.Atomic swaps are a mechanism where one cryptocurrency can be exchanged directly for another cryptocurrency without the need for a trusted third party, such as an exchange.[107]
.Jordan Kelley, founder of Robocoin, launched the first bitcoin ATM in the United States on 20 February 2014. The kiosk installed in Austin, Texas, is similar to bank ATMs but has scanners to read government-issued identification such as a driver's license or a passport to confirm users' identities.[108]
.An initial coin offering (ICO) is a controversial means of raising funds for a new cryptocurrency venture. An ICO may be used by startups with the intention of avoiding regulation. However, securities regulators in many jurisdictions, including in the U.S. and Canada, have indicated that if a coin or token is an "investment contract" (e.g., under the Howey test, i.e., an investment of money with a reasonable expectation of profit based significantly on the entrepreneurial or managerial efforts of others), it is a security and is subject to securities regulation. In an ICO campaign, a percentage of the cryptocurrency (usually in the form of "tokens") is sold to early backers of the project in exchange for legal tender or other cryptocurrencies, often bitcoin or Ether.[109][110][111]
.According to PricewaterhouseCoopers, four of the 10 biggest proposed initial coin offerings have used Switzerland as a base, where they are frequently registered as non-profit foundations. The Swiss regulatory agency FINMA stated that it would take a "balanced approach" to ICO projects and would allow "legitimate innovators to navigate the regulatory landscape and so launch their projects in a way consistent with national laws protecting investors and the integrity of the financial system." In response to numerous requests by industry representatives, a legislative ICO working group began to issue legal guidelines in 2018, which are intended to remove uncertainty from cryptocurrency offerings and to establish sustainable business practices.[112]
.The market capitalization of a cryptocurrency is calculated by multiplying the price by the number of coins in circulation. The total cryptocurrency market cap has historically been dominated by bitcoin accounting for at least 50% of the market cap value where altcoins have increased and decreased in market cap value in relation to bitcoin. Bitcoin's value is largely determined by speculation among other technological limiting factors known as blockchain rewards coded into the architecture technology of bitcoin itself. The cryptocurrency market cap follows a trend known as the "halving", which is when the block rewards received from bitcoin are halved due to technological mandated limited factors instilled into bitcoin which in turn limits the supply of bitcoin. As the date reaches near of a halving (twice thus far historically) the cryptocurrency market cap increases, followed by a downtrend.[113]
.By June 2021, cryptocurrency had begun to be offered by some wealth managers in the US for 401(k)s.[114][115][116]
.Cryptocurrency prices are much more volatile than established financial assets such as stocks. For example, over one week in May 2022, bitcoin lost 20% of its value and Ethereum lost 26%, while Solana and Cardano lost 41% and 35% respectively. The falls were attributed to warnings about inflation. By comparison, in the same week, the Nasdaq tech stock index fell 7.6 per cent and the FTSE 100 was 3.6 per cent down.[117]
.In the longer term, of the 10 leading cryptocurrencies identified by the total value of coins in circulation in January 2018, only four (bitcoin, Ethereum, Cardano and Ripple (XRP)) were still in that position in early 2022.[118] The total value of all cryptocurrencies was  $2 trillion at the end of 2021, but had halved nine months later.[119][120] The Wall Street Journal has commented that the crypto sector has become "intertwined" with the rest of the capital markets and "sensitive to the same forces that drive tech stocks and other risk assets," such as inflation forecasts.[121]
.There are also centralized databases, outside of blockchains, that store crypto market data. Compared to the blockchain, databases perform fast as there is no verification process. Four of the most popular cryptocurrency market databases are CoinMarketCap, CoinGecko, BraveNewCoin, and Cryptocompare.[122]
.According to Alan Feuer of The New York Times, libertarians and anarcho-capitalists were attracted to the philosophical idea behind bitcoin. Early bitcoin supporter Roger Ver said: "At first, almost everyone who got involved did so for philosophical reasons. We saw bitcoin as a great idea, as a way to separate money from the state."[123] Economist Paul Krugman argues that cryptocurrencies like bitcoin are "something of a cult" based in "paranoid fantasies" of government power.[124]
.David Golumbia says that the ideas influencing bitcoin advocates emerge from right-wing extremist movements such as the Liberty Lobby and the John Birch Society and their anti-Central Bank rhetoric, or, more recently, Ron Paul and Tea Party-style libertarianism.[125] Steve Bannon, who owns a "good stake" in bitcoin, sees cryptocurrency as a form of disruptive populism, taking control back from central authorities.[126]
.Bitcoin's founder, Satoshi Nakamoto, supported the idea that cryptocurrencies go well with libertarianism. "It's very attractive to the libertarian viewpoint if we can explain it properly," Nakamoto said in 2008.[127]
.According to the European Central Bank, the decentralization of money offered by bitcoin has its theoretical roots in the Austrian school of economics, especially with Friedrich von Hayek in his book Denationalisation of Money: The Argument Refined,[128] in which Hayek advocates a complete free market in the production, distribution and management of money to end the monopoly of central banks.[129][130]
.The rise in the popularity of cryptocurrencies and their adoption by financial institutions has led some governments to assess whether regulation is needed to protect users. The Financial Action Task Force (FATF) has defined cryptocurrency-related services as "virtual asset service providers" (VASPs) and recommended that they be regulated with the same money laundering (AML) and know your customer (KYC) requirements as financial institutions.[131]
.In May 2020, the Joint Working Group on interVASP Messaging Standards published "IVMS 101", a universal common language for communication of required originator and beneficiary information between VASPs. The FATF and financial regulators were informed as the data model was developed.[132]
.In June 2020, FATF updated its guidance to include the "Travel Rule" for cryptocurrencies, a measure which mandates that VASPs obtain, hold, and exchange information about the originators and beneficiaries of virtual asset transfers.[133] Subsequent standardized protocol specifications recommended using JSON for relaying data between VASPs and identity services. As of December 2020, the IVMS 101 data model has yet to be finalized and ratified by the three global standard setting bodies that created it.[134]
.The European Commission published a digital finance strategy in September 2020. This included a draft regulation on Markets in Crypto-Assets (MiCA), which aimed to provide a comprehensive regulatory framework for digital assets in the EU.[135][136]
.On 10 June 2021, the Basel Committee on Banking Supervision proposed that banks that held cryptocurrency assets must set aside capital to cover all potential losses. For instance, if a bank were to hold bitcoin worth $2 billion, it would be required to set aside enough capital to cover the entire $2 billion. This is a more extreme standard than banks are usually held to when it comes to other assets. However, this is a proposal and not a regulation.
.The IMF is seeking a coordinated, consistent and comprehensive approach to supervising cryptocurrencies. Tobias Adrian, the IMF's financial counsellor and head of its monetary and capital markets department said in a January 2022 interview that "Agreeing global regulations is never quick. But if we start now, we can achieve the goal of maintaining financial stability while also enjoying the benefits which the underlying technological innovations bring,"[137]
.In May 2024, 15 years after the advent of the first blockchain, bitcoin, the US Congress advanced a bill to the full House of Representatives to provide regulatory clarity for digital assets. The Financial Innovation and Technology for the 21st Century Act, which defines responsibilities between various US agencies, notably between the Commodity Futures Trading Commission (CFTC) for decentralized blockchains and the Securities and Exchange Commission (SEC) for blockchains that are functional but not decentralized.  Stablecoins are excluded from both CFTC and SEC regulation in this bill, "except for fraud and certain activities by registered firms."[138]
.In September 2017, China banned ICOs to cause abnormal return from cryptocurrency decreasing during announcement window. The liquidity changes by banning ICOs in China was temporarily negative while the liquidity effect became positive after news.[139]
.On 18 May 2021, China banned financial institutions and payment companies from being able to provide cryptocurrency transaction related services.[140] This led to a sharp fall in the price of the biggest proof of work cryptocurrencies. For instance, bitcoin fell 31%, Ethereum fell 44%, Binance Coin fell 32% and Dogecoin fell 30%.[141] Proof of work mining was the next focus, with regulators in popular mining regions citing the use of electricity generated from highly polluting sources such as coal to create bitcoin and Ethereum.[142]
.In September 2021, the Chinese government declared all cryptocurrency transactions of any kind illegal, completing its crackdown on cryptocurrency.[28]
.In April 2024, TVNZ's 1News reported that the Cook Islands government was proposing legislation that would allow "recovery agents" to use various means including hacking to investigate or find cryptocurrency that may have been used for illegal means or is the "proceeds of crime." The Tainted Cryptocurrency Recovery Bill was drafted by two lawyers hired by US-based debt collection company Drumcliffe. The proposed legislation was criticised by Cook Islands Crown Law's deputy solicitor general David Greig, who described it as "flawed" and said that some provisions were "clearly unconstitutional". The Cook Islands Financial Services Development Authority described Drumcliffe's involvement as a conflict of interest.[143]
.Similar criticism was echoed by Auckland University of Technology cryptocurrency specialist and senior lecturer Jeff Nijsse and University of Otago political scientist Professor Robert Patman, who described it as government overreach and described it as inconsistent with international law. Since the Cook Islands is an associated state that is part of the Realm of New Zealand, Patman said that the law would have "implications for New Zealand's governance arrangements." A spokesperson for New Zealand Foreign Minister Winston Peters confirmed that New Zealand officials were discussing the legislation with their Cook Islands counterparts.  Cook Islands Prime Minister Mark Brown defended the legislation as part of the territory's fight against international cybercrime.[143]
.On 9 June 2021, El Salvador announced that it will adopt bitcoin as legal tender, becoming the first country to do so.[144]
.The EU defines crypto assets as "a digital representation of a value or of a right that is able to be transferred and stored electronically using distributed ledger technology or similar technology."[145] The EU regulation Markets in Crypto-Assets (MiCA) covering asset-referenced tokens (ARTs) and electronic money tokens (EMTs) (also known as stablecoins) came into force on 30 June 2024. As of 17 January 2025, the European Securities and Markets Authority (ESMA) issued guidance to crypto-asset service providers (CASPs) allowing them to maintain crypto-asset services for non-compliant ARTs and EMTs until the end of March 2025.[146][147]
.The rest of MiCA came into force as of 30 December 2024, covering crypto-assets other than ART and EMT and CASPs. MiCA excludes crypto-assets if they qualify as financial instruments according to ESMA guidelines published on 17 December 2024 as well as crypto-assets that are unique and not fungible with other crypto-assets.[148][149]
.At present, India neither prohibits nor allows investment in the cryptocurrency market. In 2020, the Supreme Court of India had lifted the ban on cryptocurrency, which was imposed by the Reserve Bank of India.[150][151][152][153] Since then, an investment in cryptocurrency is considered legitimate, though there is still ambiguity about the issues regarding the extent and payment of tax on the income accrued thereupon and also its regulatory regime. But it is being contemplated that the Indian Parliament will soon pass a specific law to either ban or regulate the cryptocurrency market in India.[154] Expressing his public policy opinion on the Indian cryptocurrency market to a well-known online publication, a leading public policy lawyer and Vice President of SAARCLAW (South Asian Association for Regional Co-operation in Law) Hemant Batra has said that the "cryptocurrency market has now become very big with involvement of billions of dollars in the market hence, it is now unattainable and irreconcilable for the government to completely ban all sorts of cryptocurrency and its trading and investment".[155] He mooted regulating the cryptocurrency market rather than completely banning it. He favoured following IMF and FATF guidelines in this regard.
.South Africa, which has seen a large number of scams related to cryptocurrency, is said to be putting a regulatory timeline in place that will produce a regulatory framework.[156] The largest scam occurred in April 2021, where the two founders of an African-based cryptocurrency exchange called Africrypt, Raees Cajee and Ameer Cajee, disappeared with $3.8 billion worth of bitcoin.[157] Additionally, Mirror Trading International disappeared with $170 million worth of cryptocurrency in January 2021.[157]
.In March 2021, South Korea implemented new legislation to strengthen their oversight of digital assets. This legislation requires all digital asset managers, providers and exchanges to be registered with the Korea Financial Intelligence Unit in order to operate in South Korea.[158] Registering with this unit requires that all exchanges are certified by the Information Security Management System and that they ensure all customers have real name bank accounts. It also requires that the CEO and board members of the exchanges have not been convicted of any crimes and that the exchange holds sufficient levels of deposit insurance to cover losses arising from hacks.[158]
.Switzerland was one of the first countries to implement the FATF's Travel Rule. FINMA, the Swiss regulator, issued its own guidance to VASPs in 2019. The guidance followed the FATF's Recommendation 16, however with stricter requirements. According to FINMA's[159] requirements, VASPs need to verify the identity of the beneficiary of the transfer.
.On 30 April 2021, the Central Bank of the Republic of Turkey banned the use of cryptocurrencies and cryptoassets for making purchases on the grounds that the use of cryptocurrencies for such payments poses significant transaction risks.[160]
.In the United Kingdom, as of 10 January 2021, all cryptocurrency firms, such as exchanges, advisors and professionals that have either a presence, market product or provide services within the UK market must register with the Financial Conduct Authority. Additionally, on 27 June 2021, the financial watchdog demanded that Binance cease all regulated activities in the UK.[161]
.The incoming Labour government confirmed in November 2024 that it will proceed with the regulation of cryptoassets and new UK requirements are expected to come into force in 2026.[162]
.In 2021, 17 states in the US passed laws and resolutions concerning cryptocurrency regulation.[163] This led the Securities and Exchange Commission to start considering what steps to take. On 8 July 2021, Senator Elizabeth Warren, part of the Senate Banking Committee, wrote to the chairman of the SEC and demanded answers on cryptocurrency regulation due to the increase in cryptocurrency exchange use and the danger this posed to consumers. On 5 August 2021, the chairman, Gary Gensler, responded to Warren's letter and called for legislation focused on "crypto trading, lending and DeFi platforms," because of how vulnerable investors could be when they traded on crypto trading platforms without a broker. He also argued that many tokens in the crypto market may be unregistered securities without required disclosures or market oversight. Additionally, Gensler did not hold back in his criticism of stablecoins. These tokens, which are pegged to the value of fiat currencies, may allow individuals to bypass important public policy goals related to traditional banking and financial systems, such as anti-money laundering, tax compliance, and sanctions.[164]
.On 19 October 2021, the first bitcoin-linked exchange-traded fund (ETF) from ProShares started trading on the NYSE under the ticker "BITO." ProShares CEO Michael L. Sapir said the ETF would expose bitcoin to a wider range of investors without the hassle of setting up accounts with cryptocurrency providers. Ian Balina, the CEO of Token Metrics, stated that SEC approval of the ETF was a significant endorsement for the crypto industry because many regulators globally were not in favor of crypto, and retail investors were hesitant to accept crypto. This event would eventually open more opportunities for new capital and new people in this space.[165]
.The Department of the Treasury, on 20 May 2021, announced that it would require any transfer worth $10,000 or more to be reported to the Internal Revenue Service since cryptocurrency already posed a problem where illegal activity like tax evasion was facilitated broadly. This release from the IRS was a part of efforts to promote better compliance and consider more severe penalties for tax evaders.[166]
.On 17 February 2022, the Department of Justice named Eun Young Choi as the first director of a National Cryptocurrency Enforcement Team to help identify and deal with misuse of cryptocurrencies and other digital assets.[167]
.The Biden administration faced a dilemma as it tried to develop regulations for the cryptocurrency industry. On one hand, officials were hesitant to restrict a growing industry. On the other hand, they were committed to preventing illegal cryptocurrency transactions. To reconcile these conflicting goals, on 9 March 2022, Biden issued an executive order.[168] Followed this, on 16 September 2022, the Comprehensive Framework for Responsible Development of Digital Assets document was released[169] to support development of cryptocurrencies and restrict their illegal use. The executive order included all digital assets, but cryptocurrencies posed both the greatest security risks and potential economic benefits. Though this might not address all of the challenges in crypto industry, it was a significant milestone in the US cryptocurrency regulation history.[170]
.In February 2023, the SEC ruled that cryptocurrency exchange Kraken's estimated $42 billion in staked assets globally operated as an illegal securities seller. The company agreed to a $30 million settlement with the SEC and to cease selling its staking service in the US. The case would impact other major crypto exchanges operating staking programs.[171]
.On 23 March 2023, the SEC issued an alert to investors stating that firms offering crypto asset securities might not be complying with US laws. The SEC argued that unregistered offerings of crypto asset securities might not include important information.[172]
.On 23 January 2025, President Donald Trump signed Executive Order 14178, Strengthening American Leadership in Digital Financial Technology[173] revoking Executive Order 14067 of 9 March 2022, Ensuring Responsible Development of Digital Assets and the Department of the Treasury's Framework for International Engagement on Digital Assets of 7 July 2022.
In addition the order prohibits the establishment, issuance or promotion of Central bank digital currency and establishes a group tasked with proposing a federal regulatory framework for digital assets within 180 days.[174]
.The legal status of cryptocurrencies varies substantially from country to country and is still undefined or changing in many of them. At least one study has shown that broad generalizations about the use of bitcoin in illicit finance are significantly overstated and that blockchain analysis is an effective crime fighting and intelligence gathering tool.[175] While some countries have explicitly allowed their use and trade,[176] others have banned or restricted it. According to the Library of Congress in 2021,
an "absolute ban" on trading or using cryptocurrencies applies in 9 countries:
Algeria, Bangladesh, Bolivia, China, Egypt, Iraq, Morocco, Nepal, and the United Arab Emirates. An "implicit ban" applies in another 39 countries or regions, which include: Bahrain, Benin, Burkina Faso, Burundi, Cameroon, Chad, Cote d’Ivoire, the Dominican Republic, Ecuador, Gabon, Georgia, Guyana, Indonesia, Iran, Jordan, Kazakhstan, Kuwait, Lebanon, Lesotho, Macau, Maldives, Mali, Moldova, Namibia, Niger, Nigeria, Oman, Pakistan, Palau, Republic of Congo, Saudi Arabia, Senegal, Tajikistan, Tanzania, Togo, Turkey, Turkmenistan, Qatar and Vietnam.[177] In the United States and Canada, state and provincial securities regulators, coordinated through the North American Securities Administrators Association, are investigating "Bitcoin scams" and ICOs in 40 jurisdictions.[178]
.Various government agencies, departments, and courts have classified bitcoin differently. China Central Bank banned the handling of bitcoins by financial institutions in China in early 2014.
.In Russia, though owning cryptocurrency is legal, its residents are only allowed to purchase goods from other residents using the Russian ruble while nonresidents are allowed to use foreign currency.[179] Regulations and bans that apply to bitcoin probably extend to similar cryptocurrency systems.[180]
.In August 2018, the Bank of Thailand announced its plans to create its own cryptocurrency, the Central Bank Digital Currency (CBDC).[181]
.Cryptocurrency advertisements have been banned on the following platforms:
.On 25 March 2014, the United States Internal Revenue Service (IRS) ruled that bitcoin will be treated as property for tax purposes. Therefore, virtual currencies are considered commodities subject to capital gains tax.[189]
.As the popularity and demand for online currencies has increased since the inception of bitcoin in 2009,[190] so have concerns that such an unregulated person to person global economy that cryptocurrencies offer may become a threat to society. Concerns abound that altcoins may become tools for anonymous web criminals.[191]
.Cryptocurrency networks display a lack of regulation that has been criticized as enabling criminals who seek to evade taxes and launder money. Money laundering issues are also present in regular bank transfers, however with bank-to-bank wire transfers for instance, the account holder must at least provide a proven identity.
.Transactions that occur through the use and exchange of these altcoins are independent from formal banking systems, and therefore can make tax evasion simpler for individuals. Since charting taxable income is based upon what a recipient reports to the revenue service, it becomes extremely difficult to account for transactions made using existing cryptocurrencies, a mode of exchange that is complex and difficult to track.[191]
.Systems of anonymity that most cryptocurrencies offer can also serve as a simpler means to launder money. Rather than laundering money through an intricate net of financial actors and offshore bank accounts, laundering money through altcoins can be achieved through anonymous transactions.[191]
.Cryptocurrency makes legal enforcement against extremist groups more complicated, which consequently strengthens them.[192] White supremacist Richard Spencer went as far as to declare bitcoin the "currency of the alt-right".[193]
.In February 2014, the world's largest bitcoin exchange, Mt. Gox, declared bankruptcy. Likely due to theft, the company claimed that it had lost nearly 750,000 bitcoins belonging to their clients. This added up to approximately 7% of all bitcoins in existence, worth a total of $473 million. Mt. Gox blamed hackers, who had exploited the transaction malleability problems in the network. The price of a bitcoin fell from a high of about $1,160 in December to under $400 in February.[194]
.On 21 November 2017, Tether announced that it had been hacked, losing $31 million in USDT from its core treasury wallet.[195]
.On 7 December 2017, Slovenian cryptocurrency exchange Nicehash reported that hackers had stolen over $70 million using a hijacked company computer.[196]
.On 19 December 2017, Yapian, the owner of South Korean exchange Youbit, filed for bankruptcy after suffering two hacks that year.[197][198] Customers were still granted access to 75% of their assets.
.In May 2018, Bitcoin Gold had its transactions hijacked and abused by unknown hackers.[199] Exchanges lost an estimated $18m and bitcoin Gold was delisted from Bittrex after it refused to pay its share of the damages.
.On 13 September 2018, Homero Josh Garza was sentenced to 21 months of imprisonment, followed by three years of supervised release.[200] Garza had founded the cryptocurrency startups GAW Miners and ZenMiner in 2014, acknowledged in a plea agreement that the companies were part of a pyramid scheme, and pleaded guilty to wire fraud in 2015. The SEC separately brought a civil enforcement action in the US against Garza, who was eventually ordered to pay a judgment of $9.1 million plus $700,000 in interest. The SEC's complaint stated that Garza, through his companies, had fraudulently sold "investment contracts representing shares in the profits they claimed would be generated" from mining.[201]
.In January 2018, Japanese exchange Coincheck reported that hackers had stolen cryptocurrency worth $530 million.[202]
.In June 2018, South Korean exchange Coinrail was hacked, losing over $37 million in crypto.[203] The hack worsened a cryptocurrency selloff by an additional $42 billion.[204]
.On 9 July 2018, the exchange Bancor, whose code and fundraising had been subjects of controversy, had $23.5 million in crypto stolen.[205]
.A 2020 EU report found that users had lost crypto-assets worth hundreds of millions of US dollars in security breaches at exchanges and storage providers. Between 2011 and 2019, reported breaches ranged from four to twelve a year. In 2019, more than a billion dollars worth of cryptoassets was reported stolen. Stolen assets "typically find their way to illegal markets and are used to fund further criminal activity".[206]
.According to a 2020 report produced by the United States Attorney General's Cyber-Digital Task Force, three categories make up the majority of illicit cryptocurrency uses: "(1) financial transactions associated with the commission of crimes; (2) money laundering and the shielding of legitimate activity from tax, reporting, or other legal requirements; or (3) crimes, such as theft, directly implicating the cryptocurrency marketplace itself." The report concluded that "for cryptocurrency to realize its truly transformative potential, it is imperative that these risks be addressed" and that "the government has legal and regulatory tools available at its disposal to confront the threats posed by cryptocurrency's illicit uses".[207][208]
.According to the UK 2020 national risk assessment—a comprehensive assessment of money laundering and terrorist financing risk in the UK—the risk of using cryptoassets such as bitcoin for money laundering and terrorism financing is assessed as "medium" (from "low" in the previous 2017 report).[209] Legal scholars suggested that the money laundering opportunities may be more perceived than real.[210] Blockchain analysis company Chainalysis concluded that illicit activities like cybercrime, money laundering and terrorism financing made up only 0.15% of all crypto transactions conducted in 2021, representing a total of $14 billion.[211][212][213]
.In December 2021, Monkey Kingdom, a NFT project based in Hong Kong, lost US$1.3 million worth of cryptocurrencies via a phishing link used by the hacker.[214]
.On November 2, 2023, Sam Bankman-Fried was pronounced guilty on seven counts of fraud related to FTX.[215] Federal criminal court sentencing experts speculated on the potential amount of prison time likely to be meted out.[216][217][218] On March 28, 2024, the court sentenced Bankman-Fried to 25 years in prison.[219]
.According to blockchain data company Chainalysis, criminals laundered US$8,600,000,000 worth of cryptocurrency in 2021, up by 30% from the previous year.[220] The data suggests that rather than managing numerous illicit havens, cybercriminals make use of a small group of purpose built centralized exchanges for sending and receiving illicit cryptocurrency. In 2021, those exchanges received 47% of funds sent by crime linked addresses.[221] Almost $2.2bn worth of cryptocurrencies was embezzled from DeFi protocols in 2021, which represents 72% of all cryptocurrency theft in 2021.
.According to Bloomberg and the New York Times, Federation Tower, a two skyscraper complex in the heart of Moscow City, is home to many cryptocurrency businesses under suspicion of facilitating extensive money laundering, including accepting illicit cryptocurrency funds obtained through scams, darknet markets, and ransomware.[222] Notable businesses include Garantex,[223] Eggchange, Cashbank, Buy-Bitcoin, Tetchange, Bitzlato, and Suex, which was sanctioned by the U.S. in 2021. Bitzlato founder and owner Anatoly Legkodymov was arrested following money-laundering charges by the United States Department of Justice.[224]
.Dark money has also been flowing into Russia through a dark web marketplace called Hydra, which is powered by cryptocurrency, and enjoyed more than $1 billion in sales in 2020, according to Chainalysis.[225] The platform demands that sellers liquidate cryptocurrency only through certain regional exchanges, which has made it difficult for investigators to trace the money.
.Almost 74% of ransomware revenue in 2021 — over $400 million worth of cryptocurrency — went to software strains likely affiliated with Russia, where oversight is notoriously limited.[222] However, Russians are also leaders in the benign adoption of cryptocurrencies, as the ruble is unreliable, and President Putin favours the idea of "overcoming the excessive domination of the limited number of reserve currencies."[226]
.In 2022, RenBridge - an unregulated alternative to exchanges for transferring value between blockchains - was found to be responsible for the laundering of at least $540 million since 2020. It is especially popular with people attempting to launder money from theft. This includes a cyberattack on Japanese crypto exchange Liquid that has been linked to North Korea.[227]
.Properties of cryptocurrencies gave them popularity in applications such as a safe haven in banking crises and means of payment, which also led to the cryptocurrency use in controversial settings in the form of online black markets, such as Silk Road.[191] The original Silk Road was shut down in October 2013 and there have been two more versions in use since then. In the year following the initial shutdown of Silk Road, the number of prominent dark markets increased from four to twelve, while the amount of drug listings increased from 18,000 to 32,000.[191]
.Darknet markets present challenges in regard to legality. Cryptocurrency used in dark markets are not clearly or legally classified in almost all parts of the world. In the US, bitcoins are regarded as "virtual assets".[citation needed] This type of ambiguous classification puts pressure on law enforcement agencies around the world to adapt to the shifting drug trade of dark markets.[228][unreliable source?]
.Various studies have found that crypto-trading is rife with wash trading. Wash trading is a process, illegal in some jurisdictions, involving buyers and sellers being the same person or group, and may be used to manipulate the price of a cryptocurrency or inflate volume artificially. Exchanges with higher volumes can demand higher premiums from token issuers.[229] A study from 2019 concluded that up to 80% of trades on unregulated cryptocurrency exchanges could be wash trades.[229] A 2019 report by Bitwise Asset Management claimed that 95% of all bitcoin trading volume reported on major website CoinMarketCap had been artificially generated, and of 81 exchanges studied, only 10 provided legitimate volume figures.[230]
.In 2022, cryptocurrencies attracted attention when Western nations imposed severe economic sanctions on Russia in the aftermath of its invasion of Ukraine in February. However, American sources warned in March that some crypto-transactions could potentially be used to evade economic sanctions against Russia and Belarus.[231]
.In April 2022, the computer programmer Virgil Griffith received a five-year prison sentence in the US for attending a Pyongyang cryptocurrency conference, where he gave a presentation on blockchains which might be used for sanctions evasion.[232]
.The Bank for International Settlements summarized several criticisms of cryptocurrencies in Chapter V of their 2018 annual report. The criticisms include the lack of stability in their price, the high energy consumption, high and variable transactions costs, the poor security and fraud at cryptocurrency exchanges, vulnerability to debasement (from forking), and the influence of miners.[233][234][235]
.Cryptocurrencies have been compared to Ponzi schemes, pyramid schemes[236] and economic bubbles,[237] such as housing market bubbles.[238] Howard Marks of Oaktree Capital Management stated in 2017 that digital currencies were "nothing but an unfounded fad (or perhaps even a pyramid scheme), based on a willingness to ascribe value to something that has little or none beyond what people will pay for it", and compared them to the tulip mania (1637), South Sea Bubble (1720), and dot-com bubble (1999), which all experienced profound price booms and busts.[239]
.Regulators in several countries have warned against cryptocurrency and some have taken measures to dissuade users.[240] However, research in 2021 by the UK's financial regulator suggests such warnings either went unheard, or were ignored. Fewer than one in 10 potential cryptocurrency buyers were aware of consumer warnings on the FCA website, and 12% of crypto users were not aware that their holdings were not protected by statutory compensation.[241][242] Of 1,000 respondents between the ages of eighteen and forty, almost 70% wrongly assumed cryptocurrencies were regulated, 75% of younger crypto investors claimed to be driven by competition with friends and family, 58% said that social media enticed them to make high risk investments.[243] The FCA recommends making use of its warning list, which flags unauthorized financial firms.[244]
.Many banks do not offer virtual currency services themselves and can refuse to do business with virtual currency companies.[245] In 2014, Gareth Murphy, a senior banking officer, suggested that the widespread adoption of cryptocurrencies may lead to too much money being obfuscated, blinding economists who would use such information to better steer the economy.[246] While traditional financial products have strong consumer protections in place, there is no intermediary with the power to limit consumer losses if bitcoins are lost or stolen. One of the features cryptocurrency lacks in comparison to credit cards, for example, is consumer protection against fraud, such as chargebacks.
.The French regulator Autorité des marchés financiers (AMF) lists 16 websites of companies that solicit investment in cryptocurrency without being authorized to do so in France.[247]
.An October 2021 paper by the National Bureau of Economic Research found that bitcoin suffers from systemic risk as the top 10,000 addresses control about one-third of all bitcoin in circulation.[248] It is even worse for miners, with 0.01% controlling 50% of the capacity. According to researcher Flipside Crypto, less than 2% of anonymous accounts control 95% of all available bitcoin supply.[249] This is considered risky as a great deal of the market is in the hands of a few entities.
.A paper by John Griffin, a finance professor at the University of Texas, and Amin Shams, a graduate student found that in 2017 the price of bitcoin had been substantially inflated using another cryptocurrency, Tether.[250]
.Roger Lowenstein, author of "Bank of America: The Epic Struggle to Create the Federal Reserve," says in a New York Times story that FTX will face over $8 billion in claims.[251]
.Non-fungible tokens (NFTs) are digital assets that represent art, collectibles, gaming, etc. Like crypto, their data is stored on the blockchain. NFTs are bought and traded using cryptocurrency. The Ethereum blockchain was the first place where NFTs were implemented, but now many other blockchains have created their own versions of NFTs.
.According to Vanessa Grellet, renowned panelist in blockchain conferences,[252] there was an increasing interest from traditional stock exchanges in crypto-assets at the end of the 2010s, while crypto-exchanges such as Coinbase were gradually entering the traditional financial markets. This convergence marked a significant trend where conventional financial actors were adopting blockchain technology to enhance operational efficiency, while the crypto world introduced innovations like Security Token Offering (STO), enabling new ways of fundraising. Tokenization, turning assets such as real estate, investment funds, and private equity into blockchain-based tokens, had the potential to make traditionally illiquid assets more accessible to investors. Despite the regulatory risks associated with such developments, major financial institutions, including JPMorgan Chase, were actively working on blockchain initiatives, exemplified by the creation of Quorum, a private blockchain platform.[253]
.As the first big Wall Street bank to embrace cryptocurrencies, Morgan Stanley announced on 17 March 2021 that they will be offering access to bitcoin funds for their wealthy clients through three funds which enable bitcoin ownership for investors with an aggressive risk tolerance.[254] BNY Mellon on 11 February 2021 announced that it would begin offering cryptocurrency services to its clients.[255]
.On 20 April 2021,[256] Venmo added support to its platform to enable customers to buy, hold and sell cryptocurrencies.[257]
.In October 2021, financial services company Mastercard announced it is working with digital asset manager Bakkt on a platform that would allow any bank or merchant on the Mastercard network to offer cryptocurrency services.[258]
.Mining for proof-of-work cryptocurrencies requires enormous amounts of electricity and consequently comes with a large carbon footprint due to causing greenhouse gas emissions.[259] Proof-of-work blockchains such as bitcoin, Ethereum, Litecoin, and Monero were estimated to have added between 3 million and 15 million tons of carbon dioxide (CO2) to the atmosphere in the period from 1 January 2016 to 30 June 2017.[260] By November 2018, bitcoin was estimated to have an annual energy consumption of 45.8TWh, generating 22.0 to 22.9 million tons of CO2, rivalling nations like Jordan and Sri Lanka.[261] By the end of 2021, bitcoin was estimated to produce 65.4 million tons of CO2, as much as Greece,[262] and consume between 91 and 177 terawatt-hours annually.[263][264]
.Critics have also identified a large electronic waste problem in disposing of mining rigs.[265] Mining hardware is improving at a fast rate, quickly resulting in older generations of hardware.[266]
.Bitcoin is the least energy-efficient cryptocurrency, using 707.6 kilowatt-hours of electricity per transaction.[267]
.Before June 2021, China was the primary location for bitcoin mining. However, due to concerns over power usage and other factors, China forced out bitcoin operations, at least temporarily. As a result, the United States promptly emerged as the top global leader in the industry. An example of a gross amount of electronic waste associated with bitcoin mining operations in the US is a facility that located in Dalton, Georgia which is consuming nearly the same amount of electricity as the combined power usage of 97,000 households in its vicinity. Another example is that Riot Platforms operates a bitcoin mining facility in Rockdale, Texas, which consumes approximately as much electricity as the nearby 300,000 households. This makes it the most energy-intensive bitcoin mining operation in the United States.[268]
.The world's second-largest cryptocurrency, Ethereum, uses 62.56 kilowatt-hours of electricity per transaction.[269] XRP is the world's most energy efficient cryptocurrency, using 0.0079 kilowatt-hours of electricity per transaction.[270]
.Although the biggest PoW blockchains consume energy on the scale of medium-sized countries, the annual power demand from proof-of-stake (PoS) blockchains is on a scale equivalent to a housing estate. The Times identified six "environmentally friendly" cryptocurrencies: Chia, IOTA, Cardano, Nano, Solarcoin and Bitgreen.[271] Academics and researchers have used various methods for estimating the energy use and energy efficiency of blockchains. A study of the six largest proof-of-stake networks in May 2021 concluded:
.In terms of annual consumption (kWh/yr), the figures were: Polkadot (70,237), Tezos (113,249), Avalanche (489,311), Algorand (512,671), Cardano (598,755) and Solana (1,967,930). This equates to Polkadot consuming 7 times the electricity of an average U.S. home, Cardano 57 homes and Solana 200 times as much. The research concluded that PoS networks consumed 0.001% the electricity of the bitcoin network.[272] University College London researchers reached a similar conclusion.[273]
.Variable renewable energy power stations could invest in bitcoin mining to reduce curtailment, hedge electricity price risk, stabilize the grid, increase the profitability of renewable energy power stations and therefore accelerate transition to sustainable energy.[274][275][276][277][278]
.There are also purely technical elements to consider. For example, technological advancement in cryptocurrencies such as bitcoin result in high up-front costs to miners in the form of specialized hardware and software.[279] Cryptocurrency transactions are normally irreversible after a number of blocks confirm the transaction. Additionally, cryptocurrency private keys can be permanently lost from local storage due to malware, data loss or the destruction of the physical media. This precludes the cryptocurrency from being spent, resulting in its effective removal from the markets.[280]
.In September 2015, the establishment of the peer-reviewed academic journal Ledger (ISSN 2379-5980) was announced. It covers studies of cryptocurrencies and related technologies, and is published by the University of Pittsburgh.[281]
.The journal encourages authors to digitally sign a file hash of submitted papers, which will then be timestamped into the bitcoin blockchain. Authors are also asked to include a personal bitcoin address in the first page of their papers.[282][283]
.A number of aid agencies have started accepting donations in cryptocurrencies, including UNICEF.[284] Christopher Fabian, principal adviser at UNICEF Innovation, said the children's fund would uphold donor protocols, meaning that people making donations online would have to pass checks before they were allowed to deposit funds.[285][286]
.However, in 2021, there was a backlash against donations in bitcoin because of the environmental emissions it caused. Some agencies stopped accepting bitcoin and others turned to "greener" cryptocurrencies.[287] The U.S. arm of Greenpeace stopped accepting bitcoin donations after seven years. It said: "As the amount of energy needed to run bitcoin became clearer, this policy became no longer tenable."[288]
.In 2022, the Ukrainian government raised over US$10,000,000 worth of aid through cryptocurrency following the 2022 Russian invasion of Ukraine.[289]
.Bitcoin has been characterized as a speculative bubble by eight winners of the Nobel Memorial Prize in Economic Sciences: Paul Krugman,[290] Robert J. Shiller,[291] Joseph Stiglitz,[292] Richard Thaler,[293] James Heckman,[294] Thomas Sargent,[294] Angus Deaton,[294] and Oliver Hart;[294] and by central bank officials including Alan Greenspan,[295] Agustín Carstens,[296] Vítor Constâncio,[297] and Nout Wellink.[298]
.Investors Warren Buffett and George Soros have respectively characterized it as a "mirage"[299] and a "bubble";[300] while business executives Jack Ma and JP Morgan Chase CEO Jamie Dimon have called it a "bubble"[301] and a "fraud",[302] respectively, although Jamie Dimon later said he regretted dubbing bitcoin a fraud.[303] BlackRock CEO Laurence D. Fink called bitcoin an "index of money laundering".[304]
.In June 2022, business magnate Bill Gates said that cryptocurrencies are "100% based on greater fool theory".[305]
.Legal scholars criticize the lack of regulation, which hinders conflict resolution when crypto assets are at the center of a legal dispute, for example a divorce or an inheritance. In Switzerland, jurists generally deny that cryptocurrencies are objects that fall under property law, as cryptocurrencies do not belong to any class of legally defined objects (Typenzwang, the legal numerus clausus). Therefore, it is debated whether anybody could even be sued for embezzlement of cryptocurrency if he/she had access to someone's wallet. However, in the law of obligations and contract law, any kind of object would be legally valid, but the object would have to be tied to an identified counterparty. However, as the more popular cryptocurrencies can be freely and quickly exchanged into legal tender, they are financial assets and have to be taxed and accounted for as such.[306][307]
.In 2018, an increase in crypto-related suicides was noticed after the cryptocurrency market crashed in August. The situation was particularly critical in Korea as crypto traders were on "suicide watch". A cryptocurrency forum on Reddit even started providing suicide prevention support to affected investors.[308][309] The May 2022 collapse of the Luna currency operated by Terra also led to reports of suicidal investors in crypto-related subreddits.[310]

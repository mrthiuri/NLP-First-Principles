# ğŸ§  Understanding Language: NLP from First Principles

> *â€œHow do machines learn to understand language?â€*  
> This project started as me trying to answer that question â€” not with a fancy library, but from scratch.  
> Iâ€™d just discovered large language models and wanted to trace NLP back to its roots: from tokenization and Bag-of-Words to embeddings and real-world applications.

---

## ğŸŒ What This Is
A hands-on journey through **Natural Language Processing** â€” from rule-based systems to statistical and neural approaches.  
Itâ€™s part tutorial, part playground, part *â€œhey-I-was-curious-so-I-built-this.â€*

If youâ€™ve ever looked at an LLM and thought *â€œokay, but how did we get here?â€* â€” this notebook is for you.

---

## ğŸ“˜ Main Notebook â€” *Understanding Language.ipynb*

The core notebook walks through NLP step-by-step, building concepts and mini-models from scratch.

### Topics covered
- ğŸ§© Why language is hard for machines  
- ğŸ•°ï¸ From early rule-based NLP to statistical models  
- âœ‚ï¸ Text pre-processing (tokenization, cleaning, encoding)  
- ğŸ”¢ Representing text as numbers  
- ğŸ§± Bag of Words and TF-IDF (implemented from scratch)  
- ğŸ“¬ Spam detection using Bag of Words  
- ğŸ§® From term frequency to importance weighting  
- ğŸ“š N-grams and context  
- ğŸ§  Word2Vec â€” understanding word meaning through vectors  
- âœ¨ Transition from representation to understanding  

Each section mixes **theory, code, and visual intuition** so you can follow the logic behind how NLP systems evolved.

---

## ğŸ§ª Spin-Off Projects

This repo includes a few small, playful projects that build on ideas from the main notebook.

### ğŸ’¬ Tamaras Hair  
A simple **rule-based chatbot** that mimics early heuristic NLP â€” a throwback to when AI systems were hand-crafted, not trained.  

ğŸ“‚ `Tamaras Hair/`

---

### âš™ï¸ Encoder PlayGround  
A simple **interactive interface** that encodes any input text into a chosen system â€” binary, hex, or octal.  
A â€œside questâ€ to show that **tokenization and numerical encoding** werenâ€™t born with NLP; theyâ€™ve been with computing all along.

ğŸ“‚ `Text Encoder/Encoder-PlayGround/`

---

### ğŸ“š Book Recommendation System  
A **TF-IDF + cosine similarity** based recommender built using the Goodreads dataset.  
- Pick a book from the gallery (Gradio app)  
- Get top-10 similar books instantly  
- See classic NLP still powering useful systems today  

ğŸ“‚ `Book recommendation system/`

---

## ğŸ§© Future Ideas
- Add support for **Swahili** or other African languages  
- Visualize **embedding spaces** interactively  
- Add a section on **Transformers** â€” just to complete the arc (GPUs are quite cheap ğŸ˜„)

---

## ğŸ› ï¸ Tech Stack
- **Python 3.12**
- NumPy, Pandas, scikit-learn, joblib  
- Gradio for the demos  
- Dataset: *Goodbooks* (via Goodreads)

---

## ğŸ’¬ Final Thought
> *â€œIf you canâ€™t explain how machines understand language, build it until you can.â€*

---

**Author:** [Morgan Thiuri](https://www.linkedin.com/in/morgan-thiuri-40151327a/)  
Data Scientist | Exploring NLP from first principles  
